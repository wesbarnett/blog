{
  
    
        "post0": {
            "title": "Continious deployment to EC2 with Github Actions",
            "content": "In my previous post I covered how to setup an AWS EC2 instance with a barebones Flask project. At the end of the post we automated this by using Ansible. In this post, we take it one step further and let Github use Ansible to provision and configure our EC2 instance. If you haven’t read the last post, I recommend at least skimming through it to get an idea of what we’re setting up here. . Briefly, we want gunicorn running our flask server on port 8080. Then we want nginx to use a reverse proxy to serve that up to port 80 so people can visit our website like normal (or we can ping our REST API endpoints on port 80). We want to use Ansible via Github Actions to provision and configure everything in our EC2 instance automatically so that whenever we push the master branch to Github it updates our deployment. . This is not intended as a large-scale solution, but if you’re running a small website or REST API that you would like to demo to people, this streamlines some of the process of continously deploying. . Setup . Generate repository . First, generate a new repository based on my template by clicking here. The contents of the flask repository are discuess in the previous post here. . Go ahead and clone your repository to your local machine so you can make changes to the Flask server. . The Flask application only has one route at / that simply prints “It works!”. . Generate SSH keypair . Create an SSH keypair to be used between Github and AWS. Personally I prefer to do this locally on my machine using ssh-keypair, but you can use any third party tool. Don’t include a passphrase. . Copy private key to Github . Next you will need to copy and paste the private key into the secrets for your new repository that you generated. To do that, go to the settings for your repository and then go to “Secrets” on the left hand menu. Click “New Secret” and name your new secrets AWS_EC2_KEY. It must be named this since the value will be used in our Github Actions. Paste in the private key and save. The private key should begin with (OPENSSH might just be SSH in your implementation): . --BEGIN OPENSSH PRIVATE KEY-- . . Note: Your private key is encrypted before it reaches Github and not decrypted until it is used in a workflow according to Github. Still, understand the security implications of sharing your private key with a third-party. . Copy public key to AWS EC2 . Next, copy the public key and import it into AWS. Go here and select the white “Actions” dropdown box. Click import key, name your key, and then either browse for the public key or paste it in. It doesn’t matter what you name your key here as long as you know it’s associated with your Github. . . Create EC2 instance and associate Elastic IP . After this create an EC2 instance and associate an Elastic IP address with this. If you don’t know how to do this, you can follow the section in my previous post here. Return here after you have created an instance and associated an Elastic IP with it. . . Warning: If you&#39;re just testing this out and not wanting to run your Flask site yet, be sure to stop your EC2 instance when you are finished testing. Create A record for domain . Create an A record for your domain or subdomain for the IP address. I personally use namecheap.com. To add an A record for your domain or subdomain, follow this article. . Update Ansible configuration . Lastly, in your repository update ansible/deploy/hosts for your own domain. In the file replace every instance of test.barnett.science with your domain name. You can also change the app_name to whatever you desire, but it is not necessary. This is used as the systemd unit name that runs gunicorn as well as the directory of where the repository will be cloned. . ansible/deploy/hosts . [webservers] test.barnett.science [webservers:vars] domain_name=test.barnett.science app_name=flask-project ansible_ssh_user=ubuntu . Commit the change and push to your Github repository. You should now be able to visit your domain and see the text “It works!”. . Usage . Now that you have it working, simply make updates to your code and push. It’s recommended you create another branch to work on development and only merge into master when it is ready for production. Anything pushed to the master branch will be automatically deployed to your EC2 instance, so be sure to test locally! To do that, you can install gunicorn and flask, as well as other necessary packages and run the following at the top level of your local copy of the repository: . gunicorn --chdir application -b :8080 app:app . You can then visit http://localhost:8080 to see how it works locally. When confident with the changes, merge with master and push. . To learn more about how this repository is setup, see this section. .",
            "url": "https://barnett.science/linux/aws/ansible/github/2020/05/28/flask-actions.html",
            "relUrl": "/linux/aws/ansible/github/2020/05/28/flask-actions.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Run a Flask app on AWS EC2",
            "content": "Let’s say you’ve created a Flask application and you’re ready to launch it to AWS. At this point you may not care about scaling yet, and you just want to get it in production so you can demo it to prospective clients or employers. How do you get that up and running quickly? . In this blog post we’ll set up a simple Flask project to be run on an AWS EC2 instance using gunicorn and nginx. . After walking through this manually, we’ll automate the setup using Ansible . Flask project setup . I’m assuming you have already created your Flask application and know a little bit about how to run it locally. I have a barebones Flask project located here that I am using as a model for this blog post. All of the routes/views are located in application/app/routes.py in the project. . If you end up using your own Flask project, be sure that the directory structure is the same in order to follow this post. Specifically, __init__.py should be in application/app and should have contents that indicate where the Flask routes are stored. For example, in the skeleton used above they are in routes.py, so the contents of __init__.py is: . from flask import Flask app = Flask(__name__) from app import routes . Here’s the directory structure: . application/ app/ __init__.py routes.py requirements.txt . Running locally . To run the repository locally, clone it to your machine: . git clone https://github.com/wesbarnett/flask-project . Then install the requirements. In this example I’m creating a virtual environment: . cd flask-project python3 -m venv venv source ./venv/bin/activate python -m pip install flask gunicorn . Then to run locally do: . gunicorn --chdir application -b :8080 app:app . Then visit http://localhost:8080 and you should see a line that says “It works!”. . AWS Setup . Create and launch EC2 instance . So now you have your Flask project and you’re ready to move it to an AWS EC2 instance. It’s time to login to your AWS console’s EC2 dashboard. From there click the big orange “Launch Instance” button. . . Let’s use an Ubuntu Server 18.04 LTS image, so select that on the next page. From there you can choose what instance type you want. If you are able, choose the free tier eligible one if just testing this out. . . After selecting what instance type go ahead and skip ahead to step 6 where you can configure your security group. Add a rule for port 80 and possibly 443 if you’re going to use HTTPS later, or choose a security group that already has those ports open. Simply choose “HTTP” under type and it will automatically ensure port 80 is open for all traffic. Otherwise, you won’t be able to access your application in a web browser. . . When you click “Review and launch” you will get a warning that you are allowing traffic from anywhere. Then click “Launch”. At this point you’ll be asked to choose or create a keypair that you will use when you ssh into your instance. . . Warning: Ensure you stop the instance when you are done! Allocate elastic IP address . Next we want an elastic IP address. The advantage of this is that if we bring an instance down and bring another one up, we can just move the IP address to be associated with that new instance. Additionally if you stop an instance and restart it, without an elastic IP address, you will have to find the new IP address of your instance. . . Under “Network &amp; Security” on the left hand menu click “Elastic IPs”. Then click the orange “Allocate Elastic IP Address” button on the top right. Leave the settings alone and click the orange “Allocate” button. . At the top you’ll see a green bar with a button that says “Associate this Elastic IP Address”. Click that. Now choose your running instance and associate the IP address with that instance. . Domain name . If you are going to use a domain or subdomain, go ahead create an A record for your domain using your elastic IP address. Personally I have been using namecheap.com as my registrar. Here is an article on how to add an A address record for namecheap. . Setup instance . Once your instance is running you’ll need to ssh into the instance. From a Linux machine this looks like this: . ssh -i ~/.ssh/aws.pem ubuntu@&lt;ip-address&gt; . Here ~/.ssh/aws.pem is my key that I associated with this instance. The IP address is the Elastic IP you allocated and associated above. . Now you’ll want to clone your Flask project to the instance. For my barebones project it would be: . git clone https://github.com/wesbarnett/flask-project . Install packages . We won’t be using a virtual environment on this instance. You’ll need to install a few Ubuntu packages next: . sudo apt-get update sudo apt-get install nginx gunicorn3 python3-pip python3-flask . For the barebones project this is all we need. . Test gunicorn . Now you should be able to test gunicorn. Note that the executable here is named gunicorn3. This runs the server in the background, makes a GET request to test it, and then puts the server back into foreground. You should see “It works!” when you run curl: . gunicorn3 --chdir application -b :8080 app:app &amp; curl http://localhost:8080 fg . gunicorn systemd unit . We don’t want to have to type in gunicorn3 manually to run our Flask server. Let’s have this occur automatically at boot by creating and enabling a systemd unit. Create a file with the following contents at the indicated location: . /etc/systemd/system/gunicorn.conf . [Unit] Description=gunicorn to serve flask-project After=network.target [Service] WorkingDirectory=/home/ubuntu/flask-project ExecStart=/usr/bin/gunicorn3 -b 0.0.0.0:8080 --chdir application app:app [Install] WantedBy=multi-user.target . Change flask-project to the name of your project in the WorkingDirectory above. . Now to run your gunicorn service do: . sudo systemctl start gunicorn . You should now be able to do curl http://localhost:8080 and get a message that says “It works!”. . To enable this to run on boot, do: . sudo systemctl enable gunicorn . If you ever want to restart the server, do: . sudo systemctl restart gunicorn . nginx configuration . We don’t want to expose gunicorn to the web directly. It’s not designed for taking direct traffic and susceptible to denial-of service attacks if you do that. Instead we’ll use nginx to take the traffic and setup a reverse proxy to gunicorn. . First remove the default configuration from being enabled: . sudo rm /etc/nginx/sites-enabled/default . Then create a new configuration file: . /etc/nginx/sites-available/flask-project.conf . server { listen 80; listen [::]:80; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_pass http://localhost:8080; } } . Now enable it by creating a symlink: . sudo ln -s /etc/nginx/sites-available/flask-project.conf /etc/nginx/sites-enabled/ . Finally, start nginx: . sudo systemctl start nginx . You should now be able to enter your Elastic IP address and domain name into a web browser and see your website! . To enable nginx on boot do: . sudo systemctl enable nginx . Whenever you make a change to your code, you’ll need to pull it in with git and restart the gunicorn service. . Automate setup with Ansible . Let’s make setting up our server a even easier using Ansible. For this section I recommend creating a new, clean instance. If you’re not using the previous instance from above simply terminate it and release it’s elastic IP address. You can then associate the Elastic IP address with this new instance if you want or create a new one. Simply go back and follow the AWS setup steps again. . Install Ansible to your local machine. Ansible is a remote configuration and provisioning tool. For Linux distributions simply use your package manager. For MacOS you can use Homebrew. I’m not sure what options are available for Windows. You don’t need to install anything on your AWS instance, not even Ansible! . Create playbook . Ansible consists of playbooks that you use to tell it what to do. These playbooks then refer to configuration file templates. On your local machine create a new directory to contain your playbook and templates: . mkdir aws-flask-playbook cd aws-flask-playbook . In this directory create an Ansible configuration file: . ansible.cfg . [defaults] host_key_checking = False inventory = ./deploy/hosts . Now create a directory named deploy and in it a file named hosts. Add your Elastic IP address under [webservers]. You also need to change the location of your private key file. github_user and app_name are used in the Github url as well as naming some of the files later. . deploy/hosts . [webservers] your-elastic-ip [webservers:vars] ansible_ssh_user=ubuntu ansible_ssh_private_key_file=/home/wes/.ssh/aws.pem github_user=wesbarnett app_name=flask-project domain_name=test.barnett.science . Then create a playbook: . deploy.yaml . # Ansible playbook for deploying a Flask app - hosts: webservers become: yes become_method: sudo tasks: - name: install packages apt: name: &quot;{{ packages }}&quot; update_cache: yes vars: packages: - nginx - gunicorn3 - python3-pip - hosts: webservers become: yes become_method: sudo tasks: - name: clone repo git: repo: &#39;https://github.com/{{ github_user }}/{{ app_name }}.git&#39; dest: /srv/www/{{ app_name }} update: yes - hosts: webservers become: yes become_method: sudo tasks: - name: Install needed python packages pip: requirements: requirements.txt chdir: /srv/www/{{ app_name }} - hosts: webservers become: yes become_method: sudo tasks: - name: template systemd service config template: src: deploy/gunicorn.service dest: /etc/systemd/system/{{ app_name }}.service - name: start systemd app service systemd: name={{ app_name }}.service state=restarted enabled=yes - name: template nginx site config template: src: deploy/nginx.conf dest: /etc/nginx/sites-available/{{ app_name }}.conf - name: remove default nginx site config file: path=/etc/nginx/sites-enabled/default state=absent - name: enable nginx site file: src: /etc/nginx/sites-available/{{ app_name }}.conf dest: /etc/nginx/sites-enabled/{{ app_name }}.conf state: link force: yes - name: restart nginx systemd: name=nginx state=restarted enabled=yes . The playbook does the following: . Installs nginx, gunicorn3, and python-pip. | Clones the git repository https://github.com/&lt;github_user&gt;/&lt;app_name&gt; to /srv/www/. | Installs the python packages listed in requirements.txt of the repo to be cloned. flask should be listed if you are running a flask server. | Installs and starts a systemd service to run gunicorn. | Installs and enables an nginx configuration that forwards the gunicorn web service to port 80. | After creating create the two configuration file templates that the playbook uses in the deploy directory. These are template files - you do not and should not change anything in them, since Ansible will automatically fill in the variables from deploy/hosts. . deploy/nginx.conf . server { listen 80; listen [::]:80; server_name {{ domain_name }}; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_pass http://localhost:8080; } } . deploy/gunicorn.service . [Unit] Description=gunicorn to serve {{ app_name }} After=network.target [Service] WorkingDirectory=/srv/www/{{ app_name }} ExecStart=/usr/bin/gunicorn3 -b 0.0.0.0:8080 --chdir application app:app [Install] WantedBy=multi-user.target . Run playbook . After setting everything up above, you can now run the playbook. . ansible-playbook deploy.yaml . . After running the playbook you should be able to visit your Elastic IP address in a web browser and see “It works!” for the barebones Flask project. . Now as you make changes, simply push to Github as usual. When you’re ready to update your server, just re-run the playbook. . . Warning: If you&#39;re just testing this out and not wanting to run your Flask site yet, be sure to stop or terminate your EC2 instance.",
            "url": "https://barnett.science/linux/aws/ansible/2020/05/28/ansible-flask.html",
            "relUrl": "/linux/aws/ansible/2020/05/28/ansible-flask.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Arch Linux Installation",
            "content": ". Warning: This is my personal guide for installing an Arch Linux system. Use it only as a guide as you follow along with the official Installation Guide. This guide gives several options along the way, depending on your system. . For UEFI, EFISTUB is used to boot the kernel directly. | For systems needing BIOS, syslinux is used. | Gives options for encrypting the system if desired. | Gives notes on using btrfs subvolumes if desired. | . You may need additional packages for video drivers, etc. . Pre-installation . Download the Arch ISO and GnuPG signature. . Verify signature . If you have GnuPG installed on your current system, verify the download: . $ gpg --keyserver-options auto-key-retrieve --verify archlinux-&lt;version&gt;-dual.iso.sig . Create bootable disk . Create a bootable USB drive by doing the following on an existing Linux installation: . # dd bs=4M if=/path/to/archlinux-&lt;version&gt;-x86_64.iso of=/dev/sdx status=progress &amp;&amp; sync . where /dev/sdx is the USB drive. . Boot the live environment . Now boot from the USB drive. . Set the keyboard layout . If using a keymap other than US, temporarily set the keyboard layout by doing: . # loadkeys de-latin1 . Change de-latin1 to a layout found in /usr/share/kbd/keymaps/**/*.map.gz. . Verify the boot mode . Verify that you have booted with UEFI mode by checking that /sys/firmware/efi/efivars exists. If you’re not booted in the UEFI, you should setup your motherboard to do so to follow this installation guide. If you are not able to use UEFI, this guide has an option to boot from BIOS using syslinux. . Connect to the internet . If you have a wired connection, it should connect automatically. . If you have a wireless connection, first stop the wired connection to prevent conflicts: . # systemctl stop dhcpcd@interface.service . A list of interfaces can be found with: . # ip link . Then connect to a wifi network with wpa_supplicant: . # wpa_supplicant -B -i interface -C/run/wpa_supplicant # wpa_cli -i interface &gt; scan &gt; scan_results &gt; add_network &gt; set_network 0 ssid &quot;SSID&quot; &gt; set_network 0 psk &quot;passphrase&quot; &gt; enable_network 0 &gt; quit . Get an ip address with dhcpcd: . # dhcpcd . For both wired and wireless connections, check your connection with: . # ping archlinux.org . Update the system clock . # timedatectl set-ntp true . Partition the disk . . Warning: All data on your disk will be erased. Backup any data you wish to keep. . Note: This guide assumes your disk is at /dev/sda. Change if needed. Use lsblk to identify existing file systems. . Here are the possible layouts this guide uses. Modify to your needs. . UEFI, not encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | EFI system partition &lt;/ul&gt; | 550 MiB | . /mnt | /dev/sda2 | Linux x86-64 root (/) | Remainder of the device | . BIOS, not encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | boot partition | 550 MiB | . /mnt | /dev/sda2 | Linux x86-64 root (/) | Remainder of the device | . UEFI, encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | EFI system partition | 550 MiB | . /mnt | /dev/mapper/cryptroot | Linux x86-64 root (/) | Remainder of the device | . BIOS, encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | boot partition | 550 MiB | . /mnt | /dev/mapper/cryptroot | Linux x86-64 root (/) | Remainder of the device | . Use GPT fdisk to format the disk: . # gdisk /dev/sda . Create a new empty GUID partition table by typing o at the prompt. | Create a new partition by typing n at the prompt. Hit Enter when prompted for the partition number, keeping the default of 1. Hit Enter again for the first section, keeping the default. For the last sector, type in +550M and hit Enter. | For an EFI system, type in EF00 to indicate it is an EFI system partition for the partition type. Otherwise, use the default. | Now create at least one more partition for the installation. To create just one more partition to fill the rest of the disk, type n at the prompt and use the defaults for the partition number, first sector, last sector, and hex code. | If setting up a BIOS system with syslinux, enter expert mode by entering x. Then enter a and then 1 to set an attribute for partition 1. Then enter 2 to set it as a legacy BIOS partition and then Enter to exit the set attribute menu. 6. Finally, write the table to the disk and exit by entering w at the prompt. | . Tip: Here are one-liners for the above layout. For EFI: # sgdisk /dev/sda -o -n 1::+550M -n 2 -t 2:EF00 . For BIOS: # sgdisk /dev/sda -o -n 1::+550M -n 2 -A 1:set:2i . Create LUKS container . If encrypting your system with dm-crypt/LUKS, do: . # cryptsetup lukFormat --type luks2 /dev/sda2 # cryptsetup open /dev/sda2 cryptroot . Othwerwise, skip this step. . Format the partitions . Format your ESP as FAT32: . # mkfs.fat -F32 /dev/sda1 . Replace ext4 with the file system you are using in all of the following. . To format the LUKS container on an encrypted system do: . # mkfs.ext4 /dev/mapper/cryptroot . To format a regular (not encrypted) system, do: . # mkfs.ext4 /dev/sda2 . Mount the file systems . For the encrypted setup mount the LUKS container: . # mount /dev/mapper/cryptroot /mnt . For the regular setup mount the root partition: . # mount /dev/sda2 /mnt . In both cases make a mount point for the boot partition and mount it: . # mkdir -p /mnt/boot # mount /dev/sda1 /mnt/boot . . Tip: If using btrfs, create any subvolumes you wish to use as mount points now. Then unmount /mnt and remount your subvolumes to the appropriate mount points. For example, for the encrypted setup: # mount /dev/mapper/cryptroot /mnt # btrfs subvolume create /mnt/@ # btrfs subvolume create /mnt/@home # umount /mnt # mount -o compress=zstd,subvol=@ /dev/mapper/cryptroot /mnt # mkdir -p /mnt/home # mount -o compress=zstd,subvol=@home /dev/mapper/cryptroot /mnt/home # mkdir -p /mnt/boot # mount /dev/sda1 /mnt/boot . Installation . Select the mirrors . If you desire, edit /etc/pacman.d/mirrorlist to select which mirrors have priority. Higher in the file means higher priority. This file will be copied to the new installation. . Install packages . # pacstrap /mnt base btrfs-progs linux man-db man-pages texinfo vim which . Append additional packages you wish to install to the line. You will have the opportunity to install more packages in the chroot environment and when you boot into the new system. . If you have an AMD or Intel processor, you will want to go ahead and install the amd-ucode or intel-ucode packages, respectively to enable microcode updates later in the guide. . Configure the system . Fstab . Generate the fstab for your new installation: . # genfstab -U /mnt &gt; /mnt/etc/fstab . Chroot . chroot into the new installation: . # arch-chroot /mnt . Time zone . Set the time zone: . # ln -sf /usr/share/zoneinfo/&lt;Region&gt;/&lt;City&gt; /etc/localtime . Set the hardware clock from the system clock: . # hwclock --systohc . Localization . Uncomment needed locales in /etc/locale.gen (e.g., en_US.UTF-8). Then run: . # locale-gen . Set the LANG environment variable the locale in the following file. . /etc/locale.conf . LANG=en_US.UTF-8 . If you are not using a US keymap, make the keyboard layout permanent: . /etc/vconsole.conf . KEYMAP=de-latin1 . Network configuration . Hostname . Set the hostname. Change hostname to your preferred hostname in the following: . /etc/hostname . hostname . /etc/hosts . 127.0.0.1 localhost ::1 localhost 127.0.1.1 hostname.localdomain hostname . Configuration . systemd-networkd will be used to connect to the internet after installation is complete. . Create a minimal systemd-networkd configuration file with the following contents. Here interface is the wireless interface or the wired interface if not using wireless. . /etc/systemd/network/config.network . [Match] Name=interface [Network] DHCP=yes . Enable the systemd-networkd.service unit. . DNS resolution . To use systemd-resolved for DNS resolution, create a symlink as follows: . # ln -s /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf . Enable the systemd-resolved.service unit. . Wireless . If using a wireless interface, install the iwd package now: . # pacman -S iwd . Additionally enable the iwd.service for wireless on boot. . . Note: To prevent a known race condition that causes wireless device renaming problems, create a drop-in file for iwd.service with the following: [Unit] After=systemd-udevd.service systemd-networkd.service . Initramfs . If using an encrypted system, add the encrypt hook to your mkinitcpio configuration as shown below. Add the keymap hook if you are not using the default US keymap. If using btrfs, you can remove the fsck hook. . /etc/mkinitcpio.conf . HOOKS=(base udev autodetect modconf block filesystems keyboard fsck keymap encrypt) . . Tip: Move keyboard in front of autodetect if using an external USB keyboard that was not connected when the image is created. Regenerate initramfs: . # mkinitcpio -p linux . Root password . Set the root password: . # passwd . . Tip: You can skip this step if you are giving your normal user super user privileges via sudo. Add normal user . Install the sudo package: . # pacman -S sudo . Add a normal user, add it to the wheel group, and set the password as follows, where user is the name of your user: . # useradd -m -G wheel user # passwd user . Open the sudoers file and uncomment the wheel group, giving that user access to sudo: . # EDITOR=vim visudo . Swap file . If using btrfs, first create a subvolume for the swap file to reside on. Then, create an empty swap file and set it to not use COW: . # btrfs subvolume create /.swap # truncate -s 0 /.swap/swapfile # chattr +C /.swap/swapfile . If not using btrfs, simply create a directory: . # mkdir /.swap . In all cases do: . # dd if=/dev/zero of=/.swap/swapfile bs=1M count=2048 # chmod 600 /.swap/swapfile # mkswap /.swap/swapfile . Update fstab with a line for the swap file as follows: . # /etc/fstab /.swap/swapfile none swap defaults 0 0 . . Warning: If using btrfs instead of ext4, do not use a swap file for kernels before v5.0, since it may cause file system corruption. Instead, use a swap partition (not covered here). Boot loader . Two options are provided here. If you have a UEFI motherboard, use EFISTUB. Otherwise, use the BIOS setup with syslinux. In either case, you will need to set your kernel parameters. . EFISTUB . In the UEFI setup we are not using a boot loader. Instead we are booting the kernel directly via EFISTUB. Previously you should have created a EFI system partition of the size 550MiB and marked it with the partition type EF00. . Exit the chroot and reboot the system. From your Arch Linux live disk, boot into the UEFI Shell v2. Then do: . Shell&gt; map . Note the disk number for the hard drive where you are installing Arch Linux. This guide assumes it is 1. . Now create two UEFI entries using bcfg. . Shell&gt; bcfg boot add 0 fs1:vmlinuz-linux &quot;Arch Linux&quot; Shell&gt; bcfg boot add 1 fs1:vmlinuz-linux &quot;Arch Linux (Fallback)&quot; . Create a file with your kernel parameters as a single line: . Shell&gt; edit fs1:options.txt . . Note: Create at least one additional space before the first character of your boot line in your options files. Otherwise, the first character gets squashed by a byte order mark and will not be passed to the initramfs, resulting in an error when booting. Additionally, your options file should be one line, and one line only. Press F2 to save and F3 to quit. Now add that file as the options to your first boot entry: . Shell&gt; bcfg boot -opt 0 fs1:options.txt . Repeat the above process for your second, fallback entry, creating a text file named options-fallback.txt containing a single line with your kernel parameters, chaning the intird to the fallback image (i.e., /initramfs-linux-fallback.img). . Add it to the entry using bcfg boot -opt 1 fs:1 options-fallback.txt. . syslinux . In the BIOS setup we are using syslinux. Previously you should have created a boot partition that was marked with the attribute “legacy BIOS bootable”. . Now, while still in the chroot install syslinux: . # pacman -S syslinux . Create the following configuration file: . /boot/syslinux/syslinux.cfg . DEFAULT arch LABEL arch MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND kernel-parameters LABEL archfallback MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND fallback-kernel-parameters . where kernel-parameters is from the kernel parameters you will create in the next section. fallback-kernel-parameters is exactly the same except with initrd pointing to the fallback initramfs (i.e., /initramfs-linux-fallback.img). . Exit the chroot and unmount all of the partitions: . # umount -R /mnt . Then install the bootloader: . # syslinux --directory syslinux /dev/sda1 . And install the MBR: . # dd bs=440 count=1 conv=notrunc if=/usr/lib/syslinux/bios/gptmbr.bin of=/dev/sda . Kernel parameters . No matter what boot loader you use, you need to pass some kernel parameters to it as indicated in the above sections. . For an encrypted system, the kernel parameters will contain at least: . root=/dev/mapper/cryptroot ro initrd=/initramfs-linux.img init=/usr/lib/systemd/systemd cryptdevice=/dev/sda2:cryptroot . . Tip: Add :allow-discards after cryptroot to allow trimming if using an SSD. Then enable the fstrim.timer to trim the device weekly. For a regular system, the kernel parameters will contain at least: . root=/dev/sda2 ro initrd=/initramfs-linux.img init=/usr/lib/systemd/systemd . . Note: In either case, if using btrfs, and you want to boot from a specific subvolume, add rootfstype=btrfs rootflags=subvol=/@, where @ is the subvolume you will mount as /. . Note: If you have an Intel or AMD CPU, enable microcode updates by adding an /intel-ucode.img or /amd-ucode.img, respectively to initrd= with a comma separating the two images. It &#39;&#39;must&#39;&#39; be the first initrd entry on the line. For example: initrd=/intel-ucode.img,/initramfs-linux.img. Reboot . When you reboot you should be prompted for your LUKS password if you decided to encrypt the system. .",
            "url": "https://barnett.science/linux/2020/05/24/arch-install.html",
            "relUrl": "/linux/2020/05/24/arch-install.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Create a venv for a Jupyter kernel",
            "content": "After I setup my deep learning box, I installed JupyterHub following these instructions. I skipped the conda part myself since I like to run things in Python venv’s (virtual environments). . One aspect that I struggled with in finding in the documentation was how to setup a Python venv to be used for a Jupyter kernel. It was actually much simpler than expected. . First, create a venv that will be used for the kernel: . python -m venv my-venv . Then activate that virtual environment: . source ./my-venv/bin/activate . After that install ipykernel: . python -m pip install ipykernel . Lastly, register that environment as a kernel as described here: . python -m ipykernel install --user --name my-venv --display-name &quot;Python (my-venv)&quot; . That’s it! Now when you log into your JupyterHub and start a notebook, you should see it as available. . Installing packages . Whenenever you want to install new packages to be available in your kernel simply install them to your virtual environment. You can ssh into your box, activate your venv, and run pip to install them. . Alternatively, you can use the %pip magic in Jupyter to run the installation. For example, to install pandas: . %pip install pandas . Note that this will give you different results from running !pip, which is running it as a shell command. That won’t use the pip in the virtual environment being utilized by the kernel like %pip will. Neither will !python -m pip. You will end up installing your packages in the wrong location if you use the shell form. . .",
            "url": "https://barnett.science/deeplearning/2020/05/22/jupyter-kernel.html",
            "relUrl": "/deeplearning/2020/05/22/jupyter-kernel.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Building a deep learning box",
            "content": "During the process of going through the fast.ai course, I decided to build my own deep learning box. This post is summary of my experience, but it won’t tell you every detail needed to build your own. There are quite a few guides out there. . A couple of decades ago as a teenager I remember my brother ordering parts and putting together the family’s PC. Of course at the time it was used mainly for playing Alpha Centauri, downloading from Napster, and racking up the long distance phone bill not knowing area codes did not mean local calling. I also got exposed to HTML and Javascript at the time so that was my first real introduction to any kind of coding (Javascript has changed just a little since then). . I remember my brother using a website PC Parts Picker to put a list of compatible parts together and find where to buy them. That website is still commonly used today and what I used in picking my parts. This is the list I ended up with. (Note: Some items are now discontinued by the time you’re reading this). The important thing for me was to get an Nvidia card to be able to properly use CUDA on my machine. The card is on the lower end of what is considered acceptable, but I was on a budget. . Here are most of the parts in their boxes before assembling. . . The process was pretty straightforward. Motherboard went in first, then the processor, and so on. I did have an issue where the backplate came off when I unscrewed the mounting screws for the processor fan. I struggled to put the fan on due that until I realized it had fallen behind the case. I wish I had mounted the fan the other way since there is a plastic part that now covers one of the RAM ports. . . Most of the cords can only plug into one spot and everything on the motherboard and power supply are labeled. Just paint by numbers for the most part. I did make the mistake of not plugging the GPU into the power but got a nice message when I tried to boot up. . . . After I was able to boot I installed Arch Linux on it. I have been involved with the Arch community for quite some time in editing the Wiki and have installed it numerous times, so I was comfortable doing so (Here’s my own installation guide; use with caution. Additionally all the super computer clusters I used in graduate school were Linux-based, and our lab setup our own cluster. One downside about using Arch is that sometimes there are libraries that are made for Ubuntu that aren’t as easily compiled for Arch in a straightforward manner. . . After installing Arch I installed JupyterLab and JupyterHub, CUDA, and some of the deep learning frameworks and was able to start running pretty quickly. . . Thus far, it’s been a good experience using the machine for running a couple of Kaggle competitions. .",
            "url": "https://barnett.science/deeplearning/2020/02/02/deep-learning-box.html",
            "relUrl": "/deeplearning/2020/02/02/deep-learning-box.html",
            "date": " • Feb 2, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Bayesian Linear Regression with CmdStanPy",
            "content": "In this tutorial we&#39;ll use Stan, via CmdStanPy, to perform Bayesian Linear Regression. Visit the CmdStanPy Github page for information on how to install. . import matplotlib.pyplot as plt import numpy as np import pandas as pd from cmdstanpy import CmdStanModel from sklearn.datasets import load_boston from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split . The dataset we are using comes from Lesson 1 of Andrew Ng&#39;s Coursera course on Machine Learning. I&#39;ve chosen this since it only has one feature so we can easily visualize what is happening. You can find the contents here. . df = pd.read_csv(&quot;ex1data1.txt&quot;, header=None, names=[&quot;population&quot;, &quot;profit&quot;]) . df.head() . population profit . 0 6.1101 | 17.5920 | . 1 5.5277 | 9.1302 | . 2 8.5186 | 13.6620 | . 3 7.0032 | 11.8540 | . 4 5.8598 | 6.8233 | . df.plot(&#39;population&#39;, &#39;profit&#39;, lw=0, marker=&#39;o&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f764059d970&gt; . Let&#39;s do the typical train/test split of the data, with population being our feature, and profit our target. . X_train, X_test, y_train, y_test = train_test_split(df[&quot;population&quot;], df[&quot;profit&quot;], random_state=0) . Data dictionary and model file . Now we need to create a data dictionary for Stan. The values in the data dictionary correspond with the values in the model file that I have written located in model.stan. Here are the contents of that file, which will be loaded and compiled later in this notebook. I&#39;m generally following this example in the Stan User Guide. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[N] y; int&lt;lower=0&gt; N_new; matrix[N_new, K] x_new; } parameters { vector[K] beta; real&lt;lower=0&gt; sigma; } model { y ~ normal(x*beta, sigma); } generated quantities { vector[N] y_train_pred; vector[N_new] y_new; for (n in 1:N) { y_train_pred[n] = normal_rng(x[n]*beta, sigma); } for (n in 1:N_new) { y_new[n] = normal_rng(x_new[n]*beta, sigma); } } . N_train = X_train.values.shape[0] N_test = X_test.values.shape[0] X_train = np.hstack((np.ones((N_train,1)), X_train.values.reshape(-1,1))) X_test = np.hstack((np.ones((N_test,1)), X_test.values.reshape(-1,1))) y_train = y_train.values K = 2 data_dict = {&quot;N&quot;: N_train, &quot;K&quot;: K, &quot;x&quot;: X_train, &quot;y&quot;: y_train, &quot;N_new&quot;: N_test, &quot;x_new&quot;: X_test} . Here are the meanings of each variable or parameter: . N = Number of data points in training set. | K = Number of columns in our dataset. We have two columns, since we are going to create an intercept column. | x = N x K matrix. The actual data. | y = N sized vector. The target variable. | N_new = Number of data points in our test set. | x_new = N_new x K matrix. The test data. | beta = Parameters to be trained. | sigma = Parameter to be trained, standard deviation in the normal distribution | . What about the blocks in the Stan file? You will want to check out the reference here on this. But, in short: . Variables in the data block are where variables that are read in as data are declared. . Variables in the parameters block are where variables that are sampled by Stan&#39;s MCMC algorithm are declared. . The model block defines our model. Here we are using sampling notation to indicate our target variable, y, has a distribution corresponding with the right-hand side. In this case, it is normally distributed with a mean of X$ beta$ and a standard deviation of $ sigma$. We use this distribution because the major assumption is that the errors are normally distributed around a mean of zero and a standard deviation of $ sigma$. This is a direct consequence of that assumption, which again is described in more detail here. . The generated quantities block does not adjust the learned parameters. We use it here to make predictions on our training set (in order to get the R$^{2}$ metric on it) as well as predicting on our test set for evaluation. . Next we load and compile the Stan file as described above. . model = CmdStanModel(stan_file=&quot;model.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/model . Sampling . Now that we have compiled the Stan code, we can do sampling. The sample() method performs MCMC sampling in the following process for each chain: . Draw a value from the auxillary moment ($ rho$) distribution. | Use the leapfrog iterator to update $ rho$ and parameters $ theta$. | Measure the Hamiltonian of the new state. | Use Metroplis-Hastings to compare the new Hamiltonian and the previous Hamiltonian. Accept or reject. | Save parameters $ theta$. If accepted, the new state becomes the previous state, and the process is repeated. | For more details, see this section of the Stan reference manual. . fit = model.sample(data=data_dict) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:start chain 2 INFO:cmdstanpy:finish chain 2 INFO:cmdstanpy:start chain 3 INFO:cmdstanpy:finish chain 1 INFO:cmdstanpy:start chain 4 INFO:cmdstanpy:finish chain 3 INFO:cmdstanpy:finish chain 4 . Results . The following gives a summary of each parameter and of each sample in the generated quantities. . df_summary = fit.summary() . df_summary.head(10) . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ -114.13800 | 0.033104 | 1.233760 | -116.576000 | -113.80600 | -112.83200 | 1388.96 | 1834.80 | 1.000320 | . beta[1] -4.06342 | 0.020626 | 0.827210 | -5.422190 | -4.06876 | -2.64867 | 1608.43 | 2124.71 | 1.000920 | . beta[2] 1.20494 | 0.002254 | 0.089512 | 1.054030 | 1.20780 | 1.34983 | 1577.67 | 2084.08 | 1.001430 | . sigma 3.04292 | 0.005602 | 0.258644 | 2.656110 | 3.02426 | 3.49347 | 2131.48 | 2815.65 | 1.001030 | . y_train_pred[1] 9.97330 | 0.048726 | 3.076020 | 4.879570 | 9.95605 | 15.03880 | 3985.34 | 5264.57 | 0.999336 | . y_train_pred[2] 4.91241 | 0.049360 | 3.056540 | -0.181923 | 4.98684 | 9.75746 | 3834.55 | 5065.38 | 0.999112 | . y_train_pred[3] 2.54711 | 0.047909 | 3.034660 | -2.349200 | 2.58090 | 7.57581 | 4012.16 | 5300.00 | 0.999527 | . y_train_pred[4] 2.80746 | 0.049370 | 3.090090 | -2.509290 | 2.82996 | 7.85182 | 3917.56 | 5175.03 | 0.999559 | . y_train_pred[5] 3.86753 | 0.048720 | 3.072960 | -1.087240 | 3.76285 | 8.91217 | 3978.38 | 5255.37 | 1.001370 | . y_train_pred[6] 6.61481 | 0.048569 | 3.100650 | 1.529320 | 6.58784 | 11.64820 | 4075.50 | 5383.66 | 1.000040 | . Here are all the samples from all the chains: . df_samples = fit.get_drawset() . df_samples.head() . lp__ accept_stat__ stepsize__ treedepth__ n_leapfrog__ divergent__ energy__ beta.1 beta.2 sigma ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 -114.345 | 0.940039 | 0.268831 | 3.0 | 7.0 | 0.0 | 115.687 | -4.87938 | 1.23486 | 2.80795 | ... | 4.038810 | 0.820634 | 1.564890 | 0.89525 | 3.42023 | 0.159389 | 22.1371 | 0.955606 | 9.501570 | 6.104850 | . 1 -114.143 | 0.993854 | 0.268831 | 2.0 | 3.0 | 0.0 | 114.673 | -4.33896 | 1.29027 | 2.83174 | ... | -0.155484 | 0.652295 | 1.579160 | 3.95726 | 4.14529 | -0.440211 | 21.8035 | 10.706000 | 11.092300 | 1.476410 | . 2 -114.119 | 0.993898 | 0.268831 | 4.0 | 15.0 | 0.0 | 116.779 | -4.20318 | 1.18980 | 2.63140 | ... | 2.484450 | 1.607140 | 4.684500 | 5.79743 | 4.11147 | 4.977450 | 21.1929 | 0.983426 | 12.367600 | 7.188790 | . 3 -113.834 | 0.900557 | 0.268831 | 3.0 | 11.0 | 0.0 | 115.391 | -3.66850 | 1.11506 | 3.13985 | ... | 2.165080 | -0.119275 | 0.539258 | 2.41229 | 4.41436 | 1.592670 | 21.3086 | 8.015130 | 0.810029 | 0.892391 | . 4 -114.232 | 0.941207 | 0.268831 | 3.0 | 15.0 | 0.0 | 115.936 | -4.54294 | 1.31403 | 2.90515 | ... | -1.166400 | 4.986770 | 1.633330 | 5.03816 | 5.23080 | 3.522200 | 20.4852 | -1.422330 | 12.302300 | 8.824610 | . 5 rows × 107 columns . As expected, each parameter&#39;s conditional distribution is normal. The dashed lines in each plot separate out each chain that was sampled. . def plot_dist(df, param): _, ax = plt.subplots(1, 2, figsize=(12,4)) df[param].plot.density(ax=ax[0]) df[param].plot(ax=ax[1]) for i in [1000, 2000, 3000]: ax[1].axvline(i, color=&quot;black&quot;, ls=&quot;--&quot;) plt.tight_layout() . plot_dist(df_samples, &quot;beta.1&quot;) . plot_dist(df_samples, &quot;beta.2&quot;) . plot_dist(df_samples, &quot;sigma&quot;) . Model evalulation . The model is evaluated using the means drawn from the posterior predictive distribution and comparing those with the actual values. . y_test_pred = df_summary.loc[&quot;y_new[1]&quot;:,&quot;Mean&quot;].values y_train_pred = df_summary.loc[&quot;y_train_pred[1]&quot;:&quot;y_train_pred[72]&quot;,&quot;Mean&quot;].values . Here is the training set R$^{2}$: . r2_score(y_train, y_train_pred) . 0.7328456670720971 . Here is the test set R$^{2}$: . r2_score(y_test, y_test_pred) . 0.5840665444879212 . Inference: generated quantities in same file . We&#39;ve already made predictions both on the training and test sets using generated quantities block in our Stan code. . y_train_pred = df_samples.loc[:,&quot;y_train_pred.1&quot;:&quot;y_train_pred.72&quot;] y_test_pred = df_samples.loc[:, &quot;y_new.1&quot;:] . Note that we actually have 4,000 values for each example in our dataset. In the Bayesian point of view, one samples from a normal distribution with a mean of X$ beta$ and standard deviation $ sigma$ (where $ beta$ and $ sigma$ were previously learned in our MCMC sampling) repeatedly and then takes the mean of that as the prediction. This is the posterior predictive mean. . y_test_pred . y_new.1 y_new.2 y_new.3 y_new.4 y_new.5 y_new.6 y_new.7 y_new.8 y_new.9 y_new.10 ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 6.08919 | 0.831190 | 2.771020 | 1.19964 | 2.946870 | 0.176526 | 2.269890 | 12.43920 | -0.387497 | 6.639150 | ... | 4.038810 | 0.820634 | 1.564890 | 0.895250 | 3.42023 | 0.159389 | 22.1371 | 0.955606 | 9.501570 | 6.104850 | . 1 12.44440 | 5.118900 | 2.246190 | 2.82562 | -0.079497 | -0.518796 | 1.880710 | 7.30614 | 4.465200 | 5.113740 | ... | -0.155484 | 0.652295 | 1.579160 | 3.957260 | 4.14529 | -0.440211 | 21.8035 | 10.706000 | 11.092300 | 1.476410 | . 2 11.89770 | 3.838600 | 5.517290 | 1.51142 | -0.398633 | 0.979285 | 7.150900 | 4.49293 | 2.036590 | 1.760600 | ... | 2.484450 | 1.607140 | 4.684500 | 5.797430 | 4.11147 | 4.977450 | 21.1929 | 0.983426 | 12.367600 | 7.188790 | . 3 9.53034 | -1.520570 | 7.847520 | 1.49295 | 0.917469 | 4.955140 | 2.428350 | 3.39252 | 5.642480 | 1.699600 | ... | 2.165080 | -0.119275 | 0.539258 | 2.412290 | 4.41436 | 1.592670 | 21.3086 | 8.015130 | 0.810029 | 0.892391 | . 4 13.55290 | 6.947870 | -0.947383 | 3.79690 | 3.489320 | 0.367080 | 0.305908 | 12.57860 | 4.602460 | 3.596310 | ... | -1.166400 | 4.986770 | 1.633330 | 5.038160 | 5.23080 | 3.522200 | 20.4852 | -1.422330 | 12.302300 | 8.824610 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 3995 12.68620 | 1.036800 | 4.300930 | 4.01384 | 0.033404 | 4.478290 | 5.797940 | 5.23659 | 4.104620 | 5.144340 | ... | 1.530610 | -2.970480 | 3.818540 | -0.302308 | 9.37741 | 2.563720 | 15.6282 | 1.196290 | 3.683390 | 6.422640 | . 3996 10.12300 | 5.759870 | 8.194520 | -1.85087 | 1.570890 | -3.516320 | 1.199860 | 12.75210 | 5.988360 | 8.680570 | ... | -0.882405 | 4.475370 | -0.016239 | 5.493630 | 11.20450 | 2.397010 | 22.5335 | 5.389330 | 5.559980 | 5.270210 | . 3997 13.25500 | 2.881080 | 5.618130 | 3.52824 | 2.178100 | 1.196510 | -0.754878 | 13.79700 | 4.082110 | 0.358575 | ... | -0.214590 | 4.076230 | 4.240610 | 1.991630 | 3.35809 | -0.671965 | 29.6522 | 2.818600 | 13.254700 | -0.115525 | . 3998 12.08630 | -0.392227 | -0.154641 | 4.51602 | 2.124500 | 0.875612 | 3.252300 | 9.75291 | 6.481030 | 5.653030 | ... | 3.016780 | -1.002880 | 2.177480 | 6.925280 | 4.95899 | 1.920580 | 21.8189 | 4.591150 | 6.822760 | 0.935449 | . 3999 7.96612 | 4.395740 | -1.077040 | 0.24359 | 1.732280 | 10.293700 | 4.920210 | 9.94235 | 5.173920 | 10.439600 | ... | 8.242310 | 2.557110 | 1.761830 | 4.149350 | 3.29892 | -4.806080 | 20.8138 | 6.282450 | 13.570600 | 8.343440 | . 4000 rows × 25 columns . def plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test): test_mean = np.mean(y_test_pred, axis=0) test_lower = np.percentile(y_test_pred, 2.5, axis=0) test_upper = np.percentile(y_test_pred, 97.5, axis=0) plt.plot(X_test[:,1:].ravel(), np.mean(y_test_pred, axis=0), lw=0, marker=&#39;o&#39;, color=&#39;C3&#39;, label=&quot;Test post. pred. mean&quot;) plt.plot(X_test[:,1:].ravel(), y_test, lw=0, marker=&#39;o&#39;, color=&#39;C4&#39;, label=&quot;Test actual&quot;) sort_mask = np.argsort(X_train[:,1:].ravel()) y_upper = np.percentile(y_train_pred, 97.5, axis=0)[sort_mask] y_lower = np.percentile(y_train_pred, 2.5, axis=0)[sort_mask] plt.fill_between(X_train[:,1:].ravel()[sort_mask], y_lower, y_upper, color=&quot;C1&quot;, alpha=0.1, label=&quot;Train post. pred 95% confid.&quot;) plt.plot(X_train[:,1:].ravel()[sort_mask], np.mean(y_train_pred, axis=0)[sort_mask], label=&quot;Train post. pred. mean&quot;, color=&quot;C1&quot;) plt.legend() plt.ylabel(&quot;Profit in $10,000s&quot;) plt.xlabel(&quot;Population of city in 10,000s&quot;) plt.show() . Here&#39;s the plot with the posterior predictive distribution&#39;s means for each test point. The shaded error indicates the 95% confidence interval. One could calculate this for each point one predicts for the test set. In this case we just did it for the series of points in the training set and filled in the entire range to illustrate it. . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: generated quantities in separate file . This method we take the learned parameters from MCMC sampling and use them as data inputs for another Stan file where we can generate our new quantities. Here are the contents of our new file. This time beta and sigma are in the data block since they are not going to be learned parameters. Additionally we have no model. The purpose of the file is simply to generate predictions on new data. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[K] beta; real&lt;lower=0&gt; sigma; } parameters { } model { } generated quantities { vector[N] y; for (n in 1:N) { y[n] = normal_rng(x[n]*beta, sigma); } } . Here are our learned parameters from our previous MCMC sampling; . beta, sigma = df_summary.loc[&quot;beta[1]&quot;:&quot;beta[2]&quot;,&quot;Mean&quot;].values, df_summary.loc[&quot;sigma&quot;, &quot;Mean&quot;] . Here is our data dictionary. We are making predictions on our test set. . data_dict = {&quot;N&quot;: N_test, &quot;K&quot;: K, &quot;x&quot;: X_test, &quot;beta&quot;: beta, &quot;sigma&quot;: sigma} . predict = CmdStanModel(stan_file=&quot;predict.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/predict . Now for the sampling. We&#39;re not doing any MCMC sampling here though; we&#39;re just generating new quantities. Note that fixed_param is defined as true. . predict_fit = predict.sample(data_dict, fixed_param=True) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:finish chain 1 . predict_fit.summary().head() . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ 0.00000 | NaN | 0.00000 | 0.000000 | 0.00000 | 0.00000 | NaN | NaN | NaN | . y[1] 11.21580 | 0.087441 | 2.96484 | 6.347790 | 11.08770 | 16.03550 | 1149.680 | 41859.8 | 0.999105 | . y[2] 3.72469 | 0.112697 | 3.13389 | -1.419590 | 3.73300 | 8.73753 | 773.297 | 28155.7 | 0.999482 | . y[3] 6.21616 | 0.095375 | 3.00444 | 1.420470 | 6.28273 | 11.15670 | 992.327 | 36130.6 | 0.999321 | . y[4] 3.77119 | 0.094859 | 3.00454 | -0.931101 | 3.70973 | 8.62259 | 1003.220 | 36527.3 | 1.001740 | . predict_fit.get_drawset().head() . lp__ accept_stat__ y.1 y.2 y.3 y.4 y.5 y.6 y.7 y.8 ... y.16 y.17 y.18 y.19 y.20 y.21 y.22 y.23 y.24 y.25 . 0 0.0 | 0.0 | 9.93461 | 0.795845 | 9.37373 | 4.86905 | -1.936690 | 2.730590 | 6.71166 | 4.63234 | ... | 2.47424 | -2.106800 | 4.328990 | -4.09508 | 3.22460 | 0.928246 | 19.1007 | 2.54161 | -0.72615 | 4.27538 | . 1 0.0 | 0.0 | 9.78002 | 3.650580 | 2.46062 | 5.53808 | 0.517296 | 11.795200 | 3.18090 | 4.58967 | ... | 7.63182 | -3.622690 | 6.968730 | 2.36065 | 3.72439 | 1.174960 | 19.6114 | 3.16476 | 6.78070 | 6.59602 | . 2 0.0 | 0.0 | 16.65800 | 8.105250 | 10.15530 | 3.86963 | 3.501780 | 3.688170 | 4.22701 | 8.99956 | ... | -1.82703 | 3.957610 | 1.580840 | 1.65086 | 1.26456 | 5.537600 | 24.4481 | 1.57349 | 10.43740 | 3.81600 | . 3 0.0 | 0.0 | 11.98580 | 1.890660 | 0.21170 | 4.94291 | 7.858740 | 0.943565 | 4.44094 | 13.05240 | ... | -3.27595 | -0.240402 | 0.048488 | -2.81462 | 6.59352 | -1.051190 | 21.3794 | 6.60292 | 2.83685 | 4.78816 | . 4 0.0 | 0.0 | 8.98131 | 8.780330 | 8.03854 | -3.58606 | 5.412100 | 1.770760 | 2.06025 | 9.23470 | ... | 1.95720 | 5.884040 | 8.074480 | 4.45466 | -1.19269 | 1.124830 | 23.8451 | 13.15670 | 2.52032 | 9.40728 | . 5 rows × 27 columns . df_fit_samples = pd.DataFrame(predict_fit.sample[:,0,:], columns=predict_fit.column_names) . y_test_pred = df_fit_samples.loc[:, &quot;y.1&quot;:] . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: using numpy . Lastly, we don&#39;t actually need to use Stan to generate the posterior preditive distribution&#39;s mean. We can do that using numpy by writing the function ourself, which we do here. . y_test_pred = np.zeros((N_test, 1000)) for i in range(N_test): y_test_pred[i,:] = np.random.normal(np.matmul(X_test[i], beta), sigma, size=1000) . Note that we need to transpose our predictions to use our plot function from earlier. . plot_pred(X_train, X_test, y_train_pred, y_test_pred.transpose(), y_test) .",
            "url": "https://barnett.science/bayesian/2020/01/27/baysian-linear-regression.html",
            "relUrl": "/bayesian/2020/01/27/baysian-linear-regression.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Classification Decision Trees & Entropy",
            "content": "In this notebook I walk through how a classification decision tree is fit, how inference is performed, how to reduce overfitting, how splits are determined. This is based on a notebook I created while I was fellow at Insight Data Science when studying for interviews. . import graphviz import matplotlib.pyplot as plt import numpy as np from sklearn import tree from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier . scikit-learn now comes with a way to plot trees, but I prefer using graphviz so here is a quick function to plot a tree, which we monkey-patch into the DecisionTreeClassifier and DecisionTreeRegressor classes. . def display_tree(self): dot_data = tree.export_graphviz(self, out_file=None, filled=True, rounded=True, special_characters=True) graph = graphviz.Source(dot_data) return graph DecisionTreeClassifier.plot = display_tree . We&#39;ll use the Iris dataset as a quick way to discuss classification trees. To learn more about this dataset use help(load_iris). . iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) . Fitting . Decision trees search over all possible ways to split up features and find the split that is most informative about the target variable. The parent node splits into two child nodes based on this split. From there the children also split in the same manner until all leaves are pure, unless another stopping condition is specified. A leaf is a node that has no children. A pure leaf is a leaf with only one class of items in it. . Classifications trees split using the GINI impurity which is defined as: . $I_{G}(p) = sum_{i=1}^{J}p_{i}(1-p_{i})$ . Here $p_{i}$ is the probability of an item with label $i$ being chosen and $1 - p_{i}$ is the probability of a mistake in categorizing that item. $J$ is the number of classes. Gini reaches zero when all cases in the node fall into a single target category. . Alternatively, one can use information gain to decide where to split, where information gain is defined as the difference in entropy of the parent and the weighted sum of the entropies of the children. Entropy is defined as: . $H(p) = - sum_{i=1}^{J}p_{i} log_{2}(p_{i})$ . Let&#39;s train a decision tree: . clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 2 ≤ 2.35 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 8 X 3 ≤ 1.75 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;8 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 X 1 ≤ 3.1 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 6 entropy = 0.0 samples = 3 value = [0, 0, 3] 5&#45;&gt;6 7 entropy = 0.0 samples = 1 value = [0, 1, 0] 5&#45;&gt;7 9 X 3 ≤ 1.65 entropy = 0.811 samples = 4 value = [0, 1, 3] 8&#45;&gt;9 12 entropy = 0.0 samples = 35 value = [0, 0, 35] 8&#45;&gt;12 10 entropy = 0.0 samples = 3 value = [0, 0, 3] 9&#45;&gt;10 11 entropy = 0.0 samples = 1 value = [0, 1, 0] 9&#45;&gt;11 Here&#39;s the training score which indeed shows the tree is perfect at classfying the flowers on the training set. This tends to result in overfitting to the training set. . clf.score(X_train, y_train) . 1.0 . This is an easy dataset to classify, so the overfitting is not evident here. . clf.score(X_test, y_test) . 0.9736842105263158 . There are 7 leaves in our tree. Note that the leaves do not have to be depicted at the bottom of the tree in the diagram. A leaf is just a node without any children and could be represented near the top of the tree. . clf.get_n_leaves() . 7 . Inference . Now that we have trained our model, we can perform inference. . When inference on new samples is performed, the sample simply is examined with the &quot;rules&quot; created by the feature splits. Starting from the topmost node (the root node) in our example above, if feature three has a value of less than or equal 0.8, go to the left child node; otherwise go to the right. This process continues all the way down until the sample is put into a leaf. . The predicted class is the class in the leaf with the highest probability of that class for that leaf. In other words, simply break down the training samples by class within that leaf and choose the class with the most number of train samples. The probability of choosing that class is simply the number of training samples in that leaf belonging to that class divided by the total number of training samples in that leaf. . Since all of our leafs are pure, the classifier will always give 100% for its predictions. We&#39;ll rexamine this when we have impure leafs below. . Here are the features for the first test sample. . X_test[0] . array([5.8, 2.8, 5.1, 2.4]) . The path for this sample follows down the right side of the tree. Note that features are zero-indexed. . Root node: Is 2.4 &lt;= 0.8? No, so go right. . Is 5.1 &lt;= 4.95? No, so go right. . Is 5.1 &lt;= 5.05? No, so go right. . That brings it to the leaf on the right with 35 samples, where the 3rd class (index 2) is predicted: . clf.predict([X_test[0]]) . array([2]) . Again, the probability is 100% since there are no training samples in that leaf from the other two classes. In the plot the node is colored dark purple. For this tree purple represents the 3rd class and the darker the shade the more probable it is. . clf.predict_proba([X_test[0]]) . array([[0., 0., 1.]]) . Pruning . One way to prevent overfitting is the pre-prune the tree by specifying the maximum depth and/or maximum number of leaves. Here we set the maximum depth to 3. . clf = DecisionTreeClassifier(max_depth=3, random_state=0, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 3 ≤ 0.8 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 6 X 2 ≤ 5.05 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;6 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 7 entropy = 0.811 samples = 4 value = [0, 1, 3] 6&#45;&gt;7 8 entropy = 0.0 samples = 35 value = [0, 0, 35] 6&#45;&gt;8 clf.score(X_train, y_train) . 0.9821428571428571 . clf.score(X_test, y_test) . 0.9736842105263158 . Here is a sample where we are only 75% sure that it is class 2, since only 3 of the 4 samples in its leaf are class 2. . clf.predict([X_test[20]]) . array([2]) . clf.predict_proba([X_test[20]]) . array([[0. , 0.25, 0.75]]) . A popular and effective way to reduce overfitting is to create a &quot;forest&quot; of weak decision trees and use them together (for example, random forests). This is beyond the scope of this notebook. . Information gain &amp; splits . Let&#39;s talk a little bit more about how trees use entropy (or alternatively Gini) to determine splits. . Information gain is calculated by cycling through all possible splits in the training set. Practically this is the process: . Select the first feature. | Pick the halfway point between the first sample and the second sample. | Calculate the entropy of the two child nodes if a split is made at that point. | Repeat steps 2 and 3 for all midpoints for this feature. | Go back to step 1 and repeat for all features. | At the end, pick the feature and split that has the lowest weighted summation of the entropies for the two child nodes. . Again, information gain is the difference in entropy of the parent node and the weighted summation of entropies of the two child nodes. Since the entropy of the parent node is the same for each potential split that we try, we only need to calculate the entropies of the child nodes for a split and find the split that minimizes their weighted sum since that will maximize the information gained. . Here&#39;s the formula for information gain (IG), where $H_{parent}$ is the entropy of the parent node, $N_{left}$ is the number of samples in the left child, $H_{left}$ is the entropy of the left child, $N_{right}$ is the number of samples in the right child, $H_{right}$ is the entropy of the right child: . $IG = H_{parent} - (N_{left} H_{left} + N_{right} H_{right})$ . Here&#39;s a tree with a max depth of 1 using entropy to split: . clf = DecisionTreeClassifier(random_state=42, max_depth=1, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 2 ≤ 2.35 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False Let&#39;s implement our entropy calculation. As a reminder, the formula is: . $H(p) = - sum_{i=1}^{J}p_{i} log_{2}(p_{i})$ . This takes a list of targets and calculates the entropy for that node. . from collections import Counter def entropy(values): total = values.shape[0] c = Counter(values) if total == 0: return 0 s = 0 for x in c.values(): p = x/total s += p * np.log2(p) return -s . Plotting entropy in the binary class case . Let&#39;s take a quick look at an example where we have two classes, 0 and 1, in 100 samples (so 50 of each class) to get an idea of what entropy is describing. If we perfectly split our data into 0&#39;s in the left bucket, and 1&#39;s into the right bucket, we have 0 entropy. Whatever (hypothetical) decision rule that caused that split has split our data perfectly. . left, right = np.zeros(50), np.ones(50) . Because each node is pure, the entropy is 0 for each node: . entropy(left), entropy(right) . (-0.0, -0.0) . left[0], right[0] = 1, 0 . Now let&#39;s change 1 sample - moving one of the 0&#39;s from the left node to the right, and one of the 1&#39;s from the right node to the left. Here are the entropies of those child nodes. In both cases, the entropy goes up by the same amount. . entropy(left), entropy(right) . (0.14144054254182067, 0.14144054254182067) . Let&#39;s make this discussion even simpler by looking at just a single node containing 100 samples of either 0&#39;s or 1&#39;s. Let&#39;s start with a pure node of containing just 0&#39;s and incrementally change those 0&#39;s to 1&#39;s and find out what happens with the entropy: . node = np.zeros(100) . x = [] y = [] for i in range(99): x.append((i+1)/100.) node[i] = 1 y.append(entropy(node)) . Here&#39;s the plot of the entropy of that node as a function of the fraction of positive class samples (1&#39;s) in that node. Initially when the fraction is zero (the node is all 0&#39;s), the node is pure, so the entropy is 0. As we begin to change 0&#39;s for 1&#39;s the entropy increases and eventually reaches 1 when the fraction is 0.5 (equal amount of 1&#39;s and 0&#39;s). From there, the entropy decreases as the fraction of the positive class continues to increase and eventually reaches zero again when the node is all 1&#39;s and is thu pure. . plt.plot(x,y) plt.ylabel(&quot;Entropy&quot;) plt.xlabel(&quot;Fraction positive class&quot;) plt.show() . Remember, splits are determined by information gain, that is, the difference in entropy of the parent and the weighted summation of the entropies of the children. If you have a child node with an entropy of 1.0 that means that node contributes nothing to understanding of how to split the data into the two classes. If you have a child node with an entropy of 0.0 you have been able to perfectly segment out those samples. But again, we want the weighted combination of the entropies of the children to be low. You might have a situation where one is very low and one is very high, but it&#39;s better to split elsewhere to make the weighted sum lower. . Finding the best split through iteration . Here&#39;s our original list of targets from the Iris example: . y_train . array([1, 1, 2, 0, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0]) . Here&#39;s the associated entropy which matches above. This is the entropy of the root node. . entropy(y_train) . 1.5807197138422104 . Now let&#39;s get the entropies of the child nodes for the split that was found. In this case it was feature 3 with the split at 0.8. . feat = 3 x = 0.8 . Here are the samples that went into the left node. As you can see all of them are of class 0. There is no entropy associated with this node - the node is pure. . left = y_train[X_train[:,feat] &lt;= x] left . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . entropy(left) . -0.0 . Here are the samples that went into the right node: . right = y_train[X_train[:,feat] &gt; x] right . array([1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2]) . entropy(right) . 0.993707106604508 . Now we need the cycle through each feature, and then cycle through each possible split. . import math def calc_entropies(X, y): n = y.shape[0] m = X.shape[1] min_ents = np.zeros(m) min_splits = np.zeros(m) for feat in range(m): # Use set to remove dups; sort it to get halfway points points = sorted(list(set(X[:,feat]))) splits = [(points[i-1]+points[i])/2. for i in range(1, len(points))] entropies = [] for x in splits: l = y_train[X_train[:,feat] &lt;= x] r = y_train[X_train[:,feat] &gt; x] e = (l.shape[0]*entropy(l) + r.shape[0]*entropy(r)) / n entropies.append(e) feat_min_ent = np.argmin(entropies) min_ents[feat] = entropies[feat_min_ent] min_splits[feat] = splits[feat_min_ent] min_feat = np.argmin(min_ents) return min_feat, min_splits[min_feat], min_ents[min_feat] . calc_entropies(X_train, y_train) . (2, 2.35, 0.6654288660298044) . Note that in this example splitting feature 3 at 0.8 gives the same entropy as the above split. np.argmin returns the argument of the first minimum in the case of a tie. .",
            "url": "https://barnett.science/machinelearning/2019/05/14/decision-trees.html",
            "relUrl": "/machinelearning/2019/05/14/decision-trees.html",
            "date": " • May 14, 2019"
        }
        
    
  
    
        ,"post7": {
            "title": "Speed comparisons with Python, Numba, Fortran",
            "content": "The classic Monte Carlo &quot;Hello World&quot; is to calculate $ pi$ by generating random numbers and accepting or rejecting them. The algorithm below is taken from one of my graduate textbooks on molecular simulations - I apologize that I can&#39;t remember where, and that book is long gone. Essentially one is throwing darts into the unit square and then measuring if it is within the quarter of the unit circle. $ pi$ equals the number of darts in the circle divided by all of the attempts. Since it is a quarter of the unit circle, we multiple by 4. . I thought it would be fun to try to use some of the available tooling to get a speed performance in Python as well as using some Fortan inline since Fortran was the major language I used in graduate school. Note that fortranmagic is located in the fortran-magic package. . %load_ext fortranmagic import numpy as np from numba import jit, njit, prange . n = 10000000 . Plain Python and Numpy . def calc_pi_p(n): accept = 0.0 for i in range(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_p(n) . CPU times: user 47.9 s, sys: 25.6 ms, total: 47.9 s Wall time: 48.1 s . 3.141434 . Numba . @jit def calc_pi_numba(n): accept = 0.0 for i in range(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_numba(n) . CPU times: user 2.29 s, sys: 13.2 ms, total: 2.31 s Wall time: 2.31 s . 3.1418784 . Numba in parallel . @njit(parallel=True) def calc_pi_numba_parallel(n): accept = 0.0 for i in prange(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_numba_parallel(n) . CPU times: user 1.24 s, sys: 9.69 ms, total: 1.25 s Wall time: 1.09 s . 3.1403444 . Fortran . %%fortran function calc_pi_f(n) result(pi) implicit none integer(8) :: accept real(8) :: r(2), pi integer(8), intent(in) :: n integer(8) :: i accept = 0 do i = 1, n call random_number(r) if (dot_product(r,r) &lt;= 1.0) then accept = accept + 1 end if end do pi = 4.0d0 * dble(accept)/dble(n) end function calc_pi_f . %%time calc_pi_f(n) . CPU times: user 406 ms, sys: 0 ns, total: 406 ms Wall time: 405 ms . 3.1423488 . Looks like Fortran is the winner in this case. We didn&#39;t even parallelize the operation, but Numba does give a signifcant speedup compared to plain Python/Numpy. There might be a way to vectorize some of the operations in Numpy though to perform better, but I failed to find out how to do that in this case. .",
            "url": "https://barnett.science/performance/2018/08/27/fortran-speed.html",
            "relUrl": "/performance/2018/08/27/fortran-speed.html",
            "date": " • Aug 27, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I solve business problems through the end-to-end implementation of machine learning products. . . I have a BS in Mechanical Engineering from Mississippi State University, an MDiv from New Orleans Baptist Theological Seminary, and a PhD in Chemical &amp; Molecular Engineering from Tulane University. My graduate research utilized programming, data science, and molecular simulations to study self-assembling molecules. After receiving my PhD, I worked as a postdoctoral research scientist at Columbia University where I studied polymer interactions and gas separation membranes using molecular simulations and machine learning. . More recently I was a fellow with Insight Data Science where I created a Chrome Extension to predict where a user would post their content. I currently am in a senior machine learning engineer role at a financial company. .",
          "url": "https://barnett.science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://barnett.science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}