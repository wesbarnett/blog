{
  
    
        "post0": {
            "title": "Bayesian Linear Regression with CmdStanPy",
            "content": "In this tutorial we&#39;ll use Stan, via CmdStanPy, to perform Bayesian Linear Regression. Visit the CmdStanPy Github page for information on how to install. . import matplotlib.pyplot as plt import numpy as np import pandas as pd from cmdstanpy import CmdStanModel from sklearn.datasets import load_boston from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split . The dataset we are using comes from Lesson 1 of Andrew Ng&#39;s Coursera course on Machine Learning. I&#39;ve chosen this since it only has one feature so we can easily visualize what is happening. You can find the contents here. . df = pd.read_csv(&quot;ex1data1.txt&quot;, header=None, names=[&quot;population&quot;, &quot;profit&quot;]) . df.head() . population profit . 0 6.1101 | 17.5920 | . 1 5.5277 | 9.1302 | . 2 8.5186 | 13.6620 | . 3 7.0032 | 11.8540 | . 4 5.8598 | 6.8233 | . df.plot(&#39;population&#39;, &#39;profit&#39;, lw=0, marker=&#39;o&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc7674f5a90&gt; . Let&#39;s do the typical train/test split of the data, with population being our feature, and profit our target. . X_train, X_test, y_train, y_test = train_test_split(df[&quot;population&quot;], df[&quot;profit&quot;], random_state=0) . Data dictionary and model file . Now we need to create a data dictionary for Stan. The values in the data dictionary correspond with the values in the model file that I have written located in model.stan. Here are the contents of that file, which will be loaded and compiled later in this notebook. I&#39;m generally following this example in the Stan User Guide. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[N] y; int&lt;lower=0&gt; N_new; matrix[N_new, K] x_new; } parameters { vector[K] beta; real&lt;lower=0&gt; sigma; } model { y ~ normal(x*beta, sigma); } generated quantities { vector[N] y_train_pred; vector[N_new] y_new; for (n in 1:N) { y_train_pred[n] = normal_rng(x[n]*beta, sigma); } for (n in 1:N_new) { y_new[n] = normal_rng(x_new[n]*beta, sigma); } } . N_train = X_train.values.shape[0] N_test = X_test.values.shape[0] X_train = np.hstack((np.ones((N_train,1)), X_train.values.reshape(-1,1))) X_test = np.hstack((np.ones((N_test,1)), X_test.values.reshape(-1,1))) y_train = y_train.values K = 2 data_dict = {&quot;N&quot;: N_train, &quot;K&quot;: K, &quot;x&quot;: X_train, &quot;y&quot;: y_train, &quot;N_new&quot;: N_test, &quot;x_new&quot;: X_test} . Here are the meanings of each variable or parameter: . N = Number of data points in training set. | K = Number of columns in our dataset. We have two columns, since we are going to create an intercept column. | x = N x K matrix. The actual data. | y = N sized vector. The target variable. | N_new = Number of data points in our test set. | x_new = N_new x K matrix. The test data. | beta = Parameters to be trained. | sigma = Parameter to be trained, standard deviation in the normal distribution | . What about the blocks in the Stan file? You will want to check out the reference here on this. But, in short: . Variables in the data block are where variables that are read in as data are declared. . Variables in the parameters block are where variables that are sampled by Stan&#39;s MCMC algorithm are declared. . The model block defines our model. Here we are using sampling notation to indicate our target variable, y, has a distribution corresponding with the right-hand side. In this case, it is normally distributed with a mean of X$ beta$ and a standard deviation of $ sigma$. We use this distribution because the major assumption is that the errors are normally distributed around a mean of zero and a standard deviation of $ sigma$. This is a direct consequence of that assumption, which again is described in more detail here. . The generated quantities block does not adjust the learned parameters. We use it here to make predictions on our training set (in order to get the R$^{2}$ metric on it) as well as predicting on our test set for evaluation. . Next we load and compile the Stan file as described above. . model = CmdStanModel(stan_file=&quot;model.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/model . Sampling . Now that we have compiled the Stan code, we can do sampling. The sample() method performs MCMC sampling in the following process for each chain: . Draw a value from the auxillary moment ($ rho$) distribution. | Use the leapfrog iterator to update $ rho$ and parameters $ theta$. | Measure the Hamiltonian of the new state. | Use Metroplis-Hastings to compare the new Hamiltonian and the previous Hamiltonian. Accept or reject. | Save parameters $ theta$. If accepted, the new state becomes the previous state, and the process is repeated. | For more details, see this section of the Stan reference manual. . fit = model.sample(data=data_dict) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:start chain 2 INFO:cmdstanpy:finish chain 1 INFO:cmdstanpy:start chain 3 INFO:cmdstanpy:finish chain 2 INFO:cmdstanpy:start chain 4 INFO:cmdstanpy:finish chain 3 INFO:cmdstanpy:finish chain 4 . Results . The following gives a summary of each parameter and of each sample in the generated quantities. . df_summary = fit.summary() . df_summary.head(10) . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ -114.21100 | 0.036858 | 1.292550 | -116.77600 | -113.86600 | -112.82800 | 1229.79 | 1646.82 | 1.003350 | . beta[1] -4.05444 | 0.022669 | 0.854733 | -5.45802 | -4.03662 | -2.65164 | 1421.68 | 1903.79 | 1.001790 | . beta[2] 1.20357 | 0.002490 | 0.092446 | 1.05324 | 1.20165 | 1.35652 | 1378.13 | 1845.47 | 1.002560 | . sigma 3.03967 | 0.006326 | 0.265224 | 2.63954 | 3.02231 | 3.51813 | 1758.07 | 2354.24 | 1.005430 | . y_train_pred[1] 10.04310 | 0.048922 | 3.045080 | 5.08214 | 10.09390 | 15.07960 | 3874.27 | 5188.06 | 1.000090 | . y_train_pred[2] 4.91045 | 0.046124 | 3.031110 | -0.03392 | 4.90147 | 10.01760 | 4318.69 | 5783.19 | 0.999968 | . y_train_pred[3] 2.61639 | 0.049174 | 3.104720 | -2.40680 | 2.60535 | 7.62474 | 3986.28 | 5338.05 | 0.999514 | . y_train_pred[4] 2.81861 | 0.048951 | 3.084930 | -2.28905 | 2.81794 | 8.05269 | 3971.58 | 5318.37 | 0.999946 | . y_train_pred[5] 3.90715 | 0.048595 | 3.050000 | -1.14219 | 3.96200 | 8.91277 | 3939.22 | 5275.04 | 1.000840 | . y_train_pred[6] 6.52387 | 0.048185 | 3.076980 | 1.54866 | 6.48814 | 11.58220 | 4077.75 | 5460.54 | 1.000270 | . Here are all the samples from all the chains: . df_samples = fit.get_drawset() . df_samples.head() . lp__ accept_stat__ stepsize__ treedepth__ n_leapfrog__ divergent__ energy__ beta.1 beta.2 sigma ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 -114.148 | 0.831116 | 0.25766 | 2.0 | 3.0 | 0.0 | 115.355 | -4.85164 | 1.33744 | 3.09002 | ... | -1.210940 | 1.053170 | 4.537430 | 7.98730 | -0.705749 | 1.595220 | 25.6248 | 4.227230 | 5.58821 | 3.552860 | . 1 -112.878 | 0.998939 | 0.25766 | 4.0 | 15.0 | 0.0 | 114.463 | -4.55329 | 1.25020 | 2.87571 | ... | 3.083260 | -1.882480 | 8.180960 | 3.82194 | 1.249710 | 5.641800 | 27.5605 | 5.707750 | 5.37388 | -0.430799 | . 2 -113.328 | 0.914456 | 0.25766 | 3.0 | 11.0 | 0.0 | 114.435 | -4.46449 | 1.20331 | 3.01202 | ... | 0.581293 | 2.478930 | 1.101010 | 0.65513 | -1.680730 | 5.911050 | 20.3241 | 7.291160 | -1.23554 | 1.861660 | . 3 -113.221 | 0.988468 | 0.25766 | 2.0 | 7.0 | 0.0 | 113.887 | -4.24389 | 1.25944 | 3.09645 | ... | 3.760380 | -1.938580 | -0.219719 | 3.71438 | 4.102900 | -0.409039 | 21.0914 | 0.180785 | 4.52826 | 5.056180 | . 4 -113.124 | 0.999014 | 0.25766 | 3.0 | 11.0 | 0.0 | 113.384 | -4.47776 | 1.21375 | 2.93290 | ... | 0.330510 | -0.159161 | 0.593078 | 2.24137 | -4.612920 | 8.434020 | 26.6029 | 4.814100 | 4.63058 | 0.060730 | . 5 rows × 107 columns . As expected, each parameter&#39;s conditional distribution is normal. The dashed lines in each plot separate out each chain that was sampled. . def plot_dist(df, param): _, ax = plt.subplots(1, 2, figsize=(12,4)) df[param].plot.density(ax=ax[0]) df[param].plot(ax=ax[1]) for i in [1000, 2000, 3000]: ax[1].axvline(i, color=&quot;black&quot;, ls=&quot;--&quot;) plt.tight_layout() . plot_dist(df_samples, &quot;beta.1&quot;) . plot_dist(df_samples, &quot;beta.2&quot;) . plot_dist(df_samples, &quot;sigma&quot;) . Model evalulation . The model is evaluated using the means drawn from the posterior predictive distribution and comparing those with the actual values. . y_test_pred = df_summary.loc[&quot;y_new[1]&quot;:,&quot;Mean&quot;].values y_train_pred = df_summary.loc[&quot;y_train_pred[1]&quot;:&quot;y_train_pred[72]&quot;,&quot;Mean&quot;].values . Here is the training set R$^{2}$: . r2_score(y_train, y_train_pred) . 0.7320518315770257 . Here is the test set R$^{2}$: . r2_score(y_test, y_test_pred) . 0.5909465603175588 . Inference: generated quantities in same file . We&#39;ve already made predictions both on the training and test sets using generated quantities block in our Stan code. . y_train_pred = df_samples.loc[:,&quot;y_train_pred.1&quot;:&quot;y_train_pred.72&quot;] y_test_pred = df_samples.loc[:, &quot;y_new.1&quot;:] . Note that we actually have 4,000 values for each example in our dataset. In the Bayesian point of view, one samples from a normal distribution with a mean of X$ beta$ and standard deviation $ sigma$ (where $ beta$ and $ sigma$ were previously learned in our MCMC sampling) repeatedly and then takes the mean of that as the prediction. This is the posterior predictive mean. . y_test_pred . y_new.1 y_new.2 y_new.3 y_new.4 y_new.5 y_new.6 y_new.7 y_new.8 y_new.9 y_new.10 ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 13.86100 | 1.21005 | 9.625080 | 4.729390 | -0.544915 | 0.275943 | 7.21201 | 14.85480 | 2.488320 | 6.791070 | ... | -1.210940 | 1.053170 | 4.537430 | 7.987300 | -0.705749 | 1.595220 | 25.6248 | 4.227230 | 5.58821 | 3.552860 | . 1 10.72420 | 5.77362 | 7.998240 | 0.747717 | 3.936850 | 2.179330 | 6.48505 | 11.97870 | 4.312120 | 0.084512 | ... | 3.083260 | -1.882480 | 8.180960 | 3.821940 | 1.249710 | 5.641800 | 27.5605 | 5.707750 | 5.37388 | -0.430799 | . 2 7.76517 | 3.02216 | 7.781840 | 3.474330 | -3.105730 | 3.564890 | 2.38618 | 10.98730 | 3.349940 | 3.133640 | ... | 0.581293 | 2.478930 | 1.101010 | 0.655130 | -1.680730 | 5.911050 | 20.3241 | 7.291160 | -1.23554 | 1.861660 | . 3 12.30780 | 3.88406 | 6.825290 | 4.074230 | 5.620910 | -0.675503 | 3.97121 | 8.01391 | 7.726350 | 9.733260 | ... | 3.760380 | -1.938580 | -0.219719 | 3.714380 | 4.102900 | -0.409039 | 21.0914 | 0.180785 | 4.52826 | 5.056180 | . 4 8.45001 | 2.21607 | 0.216976 | 4.339310 | 6.563980 | 1.841760 | 5.51146 | 3.46399 | 5.527170 | 5.700020 | ... | 0.330510 | -0.159161 | 0.593078 | 2.241370 | -4.612920 | 8.434020 | 26.6029 | 4.814100 | 4.63058 | 0.060730 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 3995 14.19680 | 2.34497 | 3.825380 | 3.743840 | 1.949190 | 2.986780 | 8.72715 | 5.91935 | 7.118190 | 0.103569 | ... | -3.229020 | 5.386930 | 4.616740 | -0.565607 | 3.032600 | 8.927550 | 20.1792 | 7.154070 | 6.74880 | 6.080720 | . 3996 14.01000 | 1.55671 | 3.771160 | 1.080360 | 1.433040 | -0.227276 | 4.22622 | 8.54655 | -0.506560 | 7.595700 | ... | 1.732380 | -2.236160 | 0.934191 | 2.504280 | -0.196429 | -1.523670 | 21.5919 | 7.178920 | 5.39910 | 10.539300 | . 3997 15.09980 | 3.37498 | 1.831390 | 5.347430 | 1.579670 | 2.814540 | 8.27955 | 5.60267 | 7.673680 | 7.833060 | ... | 5.634510 | 4.672510 | 4.120470 | 8.859920 | 0.074522 | 6.997540 | 18.9550 | 4.220880 | 7.30418 | 3.094680 | . 3998 11.22930 | 5.84753 | 10.554600 | 3.532750 | 6.731970 | 7.434950 | 8.11790 | 7.14329 | 2.872040 | 5.812240 | ... | 2.371080 | 0.504117 | 10.501900 | 5.091230 | 3.632820 | 8.373250 | 18.6725 | 0.765423 | 8.99127 | 11.096800 | . 3999 10.34890 | -2.07760 | 7.540430 | 8.331610 | 4.075010 | 0.086709 | 4.54213 | 8.53442 | 0.818825 | 8.080430 | ... | 7.450320 | 2.226980 | 3.540020 | 2.519050 | 4.806600 | 6.466590 | 21.2205 | 11.872100 | 6.79971 | 2.058750 | . 4000 rows × 25 columns . def plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test): sort_mask = np.argsort(X_train[:,1:].ravel()) plt.plot(X_test[:,1:].ravel(), np.mean(y_test_pred, axis=0), lw=0, marker=&#39;o&#39;, color=&#39;C3&#39;, label=&quot;Test post. pred. mean&quot;) plt.plot(X_test[:,1:].ravel(), y_test, lw=0, marker=&#39;o&#39;, color=&#39;C4&#39;, label=&quot;Test actual&quot;) y_upper = np.percentile(y_train_pred, 97.5, axis=0)[sort_mask] y_lower = np.percentile(y_train_pred, 2.5, axis=0)[sort_mask] plt.fill_between(X_train[:,1:].ravel()[sort_mask], y_lower, y_upper, color=&quot;C1&quot;, alpha=0.1) plt.plot(X_train[:,1:].ravel()[sort_mask], np.mean(y_train_pred, axis=0)[sort_mask], label=&quot;Train post. pred. mean&quot;, color=&quot;C1&quot;) plt.legend() plt.ylabel(&quot;Profit in $10,000s&quot;) plt.xlabel(&quot;Population of city in 10,000s&quot;) plt.show() . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: generated quantities in separate file . This method we take the learned parameters from MCMC sampling and use them as data inputs for another Stan file where we can generate our new quantities. Here are the contents of our new file. This time beta and sigma are in the data block since they are not going to be learned parameters. Additionally we have no model. The purpose of the file is simply to generate predictions on new data. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[K] beta; real&lt;lower=0&gt; sigma; } parameters { } model { } generated quantities { vector[N] y; for (n in 1:N) { y[n] = normal_rng(x[n]*beta, sigma); } } . Here are our learned parameters from our previous MCMC sampling; . beta, sigma = df_summary.loc[&quot;beta[1]&quot;:&quot;beta[2]&quot;,&quot;Mean&quot;].values, df_summary.loc[&quot;sigma&quot;, &quot;Mean&quot;] . Here is our data dictionary. We are making predictions on our test set. . data_dict = {&quot;N&quot;: N_test, &quot;K&quot;: K, &quot;x&quot;: X_test, &quot;beta&quot;: beta, &quot;sigma&quot;: sigma} . predict = CmdStanModel(stan_file=&quot;predict.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/predict . Now for the sampling. We&#39;re not doing any MCMC sampling here though; we&#39;re just generating new quantities. Note that fixed_param is defined as true. . predict_fit = predict.sample(data_dict, fixed_param=True) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:finish chain 1 . predict_fit.summary().head() . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ 0.00000 | NaN | 0.00000 | 0.000000 | 0.00000 | 0.00000 | NaN | NaN | NaN | . y[1] 11.34340 | 0.093924 | 3.02409 | 6.514670 | 11.39430 | 16.23390 | 1036.670 | 39543.5 | 0.999251 | . y[2] 3.77424 | 0.094478 | 3.00060 | -1.078830 | 3.74053 | 8.89671 | 1008.690 | 38476.2 | 0.999620 | . y[3] 5.93002 | 0.095088 | 3.02563 | 0.955412 | 5.87699 | 11.10540 | 1012.460 | 38619.9 | 0.999912 | . y[4] 3.50338 | 0.099465 | 3.01245 | -1.674940 | 3.52844 | 8.47930 | 917.268 | 34988.9 | 0.999657 | . predict_fit.get_drawset().head() . lp__ accept_stat__ y.1 y.2 y.3 y.4 y.5 y.6 y.7 y.8 ... y.16 y.17 y.18 y.19 y.20 y.21 y.22 y.23 y.24 y.25 . 0 0.0 | 0.0 | 7.99995 | 4.02805 | 4.52434 | 7.56454 | 2.061870 | -4.295880 | 6.82277 | 8.88117 | ... | 4.81480 | 4.879910 | 4.12725 | 1.185910 | 4.57499 | -0.188764 | 17.4764 | 9.35162 | 2.80647 | 6.34096 | . 1 0.0 | 0.0 | 12.44470 | 5.13540 | 3.81029 | 3.80978 | -0.279801 | 2.390610 | 8.55481 | 8.99120 | ... | -2.85740 | 0.542605 | 6.30213 | 2.025270 | 4.73859 | 2.231070 | 15.4247 | 6.97024 | 6.16453 | 2.42187 | . 2 0.0 | 0.0 | 8.09853 | 7.15246 | 4.79964 | 5.85013 | 1.736150 | 1.594300 | 2.85399 | 2.70258 | ... | 2.94002 | 3.204780 | 0.81901 | 0.682184 | 2.63511 | 3.665020 | 22.5558 | 9.04426 | 5.07745 | 7.77231 | . 3 0.0 | 0.0 | 6.35768 | 4.81829 | 12.50010 | 1.98751 | -1.146150 | -0.265364 | 3.94979 | 9.99191 | ... | 2.62187 | 5.881490 | 4.84833 | 2.318180 | 4.68611 | 3.150200 | 22.1192 | 3.43943 | 1.73796 | 1.03573 | . 4 0.0 | 0.0 | 8.95467 | 3.44642 | 9.28674 | 3.66615 | 2.768230 | 3.143930 | 7.46174 | 9.89008 | ... | 1.95629 | -0.167935 | 2.34372 | 1.869520 | 2.33215 | -0.267212 | 14.0674 | 4.03022 | 6.96531 | 3.16389 | . 5 rows × 27 columns . df_fit_samples = pd.DataFrame(predict_fit.sample[:,0,:], columns=predict_fit.column_names) . y_test_pred = df_fit_samples.loc[:, &quot;y.1&quot;:] . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: using numpy . Lastly, we don&#39;t actually need to use Stan to generate the posterior preditive distribution&#39;s mean. We can do that using numpy by writing the function ourself, which we do here. . y_test_pred = np.zeros((N_test, 1000)) for i in range(N_test): y_test_pred[i,:] = np.random.normal(np.matmul(X_test[i], beta), sigma, size=1000) . Note that we need to transpose our predictions to use our plot function from earlier. . plot_pred(X_train, X_test, y_train_pred, y_test_pred.transpose(), y_test) .",
            "url": "https://barnett.science/bayesian/2020/01/27/baysian-linear-regression.html",
            "relUrl": "/bayesian/2020/01/27/baysian-linear-regression.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Classification Decision Trees & Entropy",
            "content": "import graphviz import matplotlib.pyplot as plt import numpy as np from sklearn import tree from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier . scikit-learn now comes with a way to plot trees, but I prefer using graphviz so here is a quick function to plot a tree, which we monkey-patch into the DecisionTreeClassifier and DecisionTreeRegressor classes. . def display_tree(self): dot_data = tree.export_graphviz(self, out_file=None, filled=True, rounded=True, special_characters=True) graph = graphviz.Source(dot_data) return graph DecisionTreeClassifier.plot = display_tree . Classification . We&#39;ll use the Iris dataset as a quick way to discuss classification trees. To learn more about this dataset use help(load_iris). . iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) . Fitting . Decision trees search over all possible ways to split up features and find the split that is most informative about the target variable. The parent node splits into two child nodes based on this split. From there the children also split in the same manner until all leaves are pure, unless another stopping condition is specified. A leaf is a node that has no children. A pure leaf is a leaf with only one class of items in it. . Classifications trees split using the GINI impurity which is defined as: . $I_{G}(p) = sum_{i=1}^{J}p_{i}(1-p_{i})$ . Here $p_{i}$ is the probability of an item with label $i$ being chosen and $1 - p_{i}$ is the probability of a mistake in categorizing that item. $J$ is the number of classes. Gini reaches zero when all cases in the node fall into a single target category. . Alternatively, one can use information gain to decide where to split, where information gain is defined as the difference in entropy of the parent and the weighted sum of the entropies of the children. Entropy is defined as: . $H_(p) = - sum_{i=1}^{J}p_{i} log_{2}(p_{i})$ . Let&#39;s train a decision tree: . clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 3 ≤ 0.8 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 8 X 2 ≤ 5.05 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;8 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 X 1 ≤ 3.1 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 6 entropy = 0.0 samples = 3 value = [0, 0, 3] 5&#45;&gt;6 7 entropy = 0.0 samples = 1 value = [0, 1, 0] 5&#45;&gt;7 9 X 0 ≤ 6.5 entropy = 0.811 samples = 4 value = [0, 1, 3] 8&#45;&gt;9 12 entropy = 0.0 samples = 35 value = [0, 0, 35] 8&#45;&gt;12 10 entropy = 0.0 samples = 3 value = [0, 0, 3] 9&#45;&gt;10 11 entropy = 0.0 samples = 1 value = [0, 1, 0] 9&#45;&gt;11 Here&#39;s the training score which indeed shows the tree is perfect at classfying the flowers on the training set. This tends to result in overfitting to the training set. . clf.score(X_train, y_train) . 1.0 . This is an easy dataset to classify, so the overfitting is not evident here. . clf.score(X_test, y_test) . 0.9736842105263158 . There are 7 leaves in our tree. Note that the leaves do not have to be depicted at the bottom of the tree in the diagram. A leaf is just a node without any children and could be represented near the top of the tree. . clf.get_n_leaves() . 7 . Inference . Now that we have trained our model, we can perform inference. . When inference on new samples is performed, the sample simply is examined with the &quot;rules&quot; created by the feature splits. Starting from the topmost node (the root node) in our example above, if feature three has a value of less than or equal 0.8, go to the left child node; otherwise go to the right. This process continues all the way down until the sample is put into a leaf. . The predicted class is the class in the leaf with the highest probability of that class for that leaf. In other words, simply break down the training samples by class within that leaf and choose the class with the most number of train samples. The probability of choosing that class is simply the number of training samples in that leaf belonging to that class divided by the total number of training samples in that leaf. . Since all of our leafs are pure, the classifier will always give 100% for its predictions. We&#39;ll rexamine this when we have impure leafs below. . Here are the features for the first test sample. . X_test[0] . array([5.8, 2.8, 5.1, 2.4]) . The path for this sample follows down the right side of the tree. Note that features are zero-indexed. . Root node: Is 2.4 &lt;= 0.8? No, so go right. . Is 5.1 &lt;= 4.95? No, so go right. . Is 5.1 &lt;= 5.05? No, so go right. . That brings it to the leaf on the right with 35 samples, where the 3rd class (index 2) is predicted: . clf.predict([X_test[0]]) . array([2]) . Again, the probability is 100% since there are no training samples in that leaf from the other two classes. In the plot the node is colored dark purple. For this tree purple represents the 3rd class and the darker the shade the more probable it is. . clf.predict_proba([X_test[0]]) . array([[0., 0., 1.]]) . Pruning . One way to prevent overfitting is the pre-prune the tree by specifying the maximum depth and/or maximum number of leaves. Here we set the maximum depth to 3. . clf = DecisionTreeClassifier(max_depth=3, random_state=0, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 3 ≤ 0.8 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 6 X 2 ≤ 5.05 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;6 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 7 entropy = 0.811 samples = 4 value = [0, 1, 3] 6&#45;&gt;7 8 entropy = 0.0 samples = 35 value = [0, 0, 35] 6&#45;&gt;8 clf.score(X_train, y_train) . 0.9821428571428571 . clf.score(X_test, y_test) . 0.9736842105263158 . Here is a sample where we are only 75% sure that it is class 2, since only 3 of the 4 samples in its leaf are class 2. . clf.predict([X_test[20]]) . array([2]) . clf.predict_proba([X_test[20]]) . array([[0. , 0.25, 0.75]]) . Information gain &amp; splits . Let&#39;s talk a little bit more about how trees use entropy (or alternatively Gini) to determine splits. . Information gain is calculated by cycling through all possible splits in the training set. Practically this is the process: . Select the first feature. | Pick the halfway point between the first sample and the second sample. | Calculate the entropy of the two child nodes if a split is made at that point. | Repeat steps 2 and 3 for all midpoints for this feature. | Go back to step 1 and repeat for all features. | At the end, pick the feature and split that has the lowest sum entropy for the two child nodes. . Again, information gain is the difference in entropy of the parent node and the summation of entropies of the two child nodes. Since the entropy of the parent node is the same for each potential split that we try, we only need to calculate the entropies of the child nodes for a split and find the split that minimizes that since that will maximize the information gained. . Here&#39;s a tree with a max depth of 1 using entropy to split: . clf = DecisionTreeClassifier(random_state=42, max_depth=1, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 2 ≤ 2.35 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False Let&#39;s implement our entropy calculation. This takes a list of targets and calculates the entropy for that node. . from collections import Counter def entropy(values): total = values.shape[0] c = Counter(values) if total == 0: return 0 s = 0 for x in c.values(): p = x/total s += p * np.log2(p) return -s . Here&#39;s our original list of targets: . y_train . array([1, 1, 2, 0, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0]) . Here&#39;s the associated entropy which matches above. This is the entropy of the root node. . entropy(y_train) . 1.5807197138422104 . Now let&#39;s get the entropes of the child nodes for the split that was found. In this case it was feature 3 with the split at 0.8. . feat = 3 x = 0.8 . Here are the samples that went into the left node. As you can see all of them are of class 0. There is no entropy associated with this node - the node is pure. . left = y_train[X_train[:,feat] &lt;= x] left . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . entropy(left) . -0.0 . Here are the samples that went into the right node: . right = y_train[X_train[:,feat] &gt; x] right . array([1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2]) . entropy(right) . 0.993707106604508 . Now we need the cycle through each feature, and then cycle through each possible split. . import math def calc_entropies(X, y): n = y.shape[0] m = X.shape[1] min_ents = np.zeros(m) min_splits = np.zeros(m) for feat in range(m): # Use set to remove dups; sort it to get halfway points points = sorted(list(set(X[:,feat]))) splits = [(points[i-1]+points[i])/2. for i in range(1, len(points))] entropies = [] for x in splits: l = y_train[X_train[:,feat] &lt;= x] r = y_train[X_train[:,feat] &gt; x] e = (l.shape[0]*entropy(l) + r.shape[0]*entropy(r)) / n entropies.append(e) feat_min_ent = np.argmin(entropies) min_ents[feat] = entropies[feat_min_ent] min_splits[feat] = splits[feat_min_ent] min_feat = np.argmin(min_ents) return min_feat, min_splits[min_feat], min_ents[min_feat] . calc_entropies(X_train, y_train) . (2, 2.35, 0.6654288660298044) . Note that in this example splitting feature 3 at 0.8 gives the same entropy as the above split. np.argmin returns the argument of the first minimum in the case of a tie. .",
            "url": "https://barnett.science/machinelearning/2019/05/14/decision-trees.html",
            "relUrl": "/machinelearning/2019/05/14/decision-trees.html",
            "date": " • May 14, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I solve business problems through the end-to-end implementation of machine learning products. . I received a BS in Mechanical Engineering from Mississippi State University. Later, I received a PhD in Chemical &amp; Molecular Engineering from Tulane University my research utilized programming, data science, and molecular simulations to study self-assembling molecules. After that I was a postdoctoral research scientist at Columbia University where I studied polymer interactions and gas separation membranes using molecular simulations and machine learning. . More recently I was a fellow with Insight Data Science where I created a Chrome Extension to predict where a user would post their content. I currently am in a senior machine learning engineer role at a financial company. .",
          "url": "https://barnett.science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://barnett.science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}