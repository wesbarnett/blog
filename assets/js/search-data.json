{
  
    
        "post0": {
            "title": "nbdev + GitHub Codespaces: A New Literate Programming Environment",
            "content": "Today, we are going to show you how to set up a literate programming environment, allowing you to use an IDE (VS Code) and an interactive computing environment (Jupyter), without leaving your browser, for free, in under 5 minutes. You’ll even see how VSCode and Jupyter work together automatically! But first, what is literate programming? And how did I go from skeptic to a zealot of literate programming? . Introduction . Literate programming is a programming paradigm introduced by Donald Knuth in which a computer program is given an explanation of its logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which compilable source code can be generated. According to Knuth, literate programming provides higher-quality programs by forcing programmers to explicitly state the thoughts behind the program. This process makes poorly thought-out design decisions more obvious. Knuth also claims that literate programming provides a first-rate documentation system, which is not an add-on, but is grown naturally in the process of exposition of one’s thoughts during a program’s creation. 1 . When I first learned about literate programming, I was quite skeptical. For the longest time, I had wrongly equated Jupyter notebooks with literate programming. Indeed, Jupyter is a brilliant interactive computing system, which was awarded the Association of Computing Machinery (ACM) Software System Award, and is loved by many developers. However, Jupyter falls short of the literate programming paradigm for the following reasons:2 . It can be difficult to compile source code from notebooks. | It can be difficult to diff and use version control with notebooks because they are not stored in plain text. | It is not clear how to automatically generate documentation from notebooks. | It is not clear how to properly run tests suites when writing code in notebooks. | . My skepticism quickly evaporated when I began using nbdev, a project that extends notebooks to complete the literate programming ideal. I spent a month, full time, using nbdev while contributing to the python library fastcore, and can report that Donald Knuth was definitely onto something. The process of writing prose and tests alongside code forced me to deeply understand why the code does what it does, and to think deeply about its design. Furthermore, the reduced cognitive load and speed of iteration of having documentation, code, and tests in one location boosted my productivity to levels I have never before experienced as a software developer. Furthermore, I found that developing this way bolstered collaboration such that code reviews not only happened faster but were more meaningful. In short, nbdev may be the most profound productivity tool I have ever used. . As a teaser, look how easy it is to instantiate this literate programming environment, which includes a notebook, a docs site and an IDE with all dependencies pre-installed! :point_down: . . Features of nbdev . As discussed in the docs, nbdev provides the following features: . Searchable, hyperlinked documentation, which can be automatically hosted on GitHub Pages for free. | Python modules, following best practices such as automatically defining __all__ with your exported functions, classes, and variables. | Pip and Conda installers. | Tests defined directly in notebooks which run in parallel. This testing system has been thoroughly tested with GitHub Actions. | Navigate and edit your code in a standard text editor or IDE, and export any changes automatically back into your notebooks. | . Since you are in a notebook, you can also add charts, text, links, images, videos, etc, that are included automatically in the documentation of your library, along with standardized documentation generated automatically from your code. This site is an example of docs generated automatically by nbdev. . GitHub Codespaces . Thanks to Conda and nbdev_template, setting up a development environment with nbdev is far easier than it used to be. However, we realized it could be even easier, thanks to a new GitHub product called Codespaces. Codespaces is a fully functional development environment in your browser, accessible directly from GitHub, that provides the following features: . A full VS Code IDE. | An environment that has files from the repository mounted into the environment, along with your GitHub credentials. | A development environment with dependencies pre-installed, backed by Docker. | The ability to serve additional applications on arbitrary ports. For nbdev, we serve a Jupyter notebook server as well as a Jekyll based documentation site. | A shared file system, which facilitates editing code in one browser tab and rendering the results in another. | … and more. | Codespaces enables developers to immediately participate in a project without wasting time on DevOps or complicated setup steps. Most importantly, CodeSpaces with nbdev allows developers to quickly get started with creating their own software with literate programming. . A demo of nbdev + Codespaces . This demo uses the project fastai/fastcore, which was built with nbdev, as an example. First, we can navigate to this repo and launch a Codespace: . . If you are launching a fresh Codespace, it may take several minutes to set up. Once the environment is ready, we can verify that all dependencies we want are installed (in this case fastcore and nbdev): . . Additionally, we can serve an arbitrary number of applications on user-specified ports, which we can open through VSCode as shown below: . . In this case, these applications are a notebook and docs site. Changes to a notebook are reflected immediately in the data docs. Furthermore, we can use the cli command nbdev_build_lib to sync our notebooks with python modules. This functionality is shown below: . . This is amazing! With a click of a button, I was able to: . Launch an IDE with all dependencies pre-installed. | Launch two additional applications: a Jupyter Notebook server on port 8080 and a docs site on port 4000. | Automatically update the docs and modules every time I make a change to a Jupyter notebook. | This is just the tip of the iceberg. There are additional utilities for writing and executing tests, diffing notebooks, special flags for hiding, showing, and collapsing cells in the generated docs, as well as git hooks for automation. This and more functionality is covered in the nbdev docs. . Give It A Try For Yourself . To try out nbdev yourself, take this tutorial, which will walk you through everything you need to know. The tutorial also shows you how to use a repository template with the configuration files necessary to enable Codespaces with nbdev. . You Can Write Blogs With Notebooks, Too! . This blog post was written in fastpages which is also built on nbdev! We recommend fastpages if you want an easy way to blog with Jupyter notebooks. . Additional Resources . The GitHub Codepaces site. | The official docs for Codespaces. | The nbdev docs. | The nbdev GitHub repo. | fastpages: The project used to write this blog. | The GitHub repo fastai/fastcore, which is what we used in this blog post as an example. | . Wikipedia article: Literate Programming &#8617; . | This is not a criticism of Jupyter. Jupyter doesn’t claim to be a full literate programming system. However, people can sometimes (unfairly) judge Jupyter according to this criteria. &#8617; . |",
            "url": "https://barnett.science/codespaces",
            "relUrl": "/codespaces",
            "date": " • Dec 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Introducing fastlinkcheck",
            "content": ". Motivation . Recently, fastai has been hard at work improving and overhauling nbdev, a literate programming environment for python. A key feature of nbdev is automated generation of documentation from Jupyter notebooks. This documentation system adds many niceties, such as the following types of hyperlinks automatically: . Links to source code on GitHub. | Links to both internal and external documentation by introspecting variable names in backticks. | . Because documentation is so easy to create and maintain in nbdev, we find ourselves and others creating much more of it! In addition to automatic hyperlinks, we often include our own links to relevant websites, blogs and videos when documenting code. For example, one of the largest nbdev generated sites, docs.fast.ai, has more than 300 external and internal links at the time of this writing. . The Solution . Due to the continued popularity of fastai and the growth of new nbdev projects, grooming these links manually became quite tedious. We investigated solutions that could verify links for us automatically, but were not satisfied with any existing solutions. These are the features we desired: . A platform independent solution that is not tied to a specific static site generator like Jekyll or Hugo. | Intelligent introspection of external links that are actually internal links. For example, if we are building the site docs.fast.ai, a link to https://docs.fast.ai/tutorial should not result in a web request, but rather introspection of the local file system for the presence of tutorial.html in the right location. | Verification of any links to assets like CSS, data, javascript or other files. | Logs that are well organized that allow us to see each broken link or reference to a non-existent path, and the pages these are found in. | Parallelism to verify links as fast as possible. | Lightweight, easy to install with minimal dependencies. | . We tried tools such as linkchecker and pylinkvalidator, but these required your site to be first be hosted. Since we wanted to check links on a static site, hosting is overhead we wanted to avoid. . This is what led us to create fastlinkcheck, which we discuss below. . Note: For Ruby users, htmlproofer apperas to provide overlapping functionality. We have not tried this library. . A tour of fastlinkcheck . For this tour we will be referring to the files in the fastlinkcheck repo. You should clone this repo in the current directory in order to follow along: . git clone https://github.com/fastai/fastlinkcheck.git cd fastlinkcheck . Cloning into &#39;fastlinkcheck&#39;... remote: Enumerating objects: 135, done. remote: Counting objects: 100% (135/135), done. remote: Compressing objects: 100% (98/98), done. remote: Total 608 (delta 69), reused 76 (delta 34), pack-reused 473 Receiving objects: 100% (608/608), 1.12 MiB | 10.47 MiB/s, done. Resolving deltas: 100% (302/302), done. . Installation . You can install fastlinkcheck with pip: . pip install fastlinkcheck . Usage . After installing fastlinkcheck, the cli command link_check is available from the command line. We can see various options with the --help flag. . link_check --help . usage: link_check [-h] [--host HOST] [--config_file CONFIG_FILE] [--pdb] [--xtra XTRA] path Check for broken links recursively in `path`. positional arguments: path Root directory searched recursively for HTML files optional arguments: -h, --help show this help message and exit --host HOST Host and path (without protocol) of web server --config_file CONFIG_FILE Location of file with urls to ignore --pdb Run in pdb debugger (default: False) --xtra XTRA Parse for additional args (default: &#39;&#39;) . From the root of fastlinkcheck repo, We can search the directory _example/broken_links recursively for broken links like this: . link_check _example/broken_links . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Specifying the --host parameter allows you detect links that are internal by identifying links with that host name. External links are verified by making a request to the appropriate website. On the other hand, internal links are verified by inspecting the presence and content of local files. . We must be careful when using the --host argument to only pass the host (and path, if necessary) without the protocol. For example, this is how we specify the hostname if your site&#39;s url is http://fastlinkcheck.com/test.html: . link_check _example/broken_links --host fastlinkcheck.com . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . We now have one less broken link as there is indeed a file named test.html in the root of the path we are searching. However, if we add a path to the end of --host , such as fastlinkcheck.com/mysite the link would again be listed as broken because _example/broken_links/mysite/test.html does not exist: . link_check _example/broken_links --host fastlinkcheck.com/mysite . ERROR: The Following Broken Links or Paths were found: - &#39;http://fastlinkcheck.com/test.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` - Path(&#39;/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.js&#39;) was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . You can ignore links by creating a text file that contains a list of urls and paths to ignore. For example, the file _example/broken_links/linkcheck.rc contains: . cat _example/broken_links/linkcheck.rc . test.js https://www.google.com . We can use this file to ignore urls and paths with the --config_file argument. This will filter out references to the broken link /test.js from our earlier results: . link_check _example/broken_links --host fastlinkcheck.com --config_file _example/broken_links/linkcheck.rc . ERROR: The Following Broken Links or Paths were found: - &#39;http://somecdn.com/doesntexist.html&#39; was found in the following pages: - `/Users/hamelsmu/github/fastlinkcheck/_example/broken_links/test.html` . . Finally, if there are no broken links, link_check will not return anything. The directory _example/no_broken_links/ does not contain any HTML files with broken links: . link_check _example/no_broken_links . No broken links found! . Python . You can also use these utilities from python instead of the terminal. Please see these docs for more information. . Using link_check in GitHub Actions . The link_check CLI utility that is installed with fastlinkcheck can be very useful in continuous integration systems like GitHub Actions. Here is an example GitHub Actions workflow that uses link_check: . name: Check Links on: [workflow_dispatch, push] jobs: check-links: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/setup-python@v2 - name: check for broken links run: | pip install fastlinkcheck link_check _example . We can a few more lines of code to open an issue instead when a broken link is found, using the gh cli: . ... - name: check for broken links run: | pip install fastlinkcheck link_check _example 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; [[ -s err ]] &amp;&amp; gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -R &quot;yourusername/yourrepo&quot; . We can extend this even further to only open an issue when another issue with a specific label isn&#39;t already open: . ... - name: check for broken links run: | pip install fastlinkcheck link_check &quot;docs/_site&quot; --host &quot;docs.fast.ai&quot; 2&gt; err || true export GITHUB_TOKEN=&quot;YOUR_TOKEN&quot; if [[ -z $(gh issue list -l &quot;broken-link&quot;)) &amp;&amp; (-s err) ]]; then gh issue create -t &quot;Broken links found&quot; -b &quot;$(&lt; err)&quot; -l &quot;broken-link&quot; -R &quot;yourusername/yourrepo&quot; fi . See the GitHub Actions docs for more information. . Resources . The following resources are relevant for those interested in learning more about fastlinkcheck: . The fastlinkcheck GitHub repo | The fastlinkcheck docs | .",
            "url": "https://barnett.science/fastlinkcheck/",
            "relUrl": "/fastlinkcheck/",
            "date": " • Nov 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastcore: An Underrated Python Library",
            "content": ". Background . I recently embarked on a journey to sharpen my python skills: I wanted to learn advanced patterns, idioms, and techniques. I started with reading books on advanced Python, however, the information didn&#39;t seem to stick without having somewhere to apply it. I also wanted the ability to ask questions from an expert while I was learning -- which is an arrangement that is hard to find! That&#39;s when it occurred to me: What if I could find an open source project that has fairly advanced python code and write documentation and tests? I made a bet that if I did this it would force me to learn everything very deeply, and the maintainers would be appreciative of my work and be willing to answer my questions. . And that&#39;s exactly what I did over the past month! I&#39;m pleased to report that it has been the most efficient learning experience I&#39;ve ever experienced. I&#39;ve discovered that writing documentation forced me to deeply understand not just what the code does but also why the code works the way it does, and to explore edge cases while writing tests. Most importantly, I was able to ask questions when I was stuck, and maintainers were willing to devote extra time knowing that their mentorship was in service of making their code more accessible! It turns out the library I choose, fastcore is some of the most fascinating Python I have ever encountered as its purpose and goals are fairly unique. . For the uninitiated, fastcore is a library on top of which many fast.ai projects are built on. Most importantly, fastcore extends the python programming language and strives to eliminate boilerplate and add useful functionality for common tasks. In this blog post, I&#39;m going to highlight some of my favorite tools that fastcore provides, rather than sharing what I learned about python. My goal is to pique your interest in this library, and hopefully motivate you to check out the documentation after you are done to learn more! . Why fastcore is interesting . Get exposed to ideas from other languages without leaving python: I’ve always heard that it is beneficial to learn other languages in order to become a better programmer. From a pragmatic point of view, I’ve found it difficult to learn other languages because I could never use them at work. Fastcore extends python to include patterns found in languages as diverse as Julia, Ruby and Haskell. Now that I understand these tools I am motivated to learn other languages. | You get a new set of pragmatic tools: fastcore includes utilities that will allow you to write more concise expressive code, and perhaps solve new problems. | Learn more about the Python programming language: Because fastcore extends the python programming language, many advanced concepts are exposed during the process. For the motivated, this is a great way to see how many of the internals of python work. | A whirlwind tour through fastcore . Here are some things you can do with fastcore that immediately caught my attention. . . Making **kwargs transparent . Whenever I see a function that has the argument **kwargs, I cringe a little. This is because it means the API is obfuscated and I have to read the source code to figure out what valid parameters might be. Consider the below example: . def baz(a, b=2, c =3, d=4): return a + b + c def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, **kwargs)&gt; . Without reading the source code, it might be hard for me to know that foo also accepts and additional parameters b and d. We can fix this with delegates: . def baz(a, b=2, c =3, d=4): return a + b + c @delegates(baz) # this decorator will pass down keyword arguments from baz def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4)&gt; . You can customize the behavior of this decorator. For example, you can have your cake and eat it too by passing down your arguments and also keeping **kwargs: . @delegates(baz, keep=True) def foo(c, a, **kwargs): return c + baz(a, **kwargs) inspect.signature(foo) . &lt;Signature (c, a, b=2, d=4, **kwargs)&gt; . You can also exclude arguments. For example, we exclude argument d from delegation: . def basefoo(a, b=2, c =3, d=4): pass @delegates(basefoo, but= [&#39;d&#39;]) # exclude `d` def foo(c, a, **kwargs): pass inspect.signature(foo) . &lt;Signature (c, a, b=2)&gt; . You can also delegate between classes: . class BaseFoo: def __init__(self, e, c=2): pass @delegates()# since no argument was passsed here we delegate to the superclass class Foo(BaseFoo): def __init__(self, a, b=1, **kwargs): super().__init__(**kwargs) inspect.signature(Foo) . &lt;Signature (a, b=1, c=2)&gt; . For more information, read the docs on delegates. . . Avoid boilerplate when setting instance attributes . Have you ever wondered if it was possible to avoid the boilerplate involved with setting attributes in __init__? . class Test: def __init__(self, a, b ,c): self.a, self.b, self.c = a, b, c . Ouch! That was painful. Look at all the repeated variable names. Do I really have to repeat myself like this when defining a class? Not Anymore! Checkout store_attr: . class Test: def __init__(self, a, b, c): store_attr() t = Test(5,4,3) assert t.b == 4 . You can also exclude certain attributes: . class Test: def __init__(self, a, b, c): store_attr(but=[&#39;c&#39;]) t = Test(5,4,3) assert t.b == 4 assert not hasattr(t, &#39;c&#39;) . There are many more ways of customizing and using store_attr than I highlighted here. Check out the docs for more detail. . P.S. you might be thinking that Python dataclasses also allow you to avoid this boilerplate. While true in some cases, store_attr is more flexible.1 . 1. For example, store_attr does not rely on inheritance, which means you won&#39;t get stuck using multiple inheritance when using this with your own classes. Also, unlike dataclasses, store_attr does not require python 3.7 or higher. Furthermore, you can use store_attr anytime in the object lifecycle, and in any location in your class to customize the behavior of how and when variables are stored.↩ . . Avoiding subclassing boilerplate . One thing I hate about python is the __super__().__init__() boilerplate associated with subclassing. For example: . class ParentClass: def __init__(self): self.some_attr = &#39;hello&#39; class ChildClass(ParentClass): def __init__(self): super().__init__() cc = ChildClass() assert cc.some_attr == &#39;hello&#39; # only accessible b/c you used super . We can avoid this boilerplate by using the metaclass PrePostInitMeta. We define a new class called NewParent that is a wrapper around the ParentClass: . class NewParent(ParentClass, metaclass=PrePostInitMeta): def __pre_init__(self, *args, **kwargs): super().__init__() class ChildClass(NewParent): def __init__(self):pass sc = ChildClass() assert sc.some_attr == &#39;hello&#39; . . Type Dispatch . Type dispatch, or Multiple dispatch, allows you to change the way a function behaves based upon the input types it receives. This is a prominent feature in some programming languages like Julia. For example, this is a conceptual example of how multiple dispatch works in Julia, returning different values depending on the input types of x and y: . collide_with(x::Asteroid, y::Asteroid) = ... # deal with asteroid hitting asteroid collide_with(x::Asteroid, y::Spaceship) = ... # deal with asteroid hitting spaceship collide_with(x::Spaceship, y::Asteroid) = ... # deal with spaceship hitting asteroid collide_with(x::Spaceship, y::Spaceship) = ... # deal with spaceship hitting spaceship . Type dispatch can be especially useful in data science, where you might allow different input types (i.e. Numpy arrays and Pandas dataframes) to a function that processes data. Type dispatch allows you to have a common API for functions that do similar tasks. . Unfortunately, Python does not support this out-of-the box. Fortunately, there is the @typedispatch decorator to the rescue. This decorator relies upon type hints in order to route inputs the correct version of the function: . @typedispatch def f(x:str, y:str): return f&#39;{x}{y}&#39; @typedispatch def f(x:np.ndarray): return x.sum() @typedispatch def f(x:int, y:int): return x+y . Below is a demonstration of type dispatch at work for the function f: . f(&#39;Hello &#39;, &#39;World!&#39;) . &#39;Hello World!&#39; . f(2,3) . 5 . f(np.array([5,5,5,5])) . 20 . There are limitations of this feature, as well as other ways of using this functionality that you can read about here. In the process of learning about typed dispatch, I also found a python library called multipledispatch made by Mathhew Rocklin (the creator of Dask). . After using this feature, I am now motivated to learn languages like Julia to discover what other paradigms I might be missing. . . A better version of functools.partial . functools.partial is a great utility that creates functions from other functions that lets you set default values. Lets take this function for example that filters a list to only contain values &gt;= val: . test_input = [1,2,3,4,5,6] def f(arr, val): &quot;Filter a list to remove any values that are less than val.&quot; return [x for x in arr if x &gt;= val] f(test_input, 3) . [3, 4, 5, 6] . You can create a new function out of this function using partial that sets the default value to 5: . filter5 = partial(f, val=5) filter5(test_input) . [5, 6] . One problem with partial is that it removes the original docstring and replaces it with a generic docstring: . filter5.__doc__ . &#39;partial(func, *args, **keywords) - new function with partial application n of the given arguments and keywords. n&#39; . fastcore.utils.partialler fixes this, and makes sure the docstring is retained such that the new API is transparent: . filter5 = partialler(f, val=5) filter5.__doc__ . &#39;Filter a list to remove any values that are less than val.&#39; . . Composition of functions . A technique that is pervasive in functional programming languages is function composition, whereby you chain a bunch of functions together to achieve some kind of result. This is especially useful when applying various data transformations. Consider a toy example where I have three functions: (1) Removes elements of a list less than 5 (from the prior section) (2) adds 2 to each number (3) sums all the numbers: . def add(arr, val): return [x + val for x in arr] def arrsum(arr): return sum(arr) # See the previous section on partialler add2 = partialler(add, val=2) transform = compose(filter5, add2, arrsum) transform([1,2,3,4,5,6]) . 15 . But why is this useful? You might me thinking, I can accomplish the same thing with: . arrsum(add2(filter5([1,2,3,4,5,6]))) . You are not wrong! However, composition gives you a convenient interface in case you want to do something like the following: . def fit(x, transforms:list): &quot;fit a model after performing transformations&quot; x = compose(*transforms)(x) y = [np.mean(x)] * len(x) # its a dumb model. Don&#39;t judge me return y # filters out elements &lt; 5, adds 2, then predicts the mean fit(x=[1,2,3,4,5,6], transforms=[filter5, add2]) . [7.5, 7.5] . For more information about compose, read the docs. . . A more useful __repr__ . In python, __repr__ helps you get information about an object for logging and debugging. Below is what you get by default when you define a new class. (Note: we are using store_attr, which was discussed earlier). . class Test: def __init__(self, a, b=2, c=3): store_attr() # `store_attr` was discussed previously Test(1) . &lt;__main__.Test at 0x7ffcd766cee0&gt; . We can use basic_repr to quickly give us a more sensible default: . class Test: def __init__(self, a, b=2, c=3): store_attr() __repr__ = basic_repr(&#39;a,b,c&#39;) Test(2) . Test(a=2, b=2, c=3) . . Monkey Patching With A Decorator . It can be convenient to monkey patch with a decorator, which is especially helpful when you want to patch an external library you are importing. We can use the decorator @patch from fastcore.foundation along with type hints like so: . class MyClass(int): pass @patch def func(self:MyClass, a): return self+a mc = MyClass(3) . Now, MyClass has an additional method named func: . mc.func(10) . 13 . Still not convinced? I&#39;ll show you another example of this kind of patching in the next section. . . A better pathlib.Path . When you see these extensions to pathlib.path you won&#39;t ever use vanilla pathlib again! A number of additional methods have been added to pathlib, such as: . Path.readlines: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.readlines() | Path.read: same as with open(&#39;somefile&#39;, &#39;r&#39;) as f: f.read() | Path.save: saves file as pickle | Path.load: loads pickle file | Path.ls: shows the contents of the path as a list. | etc. | . Read more about this here. Here is a demonstration of ls: . from fastcore.utils import * from pathlib import Path p = Path(&#39;.&#39;) p.ls() # you don&#39;t get this with vanilla Pathlib.Path!! . (#7) [Path(&#39;2020-09-01-fastcore.ipynb&#39;),Path(&#39;README.md&#39;),Path(&#39;fastcore_imgs&#39;),Path(&#39;2020-02-20-test.ipynb&#39;),Path(&#39;.ipynb_checkpoints&#39;),Path(&#39;2020-02-21-introducing-fastpages.ipynb&#39;),Path(&#39;my_icons&#39;)] . Wait! What&#39;s going on here? We just imported pathlib.Path - why are we getting this new functionality? Thats because we imported the fastcore.utils module, which patches this module via the @patch decorator discussed earlier. Just to drive the point home on why the @patch decorator is useful, I&#39;ll go ahead and add another method to Path right now: . @patch def fun(self:Path): return &quot;This is fun!&quot; p.fun() . &#39;This is fun!&#39; . That is magical, right? I know! That&#39;s why I&#39;m writing about it! . . An Even More Concise Way To Create Lambdas . Self, with an uppercase S, is an even more concise way to create lambdas that are calling methods on an object. For example, let&#39;s create a lambda for taking the sum of a Numpy array: . arr=np.array([5,4,3,2,1]) f = lambda a: a.sum() assert f(arr) == 15 . You can use Self in the same way: . f = Self.sum() assert f(arr) == 15 . Let&#39;s create a lambda that does a groupby and max of a Pandas dataframe: . import pandas as pd df=pd.DataFrame({&#39;Some Column&#39;: [&#39;a&#39;, &#39;a&#39;, &#39;b&#39;, &#39;b&#39;, ], &#39;Another Column&#39;: [5, 7, 50, 70]}) f = Self.groupby(&#39;Some Column&#39;).mean() f(df) . Another Column . Some Column . a 6 | . b 60 | . Read more about Self in the docs). . . Notebook Functions . These are simple but handy, and allow you to know whether or not code is executing in a Jupyter Notebook, Colab, or an Ipython Shell: . from fastcore.imports import in_notebook, in_colab, in_ipython in_notebook(), in_colab(), in_ipython() . (True, False, True) . This is useful if you are displaying certain types of visualizations, progress bars or animations in your code that you may want to modify or toggle depending on the environment. . . A Drop-In Replacement For List . You might be pretty happy with Python&#39;s list. This is one of those situations that you don&#39;t know you needed a better list until someone showed one to you. Enter L, a list like object with many extra goodies. . The best way I can describe L is to pretend that list and numpy had a pretty baby: . define a list (check out the nice __repr__ that shows the length of the list!) . L(1,2,3) . (#3) [1,2,3] . Shuffle a list: . p = L.range(20).shuffle() p . (#20) [8,7,5,12,14,16,2,15,19,6...] . Index into a list: . p[2,4,6] . (#3) [5,14,2] . L has sensible defaults, for example appending an element to a list: . 1 + L(2,3,4) . (#4) [1,2,3,4] . There is much more L has to offer. Read the docs to learn more. . But Wait ... There&#39;s More! . There are more things I would like to show you about fastcore, but there is no way they would reasonably fit into a blog post. Here is a list of some of my favorite things that I didn&#39;t demo in this blog post: . Utilities . The Basics section contain many shortcuts to perform common tasks or provide an additional interface to what standard python provides. . mk_class: quickly add a bunch of attributes to a class | wrap_class: add new methods to a class with a simple decorator | groupby: similar to Scala&#39;s groupby | merge: merge dicts | fasttuple: a tuple on steroids | Infinite Lists: useful for padding and testing | chunked: for batching and organizing stuff | . Multiprocessing . The Multiprocessing section extends python&#39;s multiprocessing library by offering features like: . progress bars | ability to pause to mitigate race conditions with external services | processing things in batches on each worker, ex: if you have a vectorized operation to perform in chunks | . Functional Programming . The functional programming section is my favorite part of this library. . maps: a map that also composes functions | mapped: A more robust map | using_attr: compose a function that operates on an attribute | . Transforms . Transforms is a collection of utilities for creating data transformations and associated pipelines. These transformation utilities build upon many of the building blocks discussed in this blog post. . Further Reading . It should be noted that you should read the main page of the docs first, followed by the section on tests to fully understand the documentation. . The fastcore documentation site. | The fastcore GitHub repo. | Blog post on delegation. | . Shameless plug: fastpages . This blog post was written entirely in a Jupyter Notebook, which GitHub automatically converted into to a blog post! Sound interesting? Check out fastpages. .",
            "url": "https://barnett.science/fastcore/",
            "relUrl": "/fastcore/",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Continuous deployment of a Flask server with Github Actions",
            "content": "In my previous post I covered how to setup an AWS EC2 instance with a bare bones Flask project. At the end of the post we automated this by using Ansible. In this post, we take it one step further and let Github use Ansible to provision and configure our EC2 instance. If you haven’t read the last post, I recommend at least skimming through it to get an idea of what we’re setting up here. . . Tip: You can actually use this Github Workflow and Action with any cloud provider or your own personal Linux server. You just need to ensure ports 80 and 443 are open and copy the public key to the remote server. Additionally you may need to change the ansible user. We want to use an Ansible playbook via Github Actions to provision and configure everything in our EC2 instance automatically so that whenever we push the master branch to Github it updates our deployment. The Ansible playbook, found here, installs the Ubuntu packages we need (pip, gunicorn, nginx); clones our Github repository and installs anything in requirements.txt; creates a systemd unit to run gunicorn and starts it; and creates an nginx configuration and starts it. The nginx configuration is just a reverse proxy from http://localhost:8080—where gunicorn is serving our Flask server—to port 80, the standard web server port. . . This is not intended as a large-scale solution, but if you’re running a small website or REST API that you would like to demo to people, this streamlines some of the process of continuously deploying. . Setup . Generate repository . First, generate a new repository based on my template by clicking here. The contents of the flask repository are discussed in the previous post here. . Go ahead and clone your repository to your local machine so you can make changes to the Flask server later. . To run the gunicorn server locally for testing, in it’s current state you need at least the gunicorn and flask packages. At the top level of the repository run: . gunicorn --chdir application -b :8080 app:app . The Flask application only has one route at / that simply prints “It works!”. If running locally you can go to localhost:8080 to see the default route. . . Tip: After you generate a new repository from the template, a pull request will be opened in the new repository also outlining these steps. Generate SSH key pair . Create an SSH key pair to be used between Github and AWS. Personally I prefer to do this locally on my machine using ssh-keypair, but you can use any third party tool. Don’t include a passphrase. . Copy private key to Github . Next you will need to copy and paste the private key into the secrets for your new repository that you generated. To do that, go to the settings for your repository and then go to “Secrets” on the left hand menu. Click “New Secret” and name your new secrets AWS_EC2_KEY. It must be named this since the value will be used in our Github Actions. Paste in the private key and save. The private key should begin with (OPENSSH might be SSH or RSA in your implementation): . --BEGIN OPENSSH PRIVATE KEY-- . . Note: Your private key is encrypted before it reaches Github and not decrypted until it is used in a workflow according to Github. Still, understand the security implications of sharing your private key with a third-party. . Copy public key to AWS EC2 . Next, copy the public key and import it into AWS. Go here and select the white “Actions” dropdown box. Click import key, name your key, and then either browse for the public key or paste it in. It doesn’t matter what you name your key here as long as you know it’s associated with your Github. . . Create EC2 instance and associate Elastic IP . After this create an EC2 instance and associate an Elastic IP address with this. If you don’t know how to do this, you can follow the section in my previous post here. Return here after you have created an instance and associated an Elastic IP with it. . . Warning: If you&#39;re just testing this out and not wanting to run your Flask site yet, be sure to stop your EC2 instance when you are finished testing. Create an A record for domain . Create an A record for your domain or subdomain for the IP address. I personally use namecheap.com. To add an A record for your domain or subdomain, follow this article. It may take a few minutes for DNS servers to be updated. . Update Ansible configuration . Lastly, in your repository update ansible/deploy/hosts for your own domain. In the file replace the instance of test.barnett.science with your domain name (you could actually list several domain names if you wanted to deploy to several different servers). Note that although Ansible allows IP addresses, the templates for this project expect this to be a domain name. You can also change the app_name from flask-project to whatever you desire, but it is not necessary. This is used as the systemd unit name that runs gunicorn as well as the directory of where the repository will be cloned. . ansible/deploy/hosts . [webservers] test.barnett.science [webservers:vars] app_name=flask-project ansible_ssh_user=ubuntu SSL=False . Commit the change and push to your Github repository. You should now be able to visit your domain and see the text “It works!”. . SSL . This step is optional for those who want to serve using HTTPS. Ensure that your site is served over HTTP before proceeding. . . Note: Although it is very simple to enable HTTPS, you can run into problems if trying to switch back to HTTP-only later. Specifically, web browsers that had been accessing the site via HTTPS will possibly now see the site as insecure and may not be able to access content. In other words, if choosing to serve over HTTPS, stick with it. To enable SSL for your site, ensure that port 443 is open in your EC2 security group for inbound connections. Then SSH into your EC2 instance and follow the instructions to use Let’s Encrypt’s certbot found here. . After this is complete, simply change SSL=True in ansible/deploy/hosts and push. This sets a boolean variable such that the nginx configuration for your domain is changed to serve using HTTPS. Additionally all HTTP traffic will be redirected to HTTPS. This is an important step, because if you do not change this variable the nginx configuration will be overwritten with one that only serves over HTTP the next time you push. . Next steps . Now that you have everything working, simply make updates to your code and push. It’s recommended you create another branch for development and only merge to master when it is ready for production. Anything pushed to the master branch will be automatically deployed to your EC2 instance, so be sure to test locally! When confident with the changes, merge with master and push. . If you make updates to the directory structure, like moving __init__.py to another location, you may break the Github action from working. It expects __init__.py to be under application/app. If you want it elsewhere, you’ll need to modify the Ansible templates yourself. Other than that, there shouldn’t be any restrictions on what you can change or add to your flask server (templates, static files, etc.). . You can add additional Python packages to the top-level requirements.txt and they will be installed automatically as part of the Ansible provisioning. . To learn more about how this repository is setup, see this section. . Lastly, you can always check how the Github action performed by going to the “Actions” tab in your new repository. Additionally you can still ssh into your EC2 instance and check the status of both the gunicorn service (flask-project.service by default) and nginx service (nginx.service) using systemd. Note that if you make any changes to flask-project.service or the nginx configuration you should do that in the ansible template in your repository, not in your instance directly; otherwise, any changes you make to those files will be overwritten by your ansible playbook when you push later. .",
            "url": "https://barnett.science/linux/aws/ansible/github/2020/05/28/flask-actions.html",
            "relUrl": "/linux/aws/ansible/github/2020/05/28/flask-actions.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Run a Flask app on AWS EC2",
            "content": "Let’s say you’ve created a Flask application and you’re ready to launch it to AWS. At this point you may not care about scaling yet, and you just want to get it in production so you can demo it to prospective clients or employers. How do you get that up and running quickly? . In this blog post we’ll set up a simple Flask project to be run on an AWS EC2 instance using gunicorn and nginx. . After walking through this manually, we’ll automate the setup using Ansible . Flask project setup . I’m assuming you have already created your Flask application and know a little bit about how to run it locally. I have a barebones Flask project located here that I am using as a model for this blog post. All of the routes/views are located in application/app/routes.py in the project. . If you end up using your own Flask project, be sure that the directory structure is the same in order to follow this post. Specifically, __init__.py should be in application/app and should have contents that indicate where the Flask routes are stored. For example, in the skeleton used above they are in routes.py, so the contents of __init__.py is: . from flask import Flask app = Flask(__name__) from app import routes . Here’s the directory structure: . application/ app/ __init__.py routes.py requirements.txt . Running locally . To run the repository locally, clone it to your machine: . git clone https://github.com/wesbarnett/flask-project . Then install the requirements. In this example I’m creating a virtual environment: . cd flask-project python3 -m venv venv source ./venv/bin/activate python -m pip install flask gunicorn . Then to run locally do: . gunicorn --chdir application -b :8080 app:app . Then visit http://localhost:8080 and you should see a line that says “It works!”. . AWS Setup . Create and launch EC2 instance . So now you have your Flask project and you’re ready to move it to an AWS EC2 instance. It’s time to login to your AWS console’s EC2 dashboard. From there click the big orange “Launch Instance” button. . . Let’s use an Ubuntu Server 18.04 LTS image, so select that on the next page. From there you can choose what instance type you want. If you are able, choose the free tier eligible one if just testing this out. . . After selecting what instance type go ahead and skip ahead to step 6 where you can configure your security group. Add a rule for port 80 and possibly 443 if you’re going to use HTTPS later, or choose a security group that already has those ports open. Simply choose “HTTP” under type and it will automatically ensure port 80 is open for all traffic. Otherwise, you won’t be able to access your application in a web browser. . . When you click “Review and launch” you will get a warning that you are allowing traffic from anywhere. Then click “Launch”. At this point you’ll be asked to choose or create a keypair that you will use when you ssh into your instance. . . Warning: Ensure you stop the instance when you are done! Allocate elastic IP address . Next we want an elastic IP address. The advantage of this is that if we bring an instance down and bring another one up, we can just move the IP address to be associated with that new instance. Additionally if you stop an instance and restart it, without an elastic IP address, you will have to find the new IP address of your instance. . . Under “Network &amp; Security” on the left hand menu click “Elastic IPs”. Then click the orange “Allocate Elastic IP Address” button on the top right. Leave the settings alone and click the orange “Allocate” button. . At the top you’ll see a green bar with a button that says “Associate this Elastic IP Address”. Click that. Now choose your running instance and associate the IP address with that instance. . Domain name . If you are going to use a domain or subdomain, go ahead create an A record for your domain using your elastic IP address. Personally I have been using namecheap.com as my registrar. Here is an article on how to add an A address record for namecheap. . Setup instance . Once your instance is running you’ll need to ssh into the instance. From a Linux machine this looks like this: . ssh -i ~/.ssh/aws.pem ubuntu@&lt;ip-address&gt; . Here ~/.ssh/aws.pem is my key that I associated with this instance. The IP address is the Elastic IP you allocated and associated above. . Now you’ll want to clone your Flask project to the instance. For my barebones project it would be: . git clone https://github.com/wesbarnett/flask-project . Install packages . We won’t be using a virtual environment on this instance. You’ll need to install a few Ubuntu packages next: . sudo apt-get update sudo apt-get install nginx gunicorn3 python3-pip python3-flask . For the barebones project this is all we need. . Test gunicorn . Now you should be able to test gunicorn. Note that the executable here is named gunicorn3. This runs the server in the background, makes a GET request to test it, and then puts the server back into foreground. You should see “It works!” when you run curl: . gunicorn3 --chdir application -b :8080 app:app &amp; curl http://localhost:8080 fg . gunicorn systemd unit . We don’t want to have to type in gunicorn3 manually to run our Flask server. Let’s have this occur automatically at boot by creating and enabling a systemd unit. Create a file with the following contents at the indicated location: . /etc/systemd/system/gunicorn.conf . [Unit] Description=gunicorn to serve flask-project After=network.target [Service] WorkingDirectory=/home/ubuntu/flask-project ExecStart=/usr/bin/gunicorn3 -b 0.0.0.0:8080 --chdir application app:app [Install] WantedBy=multi-user.target . Change flask-project to the name of your project in the WorkingDirectory above. . Now to run your gunicorn service do: . sudo systemctl start gunicorn . You should now be able to do curl http://localhost:8080 and get a message that says “It works!”. . To enable this to run on boot, do: . sudo systemctl enable gunicorn . If you ever want to restart the server, do: . sudo systemctl restart gunicorn . nginx configuration . We don’t want to expose gunicorn to the web directly. It’s not designed for taking direct traffic and susceptible to denial-of service attacks if you do that. Instead we’ll use nginx to take the traffic and setup a reverse proxy to gunicorn. . First remove the default configuration from being enabled: . sudo rm /etc/nginx/sites-enabled/default . Then create a new configuration file: . /etc/nginx/sites-available/flask-project.conf . server { listen 80; listen [::]:80; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_pass http://localhost:8080; } } . Now enable it by creating a symlink: . sudo ln -s /etc/nginx/sites-available/flask-project.conf /etc/nginx/sites-enabled/ . Finally, start nginx: . sudo systemctl start nginx . You should now be able to enter your Elastic IP address and domain name into a web browser and see your website! . To enable nginx on boot do: . sudo systemctl enable nginx . Whenever you make a change to your code, you’ll need to pull it in with git and restart the gunicorn service. . Automate setup with Ansible . Let’s make setting up our server a even easier using Ansible. For this section I recommend creating a new, clean instance. If you’re not using the previous instance from above simply terminate it and release it’s elastic IP address. You can then associate the Elastic IP address with this new instance if you want or create a new one. Simply go back and follow the AWS setup steps again. . Install Ansible to your local machine. Ansible is a remote configuration and provisioning tool. For Linux distributions simply use your package manager. For MacOS you can use Homebrew. I’m not sure what options are available for Windows. You don’t need to install anything on your AWS instance, not even Ansible! . Create playbook . Ansible consists of playbooks that you use to tell it what to do. These playbooks then refer to configuration file templates. On your local machine create a new directory to contain your playbook and templates: . mkdir aws-flask-playbook cd aws-flask-playbook . In this directory create an Ansible configuration file: . ansible.cfg . [defaults] host_key_checking = False inventory = ./deploy/hosts . Now create a directory named deploy and in it a file named hosts. Add your Elastic IP address under [webservers]. You also need to change the location of your private key file. github_user and app_name are used in the Github url as well as naming some of the files later. . deploy/hosts . [webservers] your-elastic-ip [webservers:vars] ansible_ssh_user=ubuntu ansible_ssh_private_key_file=/home/wes/.ssh/aws.pem github_user=wesbarnett app_name=flask-project domain_name=test.barnett.science . Then create a playbook: . deploy.yaml . # Ansible playbook for deploying a Flask app - hosts: webservers become: yes become_method: sudo tasks: - name: install packages apt: name: &quot;{{ packages }}&quot; update_cache: yes vars: packages: - nginx - gunicorn3 - python3-pip - hosts: webservers become: yes become_method: sudo tasks: - name: clone repo git: repo: &#39;https://github.com/{{ github_user }}/{{ app_name }}.git&#39; dest: /srv/www/{{ app_name }} update: yes - hosts: webservers become: yes become_method: sudo tasks: - name: Install needed python packages pip: requirements: requirements.txt chdir: /srv/www/{{ app_name }} - hosts: webservers become: yes become_method: sudo tasks: - name: template systemd service config template: src: deploy/gunicorn.service dest: /etc/systemd/system/{{ app_name }}.service - name: start systemd app service systemd: name={{ app_name }}.service state=restarted enabled=yes - name: template nginx site config template: src: deploy/nginx.conf dest: /etc/nginx/sites-available/{{ app_name }}.conf - name: remove default nginx site config file: path=/etc/nginx/sites-enabled/default state=absent - name: enable nginx site file: src: /etc/nginx/sites-available/{{ app_name }}.conf dest: /etc/nginx/sites-enabled/{{ app_name }}.conf state: link force: yes - name: restart nginx systemd: name=nginx state=restarted enabled=yes . The playbook does the following: . Installs nginx, gunicorn3, and python-pip. | Clones the git repository https://github.com/&lt;github_user&gt;/&lt;app_name&gt; to /srv/www/. | Installs the python packages listed in requirements.txt of the repo to be cloned. flask should be listed if you are running a flask server. | Installs and starts a systemd service to run gunicorn. | Installs and enables an nginx configuration that forwards the gunicorn web service to port 80. | After creating create the two configuration file templates that the playbook uses in the deploy directory. These are template files - you do not and should not change anything in them, since Ansible will automatically fill in the variables from deploy/hosts. . deploy/nginx.conf . server { listen 80; listen [::]:80; server_name {{ domain_name }}; location / { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header Host $http_host; proxy_pass http://localhost:8080; } } . deploy/gunicorn.service . [Unit] Description=gunicorn to serve {{ app_name }} After=network.target [Service] WorkingDirectory=/srv/www/{{ app_name }} ExecStart=/usr/bin/gunicorn3 -b 0.0.0.0:8080 --chdir application app:app [Install] WantedBy=multi-user.target . Run playbook . After setting everything up above, you can now run the playbook. . ansible-playbook deploy.yaml . . After running the playbook you should be able to visit your Elastic IP address in a web browser and see “It works!” for the barebones Flask project. . Now as you make changes, simply push to Github as usual. When you’re ready to update your server, just re-run the playbook. . . Warning: If you&#39;re just testing this out and not wanting to run your Flask site yet, be sure to stop or terminate your EC2 instance.",
            "url": "https://barnett.science/linux/aws/ansible/2020/05/28/ansible-flask.html",
            "relUrl": "/linux/aws/ansible/2020/05/28/ansible-flask.html",
            "date": " • May 28, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Arch Linux Installation",
            "content": ". Warning: This is my personal guide for installing an Arch Linux system. Use it only as a guide as you follow along with the official Installation Guide. This guide gives several options along the way, depending on your system. . For UEFI, EFISTUB is used to boot the kernel directly. | For systems needing BIOS, syslinux is used. | Gives options for encrypting the system if desired. | Gives notes on using btrfs subvolumes if desired. | . You may need additional packages for video drivers, etc. . Pre-installation . Download the Arch ISO and GnuPG signature. . Verify signature . If you have GnuPG installed on your current system, verify the download: . $ gpg --keyserver-options auto-key-retrieve --verify archlinux-&lt;version&gt;-dual.iso.sig . Create bootable disk . Create a bootable USB drive by doing the following on an existing Linux installation: . # dd bs=4M if=/path/to/archlinux-&lt;version&gt;-x86_64.iso of=/dev/sdx status=progress &amp;&amp; sync . where /dev/sdx is the USB drive. . Boot the live environment . Now boot from the USB drive. . Set the keyboard layout . If using a keymap other than US, temporarily set the keyboard layout by doing: . # loadkeys de-latin1 . Change de-latin1 to a layout found in /usr/share/kbd/keymaps/**/*.map.gz. . Verify the boot mode . Verify that you have booted with UEFI mode by checking that /sys/firmware/efi/efivars exists. If you’re not booted in the UEFI, you should setup your motherboard to do so to follow this installation guide. If you are not able to use UEFI, this guide has an option to boot from BIOS using syslinux. . Connect to the internet . If you have a wired connection, it should connect automatically. . If you have a wireless connection, first stop the wired connection to prevent conflicts: . # systemctl stop dhcpcd@interface.service . A list of interfaces can be found with: . # ip link . Then connect to a wifi network with wpa_supplicant: . # wpa_supplicant -B -i interface -C/run/wpa_supplicant # wpa_cli -i interface &gt; scan &gt; scan_results &gt; add_network &gt; set_network 0 ssid &quot;SSID&quot; &gt; set_network 0 psk &quot;passphrase&quot; &gt; enable_network 0 &gt; quit . Get an ip address with dhcpcd: . # dhcpcd . For both wired and wireless connections, check your connection with: . # ping archlinux.org . Update the system clock . # timedatectl set-ntp true . Partition the disk . . Warning: All data on your disk will be erased. Backup any data you wish to keep. . Note: This guide assumes your disk is at /dev/sda. Change if needed. Use lsblk to identify existing file systems. . Here are the possible layouts this guide uses. Modify to your needs. . UEFI, not encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | EFI system partition &lt;/ul&gt; | 550 MiB | . /mnt | /dev/sda2 | Linux x86-64 root (/) | Remainder of the device | . BIOS, not encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | boot partition | 550 MiB | . /mnt | /dev/sda2 | Linux x86-64 root (/) | Remainder of the device | . UEFI, encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | EFI system partition | 550 MiB | . /mnt | /dev/mapper/cryptroot | Linux x86-64 root (/) | Remainder of the device | . BIOS, encrypted . Mount point Partition Partition type Suggested size . /mnt/boot | /dev/sda1 | boot partition | 550 MiB | . /mnt | /dev/mapper/cryptroot | Linux x86-64 root (/) | Remainder of the device | . Use GPT fdisk to format the disk: . # gdisk /dev/sda . Create a new empty GUID partition table by typing o at the prompt. | Create a new partition by typing n at the prompt. Hit Enter when prompted for the partition number, keeping the default of 1. Hit Enter again for the first section, keeping the default. For the last sector, type in +550M and hit Enter. | For an EFI system, type in EF00 to indicate it is an EFI system partition for the partition type. Otherwise, use the default. | Now create at least one more partition for the installation. To create just one more partition to fill the rest of the disk, type n at the prompt and use the defaults for the partition number, first sector, last sector, and hex code. | If setting up a BIOS system with syslinux, enter expert mode by entering x. Then enter a and then 1 to set an attribute for partition 1. Then enter 2 to set it as a legacy BIOS partition and then Enter to exit the set attribute menu. 6. Finally, write the table to the disk and exit by entering w at the prompt. | . Tip: Here are one-liners for the above layout. For EFI: # sgdisk /dev/sda -o -n 1::+550M -n 2 -t 2:EF00 . For BIOS: # sgdisk /dev/sda -o -n 1::+550M -n 2 -A 1:set:2i . Create LUKS container . If encrypting your system with dm-crypt/LUKS, do: . # cryptsetup lukFormat --type luks2 /dev/sda2 # cryptsetup open /dev/sda2 cryptroot . Othwerwise, skip this step. . Format the partitions . Format your ESP as FAT32: . # mkfs.fat -F32 /dev/sda1 . Replace ext4 with the file system you are using in all of the following. . To format the LUKS container on an encrypted system do: . # mkfs.ext4 /dev/mapper/cryptroot . To format a regular (not encrypted) system, do: . # mkfs.ext4 /dev/sda2 . Mount the file systems . For the encrypted setup mount the LUKS container: . # mount /dev/mapper/cryptroot /mnt . For the regular setup mount the root partition: . # mount /dev/sda2 /mnt . In both cases make a mount point for the boot partition and mount it: . # mkdir -p /mnt/boot # mount /dev/sda1 /mnt/boot . . Tip: If using btrfs, create any subvolumes you wish to use as mount points now. Then unmount /mnt and remount your subvolumes to the appropriate mount points. For example, for the encrypted setup: # mount /dev/mapper/cryptroot /mnt # btrfs subvolume create /mnt/@ # btrfs subvolume create /mnt/@home # umount /mnt # mount -o compress=zstd,subvol=@ /dev/mapper/cryptroot /mnt # mkdir -p /mnt/home # mount -o compress=zstd,subvol=@home /dev/mapper/cryptroot /mnt/home # mkdir -p /mnt/boot # mount /dev/sda1 /mnt/boot . Installation . Select the mirrors . If you desire, edit /etc/pacman.d/mirrorlist to select which mirrors have priority. Higher in the file means higher priority. This file will be copied to the new installation. . Install packages . # pacstrap /mnt base btrfs-progs linux man-db man-pages texinfo vim which . Append additional packages you wish to install to the line. You will have the opportunity to install more packages in the chroot environment and when you boot into the new system. . If you have an AMD or Intel processor, you will want to go ahead and install the amd-ucode or intel-ucode packages, respectively to enable microcode updates later in the guide. . Configure the system . Fstab . Generate the fstab for your new installation: . # genfstab -U /mnt &gt; /mnt/etc/fstab . Chroot . chroot into the new installation: . # arch-chroot /mnt . Time zone . Set the time zone: . # ln -sf /usr/share/zoneinfo/&lt;Region&gt;/&lt;City&gt; /etc/localtime . Set the hardware clock from the system clock: . # hwclock --systohc . Localization . Uncomment needed locales in /etc/locale.gen (e.g., en_US.UTF-8). Then run: . # locale-gen . Set the LANG environment variable the locale in the following file. . /etc/locale.conf . LANG=en_US.UTF-8 . If you are not using a US keymap, make the keyboard layout permanent: . /etc/vconsole.conf . KEYMAP=de-latin1 . Network configuration . Hostname . Set the hostname. Change hostname to your preferred hostname in the following: . /etc/hostname . hostname . /etc/hosts . 127.0.0.1 localhost ::1 localhost 127.0.1.1 hostname.localdomain hostname . Configuration . systemd-networkd will be used to connect to the internet after installation is complete. . Create a minimal systemd-networkd configuration file with the following contents. Here interface is the wireless interface or the wired interface if not using wireless. . /etc/systemd/network/config.network . [Match] Name=interface [Network] DHCP=yes . Enable the systemd-networkd.service unit. . DNS resolution . To use systemd-resolved for DNS resolution, create a symlink as follows: . # ln -s /run/systemd/resolve/stub-resolv.conf /etc/resolv.conf . Enable the systemd-resolved.service unit. . Wireless . If using a wireless interface, install the iwd package now: . # pacman -S iwd . Additionally enable the iwd.service for wireless on boot. . . Note: To prevent a known race condition that causes wireless device renaming problems, create a drop-in file for iwd.service with the following: [Unit] After=systemd-udevd.service systemd-networkd.service . Initramfs . If using an encrypted system, add the encrypt hook to your mkinitcpio configuration as shown below. Add the keymap hook if you are not using the default US keymap. If using btrfs, you can remove the fsck hook. . /etc/mkinitcpio.conf . HOOKS=(base udev autodetect modconf block filesystems keyboard fsck keymap encrypt) . . Tip: Move keyboard in front of autodetect if using an external USB keyboard that was not connected when the image is created. Regenerate initramfs: . # mkinitcpio -p linux . Root password . Set the root password: . # passwd . . Tip: You can skip this step if you are giving your normal user super user privileges via sudo. Add normal user . Install the sudo package: . # pacman -S sudo . Add a normal user, add it to the wheel group, and set the password as follows, where user is the name of your user: . # useradd -m -G wheel user # passwd user . Open the sudoers file and uncomment the wheel group, giving that user access to sudo: . # EDITOR=vim visudo . Swap file . If using btrfs, first create a subvolume for the swap file to reside on. Then, create an empty swap file and set it to not use COW: . # btrfs subvolume create /.swap # truncate -s 0 /.swap/swapfile # chattr +C /.swap/swapfile . If not using btrfs, simply create a directory: . # mkdir /.swap . In all cases do: . # dd if=/dev/zero of=/.swap/swapfile bs=1M count=2048 # chmod 600 /.swap/swapfile # mkswap /.swap/swapfile . Update fstab with a line for the swap file as follows: . # /etc/fstab /.swap/swapfile none swap defaults 0 0 . . Warning: If using btrfs instead of ext4, do not use a swap file for kernels before v5.0, since it may cause file system corruption. Instead, use a swap partition (not covered here). Boot loader . Two options are provided here. If you have a UEFI motherboard, use EFISTUB. Otherwise, use the BIOS setup with syslinux. In either case, you will need to set your kernel parameters. . EFISTUB . In the UEFI setup we are not using a boot loader. Instead we are booting the kernel directly via EFISTUB. Previously you should have created a EFI system partition of the size 550MiB and marked it with the partition type EF00. . Exit the chroot and reboot the system. From your Arch Linux live disk, boot into the UEFI Shell v2. Then do: . Shell&gt; map . Note the disk number for the hard drive where you are installing Arch Linux. This guide assumes it is 1. . Now create two UEFI entries using bcfg. . Shell&gt; bcfg boot add 0 fs1:vmlinuz-linux &quot;Arch Linux&quot; Shell&gt; bcfg boot add 1 fs1:vmlinuz-linux &quot;Arch Linux (Fallback)&quot; . Create a file with your kernel parameters as a single line: . Shell&gt; edit fs1:options.txt . . Note: Create at least one additional space before the first character of your boot line in your options files. Otherwise, the first character gets squashed by a byte order mark and will not be passed to the initramfs, resulting in an error when booting. Additionally, your options file should be one line, and one line only. Press F2 to save and F3 to quit. Now add that file as the options to your first boot entry: . Shell&gt; bcfg boot -opt 0 fs1:options.txt . Repeat the above process for your second, fallback entry, creating a text file named options-fallback.txt containing a single line with your kernel parameters, chaning the intird to the fallback image (i.e., /initramfs-linux-fallback.img). . Add it to the entry using bcfg boot -opt 1 fs:1 options-fallback.txt. . syslinux . In the BIOS setup we are using syslinux. Previously you should have created a boot partition that was marked with the attribute “legacy BIOS bootable”. . Now, while still in the chroot install syslinux: . # pacman -S syslinux . Create the following configuration file: . /boot/syslinux/syslinux.cfg . DEFAULT arch LABEL arch MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND kernel-parameters LABEL archfallback MENU LABEL Arch Linux LINUX ../vmlinuz-linux APPEND fallback-kernel-parameters . where kernel-parameters is from the kernel parameters you will create in the next section. fallback-kernel-parameters is exactly the same except with initrd pointing to the fallback initramfs (i.e., /initramfs-linux-fallback.img). . Exit the chroot and unmount all of the partitions: . # umount -R /mnt . Then install the bootloader: . # syslinux --directory syslinux /dev/sda1 . And install the MBR: . # dd bs=440 count=1 conv=notrunc if=/usr/lib/syslinux/bios/gptmbr.bin of=/dev/sda . Kernel parameters . No matter what boot loader you use, you need to pass some kernel parameters to it as indicated in the above sections. . For an encrypted system, the kernel parameters will contain at least: . root=/dev/mapper/cryptroot ro initrd=/initramfs-linux.img init=/usr/lib/systemd/systemd cryptdevice=/dev/sda2:cryptroot . . Tip: Add :allow-discards after cryptroot to allow trimming if using an SSD. Then enable the fstrim.timer to trim the device weekly. For a regular system, the kernel parameters will contain at least: . root=/dev/sda2 ro initrd=/initramfs-linux.img init=/usr/lib/systemd/systemd . . Note: In either case, if using btrfs, and you want to boot from a specific subvolume, add rootfstype=btrfs rootflags=subvol=/@, where @ is the subvolume you will mount as /. . Note: If you have an Intel or AMD CPU, enable microcode updates by adding an /intel-ucode.img or /amd-ucode.img, respectively to initrd= with a comma separating the two images. It &#39;&#39;must&#39;&#39; be the first initrd entry on the line. For example: initrd=/intel-ucode.img,/initramfs-linux.img. Reboot . When you reboot you should be prompted for your LUKS password if you decided to encrypt the system. .",
            "url": "https://barnett.science/linux/2020/05/24/arch-install.html",
            "relUrl": "/linux/2020/05/24/arch-install.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Create a venv for a Jupyter kernel",
            "content": "After I setup my deep learning box, I installed JupyterHub following these instructions. I skipped the conda part myself since I like to run things in Python venv’s (virtual environments). . One aspect that I struggled with in finding in the documentation was how to setup a Python venv to be used for a Jupyter kernel. It was actually much simpler than expected. . First, create a venv that will be used for the kernel: . python -m venv my-venv . Then activate that virtual environment: . source ./my-venv/bin/activate . After that install ipykernel: . python -m pip install ipykernel . Lastly, register that environment as a kernel as described here: . python -m ipykernel install --user --name my-venv --display-name &quot;Python (my-venv)&quot; . That’s it! Now when you log into your JupyterHub and start a notebook, you should see it as available. . Installing packages . Whenenever you want to install new packages to be available in your kernel simply install them to your virtual environment. You can ssh into your box, activate your venv, and run pip to install them. . Alternatively, you can use the %pip magic in Jupyter to run the installation. For example, to install pandas: . %pip install pandas . Note that this will give you different results from running !pip, which is running it as a shell command. That won’t use the pip in the virtual environment being utilized by the kernel like %pip will. Neither will !python -m pip. You will end up installing your packages in the wrong location if you use the shell form. . .",
            "url": "https://barnett.science/deeplearning/2020/05/22/jupyter-kernel.html",
            "relUrl": "/deeplearning/2020/05/22/jupyter-kernel.html",
            "date": " • May 22, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "GitHub Actions: Providing Data Scientists With New Superpowers",
            "content": "What Superpowers? . Hi, I’m Hamel Husain. I’m a machine learning engineer at GitHub. Recently, GitHub released a new product called GitHub Actions, which has mostly flown under the radar in the machine learning and data science community as just another continuous integration tool. . Recently, I’ve been able to use GitHub Actions to build some very unique tools for Data Scientists, which I want to share with you today. Most importantly, I hope to get you excited about GitHub Actions, and the promise it has for giving you new superpowers as a Data Scientist. Here are two projects I recently built with Actions that show off its potential: . fastpages . fastpages is an automated, open-source blogging platform with enhanced support for Jupyter notebooks. You save your notebooks, markdown, or Word docs into a directory on GitHub, and they automatically become blog posts. Read the announcement below: . We&#39;re launching `fastpages`, a platform which allows you to host a blog for free, with no ads. You can blog with @ProjectJupyter notebooks, @office Word, directly from @github&#39;s markdown editor, etc.Nothing to install, &amp; setup is automated!https://t.co/dNSA0oQUrN . &mdash; Jeremy Howard (@jeremyphoward) February 24, 2020 Machine Learning Ops . Wouldn’t it be cool if you could invoke a chatbot natively on GitHub to test your machine learning models on the infrastructure of your choice (GPUs), log all the results, and give you a rich report back in a pull request so that everyone could see the results? You can with GitHub Actions! . Consider the below annotated screenshot of this Pull Request: . . A more in-depth explanation about the above project can be viewed in this video: . Using GitHub Actions for machine learning workflows is starting to catch on. Julien Chaumond, CTO of Hugging Face, says: . GitHub Actions are great because they let us do CI on GPUs (as most of our users use the library on GPUs not on CPUs), on our own infra! 1 . Additionally, you can host a GitHub Action for other people so others can use parts of your workflow without having to re-create your steps. I provide examples of this below. . A Gentle Introduction To GitHub Actions . What Are GitHub Actions? . GitHub Actions allow you to run arbitrary code in response to events. Events are activities that happen on GitHub such as: . Opening a pull request | Making an issue comment | Labeling an issue | Creating a new branch | … and many more | . When an event is created, the GitHub Actions context is hydrated with a payload containing metadata for that event. Below is an example of a payload that is received when an issue is created: . { &quot;action&quot;: &quot;created&quot;, &quot;issue&quot;: { &quot;id&quot;: 444500041, &quot;number&quot;: 1, &quot;title&quot;: &quot;Spelling error in the README file&quot;, &quot;user&quot;: { &quot;login&quot;: &quot;Codertocat&quot;, &quot;type&quot;: &quot;User&quot;, }, &quot;labels&quot;: [ { &quot;id&quot;: 1362934389, &quot;node_id&quot;: &quot;MDU6TGFiZWwxMzYyOTM0Mzg5&quot;, &quot;name&quot;: &quot;bug&quot;, } ], &quot;body&quot;: &quot;It looks like you accidently spelled &#39;commit&#39; with two &#39;t&#39;s.&quot; } . This functionality allows you to respond to various events on GitHub in an automated way. In addition to this payload, GitHub Actions also provide a plethora of variables and environment variables that afford easy to access metadata such as the username and the owner of the repo. Additionally, other people can package useful functionality into an Action that other people can inherit. For example, consider the below Action that helps you publish python packages to PyPi: . The Usage section describes how this Action can be used: . - name: Publish a Python distribution to PyPI uses: pypa/gh-action-pypi-publish@master with: user: __token__ password: ${{ secrets.pypi_password }} . This Action expects two inputs: user and a password. You will notice that the password is referencing a variable called secrets, which is a variable that contains an encrypted secret that you can upload to your GitHub repository. There are thousands of Actions (that are free) for a wide variety of tasks that can be discovered on the GitHub Marketplace. The ability to inherit ready-made Actions in your workflow allows you to accomplish complex tasks without implementing all of the logic yourself. Some useful Actions for those getting started are: . actions/checkout: Allows you to quickly clone the contents of your repository into your environment, which you often want to do. This does a number of other things such as automatically mount your repository’s files into downstream Docker containers. | mxschmitt/action-tmate: Proivdes a way to debug Actions interactively. This uses port forwarding to give you a terminal in the browser that is connected to your Actions runner. Be careful not to expose sensitive information if you use this. | actions/github-script: Gives you a pre-authenticated ocotokit.js client that allows you to interact with the GitHub API to accomplish almost any task on GitHub automatically. Only these endpoints are supported (for example, the secrets endpoint is not in that list). | . In addition to the aforementioned Actions, it is helpful to go peruse the official GitHub Actions docs before diving in. . Example: A fastpages Action Workflow . The best to way familiarize yourself with Actions is by studying examples. Let’s take a look at the Action workflow that automates the build of fastpages (the platform used to write this blog post). . Part 1: Define Workflow Triggers . First, we define triggers in ci.yaml. Like all Actions workflows, this is a YAML file located in the .github/workflows directory of the GitHub repo. . The top of this YAML file looks like this: . name: CI on: push: branches: - master pull_request: . This means that this workflow is triggered on either a push or pull request event. Furthermore, push events are filtered such that only pushes to the master branch will trigger the workflow, whereas all pull requests will trigger this workflow. It is important to note that pull requests opened from forks will have read-only access to the base repository and cannot access any secrets for security reasons. The reason for defining the workflow in this way is we wanted to trigger the same workflow to test pull requests as well as build and deploy the website when a PR is merged into master. This will be clarified as we step through the rest of the YAML file. . Part 2: Define Jobs . Next, we define jobs (there is only one in this workflow). Per the docs: . A workflow run is made up of one or more jobs. Jobs run in parallel by default. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: . The keyword build-site is the name of your job and you can name it whatever you want. In this case, we have a conditional if statement that dictates if this job should be run or not. We are trying to ensure that this workflow does not run when the first commit to a repo is made with the message ‘Initial commit’. The first variable in the if statement, github.event, contains a json payload of the event that triggered this workflow. When developing workflows, it is helpful to print this variable in order to inspect its structure, which you can accomplish with the following YAML: . - name: see payload run: | echo &quot;PAYLOAD: n${PAYLOAD} n&quot; env: PAYLOAD: ${{ toJSON(github.event) }} . Note: the above step is only for debugging and is not currently in the workflow. . toJson is a handy function that returns a pretty-printed JSON representation of the variable. The output is printed directly in the logs contained in the Actions tab of your repo. In this example, printing the payload for a push event will look like this (truncated for brevity): . { &quot;ref&quot;: &quot;refs/tags/simple-tag&quot;, &quot;before&quot;: &quot;6113728f27ae8c7b1a77c8d03f9ed6e0adf246&quot;, &quot;created&quot;: false, &quot;deleted&quot;: true, &quot;forced&quot;: false, &quot;base_ref&quot;: null, &quot;commits&quot;: [ { &quot;message&quot;: &quot;updated README.md&quot;, &quot;author&quot;: &quot;hamelsmu&quot; }, ], &quot;head_commit&quot;: null, } . Therefore, the variable github.event.commits[0].message will retrieve the first commit message in the array of commits. Since we are looking for situations where there is only one commit, this logic suffices. The second variable in the if statement, github.run_number is a special variable in Actions which: . [is a] unique number for each run of a particular workflow in a repository. This number begins at 1 for the workflow’s first run, and increments with each new run. This number does not change if you re-run the workflow run. . Therefore, the if statement introduced above: . if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 . Allows the workflow to run when the commit message is “Initial commit” as long as it is not the first commit. ( || is a logical or operator). . Finally, the line runs-on: ubuntu-latest specifies the host operating system that your workflows will run in. . Part 3: Define Steps . Per the docs: . A job contains a sequence of tasks called steps. Steps can run commands, run setup tasks, or run an Action in your repository, a public repository, or an Action published in a Docker registry. Not all steps run Actions, but all Actions run as a step. Each step runs in its own process in the runner environment and has access to the workspace and filesystem. Because steps run in their own process, changes to environment variables are not preserved between steps. GitHub provides built-in steps to set up and complete a job. . Below are the first two steps in our workflow: . - name: Copy Repository Contents uses: actions/checkout@master with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The first step creates a copy of your repository in the Actions file system, with the help of the utility action/checkout. This utility only fetches the last commit by default and saves files into a directory (whose path is stored in the environment variable GITHUB_WORKSPACE that is accessible by subsequent steps in your job. The second step runs the fastai/fastpages Action, which converts notebooks and word documents to blog posts automatically. In this case, the syntax: . uses: ./_action_files . is a special case where the pre-made GitHub Action we want to run happens to be defined in the same repo that runs this workflow. This syntax allows us to test changes to this pre-made Action when evaluating PRs by referencing the directory in the current repository that defines that pre-made Action. Note: Building pre-made Actions is beyond the scope of this tutorial. . The next three steps in our workflow are defined below: . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;gem install bundler &amp;&amp; jekyll build -V&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : . The step named setup directories for Jekyll build executes shell commands that remove the _site folder in order to get rid of stale files related to the page we want to build, as well as grant permissions to all the files in our repo to subsequent steps. . The step named Jekyll build executes a docker container hosted by the Jekyll community on Dockerhub called jekyll/jekyll. For those not familiar with Docker, see this tutorial. The name of this container is called fastai/fastpages-jekyll because I’m adding some additional dependencies to jekyll/jekyll and hosting those on my DockerHub account for faster build times2. The args parameter allows you to execute arbitrary commands with the Docker container by overriding the CMD instruction in the Dockerfile. We use this Docker container hosted on Dockerhub so we don’t have to deal with installing and configuring all of the complicated dependencies for Jekyll. The files from our repo are already available in the Actions runtime due to the first step in this workflow, and are mounted into this Docker container automatically for us. In this case, we are running the command jekyll build, which builds our website and places relevant assets them into the _site folder. For more information about Jekyll, read the official docs. Finally, the env parameter allows me to pass an environment variable into the Docker container. . The final command above copies a CNAME file into the _site folder, which we need for the custom domain https://fastpages.fast.ai. Setting up custom domains are outside the scope of this article. . The final step in our workflow is defined below: . - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.SSH_DEPLOY_KEY }} publish_dir: ./_site . The statement . if: github.event_name == &#39;push&#39; . uses the variable github.event_name to ensure this step only runs when a push event ( in this case only pushes to the master branch trigger this workflow) occur. . This step deploys the fastpages website by copying the contents of the _site folder to the root of the gh-pages branch, which GitHub Pages uses for hosting. This step uses the peaceiris/actions-gh-pages Action, pinned at version 3. Their README describes various options and inputs for this Action. . Conclusion . We hope that this has shed some light on how we use GitHub Actions to automate fastpages. While we only covered one workflow above, we hope this provides enough intuition to understand the other workflows in fastpages. We have only scratched the surface of GitHub Actions in this blog post, but we provide other materials below for those who want to dive in deeper. We have not covered how to host an Action for other people, but you can start with these docs to learn more. . Still confused about how GitHub Actions could be used for Data Science? Here are some ideas of things you can build: . Jupyter Widgets that trigger GitHub Actions to perform various tasks on GitHub via the repository dispatch event | Integration with Pachyderm for data versioning. | Integration with your favorite cloud machine learning services, such Sagemaker, Azure ML or GCP’s AI Platform. | . Related Materials . GitHub Actions official documentation | Hello world Docker Action: A template to demonstrate how to build a Docker Action for other people to use. | Awesome Actions: A curated list of interesting GitHub Actions by topic. | A tutorial on Docker for Data Scientists. | . Getting In Touch . Please feel free to get in touch with us on Twitter: . Hamel Husain @HamelHusain | Jeremy Howard @jeremyphoward | . . Footnotes . You can see some of Hugging Face’s Actions workflows for machine learning on GitHub &#8617; . | These additional dependencies are defined here, which uses the “jekyll build” command to add ruby dedpendencies from the Gemfile located at the root of the repo. Additionally, this docker image is built by another Action workflow defined here. &#8617; . |",
            "url": "https://barnett.science/actions/markdown/2020/03/06/fastpages-actions.html",
            "relUrl": "/actions/markdown/2020/03/06/fastpages-actions.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages . In this post, we will cover special features that fastpages provides for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions. . Interactive Charts With Altair . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Other Features . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Images w/Captions . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . More Examples . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://barnett.science/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://barnett.science/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Building a deep learning box",
            "content": "During the process of going through the fast.ai course, I decided to build my own deep learning box. This post is summary of my experience, but it won’t tell you every detail needed to build your own. There are quite a few guides out there. . A couple of decades ago as a teenager I remember my brother ordering parts and putting together the family’s PC. Of course at the time it was used mainly for playing Alpha Centauri, downloading from Napster, and racking up the long distance phone bill not knowing area codes did not mean local calling. I also got exposed to HTML and Javascript at the time so that was my first real introduction to any kind of coding (Javascript has changed just a little since then). . I remember my brother using a website PC Parts Picker to put a list of compatible parts together and find where to buy them. That website is still commonly used today and what I used in picking my parts. This is the list I ended up with. (Note: Some items are now discontinued by the time you’re reading this). The important thing for me was to get an Nvidia card to be able to properly use CUDA on my machine. The card is on the lower end of what is considered acceptable, but I was on a budget. . Here are most of the parts in their boxes before assembling. . . The process was pretty straightforward. Motherboard went in first, then the processor, and so on. I did have an issue where the backplate came off when I unscrewed the mounting screws for the processor fan. I struggled to put the fan on due that until I realized it had fallen behind the case. I wish I had mounted the fan the other way since there is a plastic part that now covers one of the RAM ports. . . Most of the cords can only plug into one spot and everything on the motherboard and power supply are labeled. Just paint by numbers for the most part. I did make the mistake of not plugging the GPU into the power but got a nice message when I tried to boot up. . . . After I was able to boot I installed Arch Linux on it. I have been involved with the Arch community for quite some time in editing the Wiki and have installed it numerous times, so I was comfortable doing so (Here’s my own installation guide; use with caution. Additionally all the super computer clusters I used in graduate school were Linux-based, and our lab setup our own cluster. One downside about using Arch is that sometimes there are libraries that are made for Ubuntu that aren’t as easily compiled for Arch in a straightforward manner. . . After installing Arch I installed JupyterLab and JupyterHub, CUDA, and some of the deep learning frameworks and was able to start running pretty quickly. . . Thus far, it’s been a good experience using the machine for running a couple of Kaggle competitions. .",
            "url": "https://barnett.science/deeplearning/2020/02/02/deep-learning-box.html",
            "relUrl": "/deeplearning/2020/02/02/deep-learning-box.html",
            "date": " • Feb 2, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Bayesian Linear Regression with CmdStanPy",
            "content": "In this tutorial we&#39;ll use Stan, via CmdStanPy, to perform Bayesian Linear Regression. Visit the CmdStanPy Github page for information on how to install. . import matplotlib.pyplot as plt import numpy as np import pandas as pd from cmdstanpy import CmdStanModel from sklearn.datasets import load_boston from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split . The dataset we are using comes from Lesson 1 of Andrew Ng&#39;s Coursera course on Machine Learning. I&#39;ve chosen this since it only has one feature so we can easily visualize what is happening. You can find the contents here. . df = pd.read_csv(&quot;ex1data1.txt&quot;, header=None, names=[&quot;population&quot;, &quot;profit&quot;]) . df.head() . population profit . 0 6.1101 | 17.5920 | . 1 5.5277 | 9.1302 | . 2 8.5186 | 13.6620 | . 3 7.0032 | 11.8540 | . 4 5.8598 | 6.8233 | . df.plot(&#39;population&#39;, &#39;profit&#39;, lw=0, marker=&#39;o&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f764059d970&gt; . Let&#39;s do the typical train/test split of the data, with population being our feature, and profit our target. . X_train, X_test, y_train, y_test = train_test_split(df[&quot;population&quot;], df[&quot;profit&quot;], random_state=0) . Data dictionary and model file . Now we need to create a data dictionary for Stan. The values in the data dictionary correspond with the values in the model file that I have written located in model.stan. Here are the contents of that file, which will be loaded and compiled later in this notebook. I&#39;m generally following this example in the Stan User Guide. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[N] y; int&lt;lower=0&gt; N_new; matrix[N_new, K] x_new; } parameters { vector[K] beta; real&lt;lower=0&gt; sigma; } model { y ~ normal(x*beta, sigma); } generated quantities { vector[N] y_train_pred; vector[N_new] y_new; for (n in 1:N) { y_train_pred[n] = normal_rng(x[n]*beta, sigma); } for (n in 1:N_new) { y_new[n] = normal_rng(x_new[n]*beta, sigma); } } . N_train = X_train.values.shape[0] N_test = X_test.values.shape[0] X_train = np.hstack((np.ones((N_train,1)), X_train.values.reshape(-1,1))) X_test = np.hstack((np.ones((N_test,1)), X_test.values.reshape(-1,1))) y_train = y_train.values K = 2 data_dict = {&quot;N&quot;: N_train, &quot;K&quot;: K, &quot;x&quot;: X_train, &quot;y&quot;: y_train, &quot;N_new&quot;: N_test, &quot;x_new&quot;: X_test} . Here are the meanings of each variable or parameter: . N = Number of data points in training set. | K = Number of columns in our dataset. We have two columns, since we are going to create an intercept column. | x = N x K matrix. The actual data. | y = N sized vector. The target variable. | N_new = Number of data points in our test set. | x_new = N_new x K matrix. The test data. | beta = Parameters to be trained. | sigma = Parameter to be trained, standard deviation in the normal distribution | . What about the blocks in the Stan file? You will want to check out the reference here on this. But, in short: . Variables in the data block are where variables that are read in as data are declared. . Variables in the parameters block are where variables that are sampled by Stan&#39;s MCMC algorithm are declared. . The model block defines our model. Here we are using sampling notation to indicate our target variable, y, has a distribution corresponding with the right-hand side. In this case, it is normally distributed with a mean of X$ beta$ and a standard deviation of $ sigma$. We use this distribution because the major assumption is that the errors are normally distributed around a mean of zero and a standard deviation of $ sigma$. This is a direct consequence of that assumption, which again is described in more detail here. . The generated quantities block does not adjust the learned parameters. We use it here to make predictions on our training set (in order to get the R$^{2}$ metric on it) as well as predicting on our test set for evaluation. . Next we load and compile the Stan file as described above. . model = CmdStanModel(stan_file=&quot;model.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/model . Sampling . Now that we have compiled the Stan code, we can do sampling. The sample() method performs MCMC sampling in the following process for each chain: . Draw a value from the auxillary moment ($ rho$) distribution. | Use the leapfrog iterator to update $ rho$ and parameters $ theta$. | Measure the Hamiltonian of the new state. | Use Metroplis-Hastings to compare the new Hamiltonian and the previous Hamiltonian. Accept or reject. | Save parameters $ theta$. If accepted, the new state becomes the previous state, and the process is repeated. | For more details, see this section of the Stan reference manual. . fit = model.sample(data=data_dict) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:start chain 2 INFO:cmdstanpy:finish chain 2 INFO:cmdstanpy:start chain 3 INFO:cmdstanpy:finish chain 1 INFO:cmdstanpy:start chain 4 INFO:cmdstanpy:finish chain 3 INFO:cmdstanpy:finish chain 4 . Results . The following gives a summary of each parameter and of each sample in the generated quantities. . df_summary = fit.summary() . df_summary.head(10) . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ -114.13800 | 0.033104 | 1.233760 | -116.576000 | -113.80600 | -112.83200 | 1388.96 | 1834.80 | 1.000320 | . beta[1] -4.06342 | 0.020626 | 0.827210 | -5.422190 | -4.06876 | -2.64867 | 1608.43 | 2124.71 | 1.000920 | . beta[2] 1.20494 | 0.002254 | 0.089512 | 1.054030 | 1.20780 | 1.34983 | 1577.67 | 2084.08 | 1.001430 | . sigma 3.04292 | 0.005602 | 0.258644 | 2.656110 | 3.02426 | 3.49347 | 2131.48 | 2815.65 | 1.001030 | . y_train_pred[1] 9.97330 | 0.048726 | 3.076020 | 4.879570 | 9.95605 | 15.03880 | 3985.34 | 5264.57 | 0.999336 | . y_train_pred[2] 4.91241 | 0.049360 | 3.056540 | -0.181923 | 4.98684 | 9.75746 | 3834.55 | 5065.38 | 0.999112 | . y_train_pred[3] 2.54711 | 0.047909 | 3.034660 | -2.349200 | 2.58090 | 7.57581 | 4012.16 | 5300.00 | 0.999527 | . y_train_pred[4] 2.80746 | 0.049370 | 3.090090 | -2.509290 | 2.82996 | 7.85182 | 3917.56 | 5175.03 | 0.999559 | . y_train_pred[5] 3.86753 | 0.048720 | 3.072960 | -1.087240 | 3.76285 | 8.91217 | 3978.38 | 5255.37 | 1.001370 | . y_train_pred[6] 6.61481 | 0.048569 | 3.100650 | 1.529320 | 6.58784 | 11.64820 | 4075.50 | 5383.66 | 1.000040 | . Here are all the samples from all the chains: . df_samples = fit.get_drawset() . df_samples.head() . lp__ accept_stat__ stepsize__ treedepth__ n_leapfrog__ divergent__ energy__ beta.1 beta.2 sigma ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 -114.345 | 0.940039 | 0.268831 | 3.0 | 7.0 | 0.0 | 115.687 | -4.87938 | 1.23486 | 2.80795 | ... | 4.038810 | 0.820634 | 1.564890 | 0.89525 | 3.42023 | 0.159389 | 22.1371 | 0.955606 | 9.501570 | 6.104850 | . 1 -114.143 | 0.993854 | 0.268831 | 2.0 | 3.0 | 0.0 | 114.673 | -4.33896 | 1.29027 | 2.83174 | ... | -0.155484 | 0.652295 | 1.579160 | 3.95726 | 4.14529 | -0.440211 | 21.8035 | 10.706000 | 11.092300 | 1.476410 | . 2 -114.119 | 0.993898 | 0.268831 | 4.0 | 15.0 | 0.0 | 116.779 | -4.20318 | 1.18980 | 2.63140 | ... | 2.484450 | 1.607140 | 4.684500 | 5.79743 | 4.11147 | 4.977450 | 21.1929 | 0.983426 | 12.367600 | 7.188790 | . 3 -113.834 | 0.900557 | 0.268831 | 3.0 | 11.0 | 0.0 | 115.391 | -3.66850 | 1.11506 | 3.13985 | ... | 2.165080 | -0.119275 | 0.539258 | 2.41229 | 4.41436 | 1.592670 | 21.3086 | 8.015130 | 0.810029 | 0.892391 | . 4 -114.232 | 0.941207 | 0.268831 | 3.0 | 15.0 | 0.0 | 115.936 | -4.54294 | 1.31403 | 2.90515 | ... | -1.166400 | 4.986770 | 1.633330 | 5.03816 | 5.23080 | 3.522200 | 20.4852 | -1.422330 | 12.302300 | 8.824610 | . 5 rows × 107 columns . As expected, each parameter&#39;s conditional distribution is normal. The dashed lines in each plot separate out each chain that was sampled. . def plot_dist(df, param): _, ax = plt.subplots(1, 2, figsize=(12,4)) df[param].plot.density(ax=ax[0]) df[param].plot(ax=ax[1]) for i in [1000, 2000, 3000]: ax[1].axvline(i, color=&quot;black&quot;, ls=&quot;--&quot;) plt.tight_layout() . plot_dist(df_samples, &quot;beta.1&quot;) . plot_dist(df_samples, &quot;beta.2&quot;) . plot_dist(df_samples, &quot;sigma&quot;) . Model evalulation . The model is evaluated using the means drawn from the posterior predictive distribution and comparing those with the actual values. . y_test_pred = df_summary.loc[&quot;y_new[1]&quot;:,&quot;Mean&quot;].values y_train_pred = df_summary.loc[&quot;y_train_pred[1]&quot;:&quot;y_train_pred[72]&quot;,&quot;Mean&quot;].values . Here is the training set R$^{2}$: . r2_score(y_train, y_train_pred) . 0.7328456670720971 . Here is the test set R$^{2}$: . r2_score(y_test, y_test_pred) . 0.5840665444879212 . Inference: generated quantities in same file . We&#39;ve already made predictions both on the training and test sets using generated quantities block in our Stan code. . y_train_pred = df_samples.loc[:,&quot;y_train_pred.1&quot;:&quot;y_train_pred.72&quot;] y_test_pred = df_samples.loc[:, &quot;y_new.1&quot;:] . Note that we actually have 4,000 values for each example in our dataset. In the Bayesian point of view, one samples from a normal distribution with a mean of X$ beta$ and standard deviation $ sigma$ (where $ beta$ and $ sigma$ were previously learned in our MCMC sampling) repeatedly and then takes the mean of that as the prediction. This is the posterior predictive mean. . y_test_pred . y_new.1 y_new.2 y_new.3 y_new.4 y_new.5 y_new.6 y_new.7 y_new.8 y_new.9 y_new.10 ... y_new.16 y_new.17 y_new.18 y_new.19 y_new.20 y_new.21 y_new.22 y_new.23 y_new.24 y_new.25 . 0 6.08919 | 0.831190 | 2.771020 | 1.19964 | 2.946870 | 0.176526 | 2.269890 | 12.43920 | -0.387497 | 6.639150 | ... | 4.038810 | 0.820634 | 1.564890 | 0.895250 | 3.42023 | 0.159389 | 22.1371 | 0.955606 | 9.501570 | 6.104850 | . 1 12.44440 | 5.118900 | 2.246190 | 2.82562 | -0.079497 | -0.518796 | 1.880710 | 7.30614 | 4.465200 | 5.113740 | ... | -0.155484 | 0.652295 | 1.579160 | 3.957260 | 4.14529 | -0.440211 | 21.8035 | 10.706000 | 11.092300 | 1.476410 | . 2 11.89770 | 3.838600 | 5.517290 | 1.51142 | -0.398633 | 0.979285 | 7.150900 | 4.49293 | 2.036590 | 1.760600 | ... | 2.484450 | 1.607140 | 4.684500 | 5.797430 | 4.11147 | 4.977450 | 21.1929 | 0.983426 | 12.367600 | 7.188790 | . 3 9.53034 | -1.520570 | 7.847520 | 1.49295 | 0.917469 | 4.955140 | 2.428350 | 3.39252 | 5.642480 | 1.699600 | ... | 2.165080 | -0.119275 | 0.539258 | 2.412290 | 4.41436 | 1.592670 | 21.3086 | 8.015130 | 0.810029 | 0.892391 | . 4 13.55290 | 6.947870 | -0.947383 | 3.79690 | 3.489320 | 0.367080 | 0.305908 | 12.57860 | 4.602460 | 3.596310 | ... | -1.166400 | 4.986770 | 1.633330 | 5.038160 | 5.23080 | 3.522200 | 20.4852 | -1.422330 | 12.302300 | 8.824610 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 3995 12.68620 | 1.036800 | 4.300930 | 4.01384 | 0.033404 | 4.478290 | 5.797940 | 5.23659 | 4.104620 | 5.144340 | ... | 1.530610 | -2.970480 | 3.818540 | -0.302308 | 9.37741 | 2.563720 | 15.6282 | 1.196290 | 3.683390 | 6.422640 | . 3996 10.12300 | 5.759870 | 8.194520 | -1.85087 | 1.570890 | -3.516320 | 1.199860 | 12.75210 | 5.988360 | 8.680570 | ... | -0.882405 | 4.475370 | -0.016239 | 5.493630 | 11.20450 | 2.397010 | 22.5335 | 5.389330 | 5.559980 | 5.270210 | . 3997 13.25500 | 2.881080 | 5.618130 | 3.52824 | 2.178100 | 1.196510 | -0.754878 | 13.79700 | 4.082110 | 0.358575 | ... | -0.214590 | 4.076230 | 4.240610 | 1.991630 | 3.35809 | -0.671965 | 29.6522 | 2.818600 | 13.254700 | -0.115525 | . 3998 12.08630 | -0.392227 | -0.154641 | 4.51602 | 2.124500 | 0.875612 | 3.252300 | 9.75291 | 6.481030 | 5.653030 | ... | 3.016780 | -1.002880 | 2.177480 | 6.925280 | 4.95899 | 1.920580 | 21.8189 | 4.591150 | 6.822760 | 0.935449 | . 3999 7.96612 | 4.395740 | -1.077040 | 0.24359 | 1.732280 | 10.293700 | 4.920210 | 9.94235 | 5.173920 | 10.439600 | ... | 8.242310 | 2.557110 | 1.761830 | 4.149350 | 3.29892 | -4.806080 | 20.8138 | 6.282450 | 13.570600 | 8.343440 | . 4000 rows × 25 columns . def plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test): test_mean = np.mean(y_test_pred, axis=0) test_lower = np.percentile(y_test_pred, 2.5, axis=0) test_upper = np.percentile(y_test_pred, 97.5, axis=0) plt.plot(X_test[:,1:].ravel(), np.mean(y_test_pred, axis=0), lw=0, marker=&#39;o&#39;, color=&#39;C3&#39;, label=&quot;Test post. pred. mean&quot;) plt.plot(X_test[:,1:].ravel(), y_test, lw=0, marker=&#39;o&#39;, color=&#39;C4&#39;, label=&quot;Test actual&quot;) sort_mask = np.argsort(X_train[:,1:].ravel()) y_upper = np.percentile(y_train_pred, 97.5, axis=0)[sort_mask] y_lower = np.percentile(y_train_pred, 2.5, axis=0)[sort_mask] plt.fill_between(X_train[:,1:].ravel()[sort_mask], y_lower, y_upper, color=&quot;C1&quot;, alpha=0.1, label=&quot;Train post. pred 95% confid.&quot;) plt.plot(X_train[:,1:].ravel()[sort_mask], np.mean(y_train_pred, axis=0)[sort_mask], label=&quot;Train post. pred. mean&quot;, color=&quot;C1&quot;) plt.legend() plt.ylabel(&quot;Profit in $10,000s&quot;) plt.xlabel(&quot;Population of city in 10,000s&quot;) plt.show() . Here&#39;s the plot with the posterior predictive distribution&#39;s means for each test point. The shaded error indicates the 95% confidence interval. One could calculate this for each point one predicts for the test set. In this case we just did it for the series of points in the training set and filled in the entire range to illustrate it. . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: generated quantities in separate file . This method we take the learned parameters from MCMC sampling and use them as data inputs for another Stan file where we can generate our new quantities. Here are the contents of our new file. This time beta and sigma are in the data block since they are not going to be learned parameters. Additionally we have no model. The purpose of the file is simply to generate predictions on new data. . data { int&lt;lower=0&gt; N; int&lt;lower=0&gt; K; matrix[N, K] x; vector[K] beta; real&lt;lower=0&gt; sigma; } parameters { } model { } generated quantities { vector[N] y; for (n in 1:N) { y[n] = normal_rng(x[n]*beta, sigma); } } . Here are our learned parameters from our previous MCMC sampling; . beta, sigma = df_summary.loc[&quot;beta[1]&quot;:&quot;beta[2]&quot;,&quot;Mean&quot;].values, df_summary.loc[&quot;sigma&quot;, &quot;Mean&quot;] . Here is our data dictionary. We are making predictions on our test set. . data_dict = {&quot;N&quot;: N_test, &quot;K&quot;: K, &quot;x&quot;: X_test, &quot;beta&quot;: beta, &quot;sigma&quot;: sigma} . predict = CmdStanModel(stan_file=&quot;predict.stan&quot;) . INFO:cmdstanpy:found newer exe file, not recompiling INFO:cmdstanpy:compiled model file: /home/wes/Documents/data-science/bayesian-regression/predict . Now for the sampling. We&#39;re not doing any MCMC sampling here though; we&#39;re just generating new quantities. Note that fixed_param is defined as true. . predict_fit = predict.sample(data_dict, fixed_param=True) . INFO:cmdstanpy:start chain 1 INFO:cmdstanpy:finish chain 1 . predict_fit.summary().head() . Mean MCSE StdDev 5% 50% 95% N_Eff N_Eff/s R_hat . name . lp__ 0.00000 | NaN | 0.00000 | 0.000000 | 0.00000 | 0.00000 | NaN | NaN | NaN | . y[1] 11.21580 | 0.087441 | 2.96484 | 6.347790 | 11.08770 | 16.03550 | 1149.680 | 41859.8 | 0.999105 | . y[2] 3.72469 | 0.112697 | 3.13389 | -1.419590 | 3.73300 | 8.73753 | 773.297 | 28155.7 | 0.999482 | . y[3] 6.21616 | 0.095375 | 3.00444 | 1.420470 | 6.28273 | 11.15670 | 992.327 | 36130.6 | 0.999321 | . y[4] 3.77119 | 0.094859 | 3.00454 | -0.931101 | 3.70973 | 8.62259 | 1003.220 | 36527.3 | 1.001740 | . predict_fit.get_drawset().head() . lp__ accept_stat__ y.1 y.2 y.3 y.4 y.5 y.6 y.7 y.8 ... y.16 y.17 y.18 y.19 y.20 y.21 y.22 y.23 y.24 y.25 . 0 0.0 | 0.0 | 9.93461 | 0.795845 | 9.37373 | 4.86905 | -1.936690 | 2.730590 | 6.71166 | 4.63234 | ... | 2.47424 | -2.106800 | 4.328990 | -4.09508 | 3.22460 | 0.928246 | 19.1007 | 2.54161 | -0.72615 | 4.27538 | . 1 0.0 | 0.0 | 9.78002 | 3.650580 | 2.46062 | 5.53808 | 0.517296 | 11.795200 | 3.18090 | 4.58967 | ... | 7.63182 | -3.622690 | 6.968730 | 2.36065 | 3.72439 | 1.174960 | 19.6114 | 3.16476 | 6.78070 | 6.59602 | . 2 0.0 | 0.0 | 16.65800 | 8.105250 | 10.15530 | 3.86963 | 3.501780 | 3.688170 | 4.22701 | 8.99956 | ... | -1.82703 | 3.957610 | 1.580840 | 1.65086 | 1.26456 | 5.537600 | 24.4481 | 1.57349 | 10.43740 | 3.81600 | . 3 0.0 | 0.0 | 11.98580 | 1.890660 | 0.21170 | 4.94291 | 7.858740 | 0.943565 | 4.44094 | 13.05240 | ... | -3.27595 | -0.240402 | 0.048488 | -2.81462 | 6.59352 | -1.051190 | 21.3794 | 6.60292 | 2.83685 | 4.78816 | . 4 0.0 | 0.0 | 8.98131 | 8.780330 | 8.03854 | -3.58606 | 5.412100 | 1.770760 | 2.06025 | 9.23470 | ... | 1.95720 | 5.884040 | 8.074480 | 4.45466 | -1.19269 | 1.124830 | 23.8451 | 13.15670 | 2.52032 | 9.40728 | . 5 rows × 27 columns . df_fit_samples = pd.DataFrame(predict_fit.sample[:,0,:], columns=predict_fit.column_names) . y_test_pred = df_fit_samples.loc[:, &quot;y.1&quot;:] . plot_pred(X_train, X_test, y_train_pred, y_test_pred, y_test) . Inference: using numpy . Lastly, we don&#39;t actually need to use Stan to generate the posterior preditive distribution&#39;s mean. We can do that using numpy by writing the function ourself, which we do here. . y_test_pred = np.zeros((N_test, 1000)) for i in range(N_test): y_test_pred[i,:] = np.random.normal(np.matmul(X_test[i], beta), sigma, size=1000) . Note that we need to transpose our predictions to use our plot function from earlier. . plot_pred(X_train, X_test, y_train_pred, y_test_pred.transpose(), y_test) .",
            "url": "https://barnett.science/bayesian/2020/01/27/baysian-linear-regression.html",
            "relUrl": "/bayesian/2020/01/27/baysian-linear-regression.html",
            "date": " • Jan 27, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://barnett.science/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Microsoft Word Example Post",
            "content": "When writing a blog post with Microsoft Word – the filename becomes the title. In this case the file name is “2020-01-01-Microsoft-Word-Example-Post.docx”. . There is minimal support for Word documents in fastpages compared to Jupyter notebooks. Some known limitations: . alt text in Word documents are not yet supported by fastpages, and will break links to images. . | You can only specify front matter for Word documents globally. See the README for more details. . | . For greater control over the content produced from Word documents, you will need to convert Word to markdown files manually. You can follow the steps in this blog post, which walk you through how to use pandoc to do the conversion. Note: If you wish to customize your Word generated blog post in markdown, make sure you delete your Word document from the _word directory so your markdown file doesn’t get overwritten! . If your primary method of writing blog posts is Word documents, and you plan on always manually editing Word generated markdown files, you are probably better off using fast_template instead of fastpages. . The material below is a reproduction of this blog post, and serves as an illustrative example. . Maintaining a healthy open source project can entail a huge amount of toil. Popular projects often have orders of magnitude more users and episodic contributors opening issues and PRs than core maintainers capable of handling these issues. . Consider this graphic prepared by the NumFOCUS foundation showing the number of maintainers for three widely used scientific computing projects: . . We can see that across these three projects, there is a very low ratio maintainers to users. Fixing this problem is not an easy task and likely requires innovative solutions to address the economics as well as tools. . Due to its recent momentum and popularity, Kubeflow suffers from a similar fate as illustrated by the growth of new issues opened: . . Source: “TensorFlow World 2019, Automating Your Developer Workflow With ML” . Coincidentally, while building out end to end machine learning examples for Kubeflow, we built two examples using publicly available GitHub data: GitHub Issue Summarization and Code Search. While these tutorials were useful for demonstrating components of Kubeflow, we realized that we could take this a step further and build concrete data products that reduce toil for maintainers. . This is why we started the project kubeflow/code-intelligence, with the goals of increasing project velocity and health using data driven tools. Below are two projects we are currently experimenting with : . Issue Label Bot: This is a bot that automatically labels GitHub issues using Machine Learning. This bot is a GitHub App that was originally built for Kubeflow but is now also used by several large open source projects. The current version of this bot only applies a very limited set of labels, however we are currently A/B testing new models that allow personalized labels. Here is a blog post discussing this project in more detail. . | Issue Triage GitHub Action: to compliment the Issue Label Bot, we created a GitHub Action that automatically adds / removes Issues to the Kubeflow project board tracking issues needing triage. . | Together these projects allow us to reduce the toil of triaging issues. The GitHub Action makes it much easier for the Kubeflow maintainers to track issues needing triage. With the label bot we have taken the first steps in using ML to replace human intervention. We plan on using features extracted by ML to automate more steps in the triage process to further reduce toil. . Building Solutions with GitHub Actions . One of the premises of Kubeflow is that a barrier to building data driven, ML powered solutions is getting models into production and integrated into a solution. In the case of building models to improve OSS project health, that often means integrating with GitHub where the project is hosted. . We are really excited by GitHub’s newly released feature GitHub Actions because we think it will make integrating ML with GitHub much easier. . For simple scripts, like the issue triage script, GitHub actions make it easy to automate executing the script in response to GitHub events without having to build and host a GitHub app. . To automate adding/removing issues needing triage to a Kanban board we wrote a simple python script that interfaces with GitHub’s GraphQL API to modify issues. . As we continue to iterate on ML Models to further reduce toil, GitHub Actions will make it easy to leverage Kubeflow to put our models into production faster. A number of prebuilt GitHub Actions make it easy to create Kubernetes resources in response to GitHub events. For example, we have created GitHub Actions to launch Argo Workflows. This means once we have a Kubernetes job or workflow to perform inference we can easily integrate the model with GitHub and have the full power of Kubeflow and Kubernetes (eg. GPUs). We expect this will allow us to iterate much faster compared to building and maintaining GitHub Apps. . Call To Action . We have a lot more work to do in order to achieve our goal of reducing the amount of toil involved in maintaining OSS projects. If your interested in helping out here’s a couple of issues to get started: . Help us create reports that pull and visualize key performance indicators (KPI). https://github.com/kubeflow/code-intelligence/issues/71 . We have defined our KPI here: issue #19 | . | Combine repo specific and non-repo specific label predictions: https://github.com/kubeflow/code-intelligence/issues/70 . | . In addition to the aforementioned issues we welcome contributions for these other issues in our repo. .",
            "url": "https://barnett.science/2020/01/01/Microsoft-Word-Example-Post.html",
            "relUrl": "/2020/01/01/Microsoft-Word-Example-Post.html",
            "date": " • Jan 1, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Classification Decision Trees & Entropy",
            "content": "In this notebook I walk through how a classification decision tree is fit, how inference is performed, how to reduce overfitting, how splits are determined. This is based on a notebook I created while I was fellow at Insight Data Science when studying for interviews. . import graphviz import matplotlib.pyplot as plt import numpy as np from sklearn import tree from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier . scikit-learn now comes with a way to plot trees, but I prefer using graphviz so here is a quick function to plot a tree, which we monkey-patch into the DecisionTreeClassifier and DecisionTreeRegressor classes. . def display_tree(self): dot_data = tree.export_graphviz(self, out_file=None, filled=True, rounded=True, special_characters=True) graph = graphviz.Source(dot_data) return graph DecisionTreeClassifier.plot = display_tree . We&#39;ll use the Iris dataset as a quick way to discuss classification trees. To learn more about this dataset use help(load_iris). . iris = load_iris() X = iris.data y = iris.target X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0) . Fitting . Decision trees search over all possible ways to split up features and find the split that is most informative about the target variable. The parent node splits into two child nodes based on this split. From there the children also split in the same manner until all leaves are pure, unless another stopping condition is specified. A leaf is a node that has no children. A pure leaf is a leaf with only one class of items in it. . Classifications trees split using the GINI impurity which is defined as: . $I_{G}(p) = sum_{i=1}^{J}p_{i}(1-p_{i})$ . Here $p_{i}$ is the probability of an item with label $i$ being chosen and $1 - p_{i}$ is the probability of a mistake in categorizing that item. $J$ is the number of classes. Gini reaches zero when all cases in the node fall into a single target category. . Alternatively, one can use information gain to decide where to split, where information gain is defined as the difference in entropy of the parent and the weighted sum of the entropies of the children. Entropy is defined as: . $H(p) = - sum_{i=1}^{J}p_{i} log_{2}(p_{i})$ . Let&#39;s train a decision tree: . clf = DecisionTreeClassifier(criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 2 ≤ 2.35 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 8 X 3 ≤ 1.75 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;8 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 X 1 ≤ 3.1 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 6 entropy = 0.0 samples = 3 value = [0, 0, 3] 5&#45;&gt;6 7 entropy = 0.0 samples = 1 value = [0, 1, 0] 5&#45;&gt;7 9 X 3 ≤ 1.65 entropy = 0.811 samples = 4 value = [0, 1, 3] 8&#45;&gt;9 12 entropy = 0.0 samples = 35 value = [0, 0, 35] 8&#45;&gt;12 10 entropy = 0.0 samples = 3 value = [0, 0, 3] 9&#45;&gt;10 11 entropy = 0.0 samples = 1 value = [0, 1, 0] 9&#45;&gt;11 Here&#39;s the training score which indeed shows the tree is perfect at classfying the flowers on the training set. This tends to result in overfitting to the training set. . clf.score(X_train, y_train) . 1.0 . This is an easy dataset to classify, so the overfitting is not evident here. . clf.score(X_test, y_test) . 0.9736842105263158 . There are 7 leaves in our tree. Note that the leaves do not have to be depicted at the bottom of the tree in the diagram. A leaf is just a node without any children and could be represented near the top of the tree. . clf.get_n_leaves() . 7 . Inference . Now that we have trained our model, we can perform inference. . When inference on new samples is performed, the sample simply is examined with the &quot;rules&quot; created by the feature splits. Starting from the topmost node (the root node) in our example above, if feature three has a value of less than or equal 0.8, go to the left child node; otherwise go to the right. This process continues all the way down until the sample is put into a leaf. . The predicted class is the class in the leaf with the highest probability of that class for that leaf. In other words, simply break down the training samples by class within that leaf and choose the class with the most number of train samples. The probability of choosing that class is simply the number of training samples in that leaf belonging to that class divided by the total number of training samples in that leaf. . Since all of our leafs are pure, the classifier will always give 100% for its predictions. We&#39;ll rexamine this when we have impure leafs below. . Here are the features for the first test sample. . X_test[0] . array([5.8, 2.8, 5.1, 2.4]) . The path for this sample follows down the right side of the tree. Note that features are zero-indexed. . Root node: Is 2.4 &lt;= 0.8? No, so go right. . Is 5.1 &lt;= 4.95? No, so go right. . Is 5.1 &lt;= 5.05? No, so go right. . That brings it to the leaf on the right with 35 samples, where the 3rd class (index 2) is predicted: . clf.predict([X_test[0]]) . array([2]) . Again, the probability is 100% since there are no training samples in that leaf from the other two classes. In the plot the node is colored dark purple. For this tree purple represents the 3rd class and the darker the shade the more probable it is. . clf.predict_proba([X_test[0]]) . array([[0., 0., 1.]]) . Pruning . One way to prevent overfitting is the pre-prune the tree by specifying the maximum depth and/or maximum number of leaves. Here we set the maximum depth to 3. . clf = DecisionTreeClassifier(max_depth=3, random_state=0, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 3 ≤ 0.8 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 X 2 ≤ 4.95 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False 3 X 3 ≤ 1.65 entropy = 0.414 samples = 36 value = [0, 33, 3] 2&#45;&gt;3 6 X 2 ≤ 5.05 entropy = 0.172 samples = 39 value = [0, 1, 38] 2&#45;&gt;6 4 entropy = 0.0 samples = 32 value = [0, 32, 0] 3&#45;&gt;4 5 entropy = 0.811 samples = 4 value = [0, 1, 3] 3&#45;&gt;5 7 entropy = 0.811 samples = 4 value = [0, 1, 3] 6&#45;&gt;7 8 entropy = 0.0 samples = 35 value = [0, 0, 35] 6&#45;&gt;8 clf.score(X_train, y_train) . 0.9821428571428571 . clf.score(X_test, y_test) . 0.9736842105263158 . Here is a sample where we are only 75% sure that it is class 2, since only 3 of the 4 samples in its leaf are class 2. . clf.predict([X_test[20]]) . array([2]) . clf.predict_proba([X_test[20]]) . array([[0. , 0.25, 0.75]]) . A popular and effective way to reduce overfitting is to create a &quot;forest&quot; of weak decision trees and use them together (for example, random forests). This is beyond the scope of this notebook. . Information gain &amp; splits . Let&#39;s talk a little bit more about how trees use entropy (or alternatively Gini) to determine splits. . Information gain is calculated by cycling through all possible splits in the training set. Practically this is the process: . Select the first feature. | Pick the halfway point between the first sample and the second sample. | Calculate the entropy of the two child nodes if a split is made at that point. | Repeat steps 2 and 3 for all midpoints for this feature. | Go back to step 1 and repeat for all features. | At the end, pick the feature and split that has the lowest weighted summation of the entropies for the two child nodes. . Again, information gain is the difference in entropy of the parent node and the weighted summation of entropies of the two child nodes. Since the entropy of the parent node is the same for each potential split that we try, we only need to calculate the entropies of the child nodes for a split and find the split that minimizes their weighted sum since that will maximize the information gained. . Here&#39;s the formula for information gain (IG), where $H_{parent}$ is the entropy of the parent node, $N_{left}$ is the number of samples in the left child, $H_{left}$ is the entropy of the left child, $N_{right}$ is the number of samples in the right child, $H_{right}$ is the entropy of the right child: . $IG = H_{parent} - (N_{left} H_{left} + N_{right} H_{right})$ . Here&#39;s a tree with a max depth of 1 using entropy to split: . clf = DecisionTreeClassifier(random_state=42, max_depth=1, criterion=&quot;entropy&quot;).fit(X_train, y_train) . clf.plot() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 X 2 ≤ 2.35 entropy = 1.581 samples = 112 value = [37, 34, 41] 1 entropy = 0.0 samples = 37 value = [37, 0, 0] 0&#45;&gt;1 True 2 entropy = 0.994 samples = 75 value = [0, 34, 41] 0&#45;&gt;2 False Let&#39;s implement our entropy calculation. As a reminder, the formula is: . $H(p) = - sum_{i=1}^{J}p_{i} log_{2}(p_{i})$ . This takes a list of targets and calculates the entropy for that node. . from collections import Counter def entropy(values): total = values.shape[0] c = Counter(values) if total == 0: return 0 s = 0 for x in c.values(): p = x/total s += p * np.log2(p) return -s . Plotting entropy in the binary class case . Let&#39;s take a quick look at an example where we have two classes, 0 and 1, in 100 samples (so 50 of each class) to get an idea of what entropy is describing. If we perfectly split our data into 0&#39;s in the left bucket, and 1&#39;s into the right bucket, we have 0 entropy. Whatever (hypothetical) decision rule that caused that split has split our data perfectly. . left, right = np.zeros(50), np.ones(50) . Because each node is pure, the entropy is 0 for each node: . entropy(left), entropy(right) . (-0.0, -0.0) . left[0], right[0] = 1, 0 . Now let&#39;s change 1 sample - moving one of the 0&#39;s from the left node to the right, and one of the 1&#39;s from the right node to the left. Here are the entropies of those child nodes. In both cases, the entropy goes up by the same amount. . entropy(left), entropy(right) . (0.14144054254182067, 0.14144054254182067) . Let&#39;s make this discussion even simpler by looking at just a single node containing 100 samples of either 0&#39;s or 1&#39;s. Let&#39;s start with a pure node of containing just 0&#39;s and incrementally change those 0&#39;s to 1&#39;s and find out what happens with the entropy: . node = np.zeros(100) . x = [] y = [] for i in range(99): x.append((i+1)/100.) node[i] = 1 y.append(entropy(node)) . Here&#39;s the plot of the entropy of that node as a function of the fraction of positive class samples (1&#39;s) in that node. Initially when the fraction is zero (the node is all 0&#39;s), the node is pure, so the entropy is 0. As we begin to change 0&#39;s for 1&#39;s the entropy increases and eventually reaches 1 when the fraction is 0.5 (equal amount of 1&#39;s and 0&#39;s). From there, the entropy decreases as the fraction of the positive class continues to increase and eventually reaches zero again when the node is all 1&#39;s and is thu pure. . plt.plot(x,y) plt.ylabel(&quot;Entropy&quot;) plt.xlabel(&quot;Fraction positive class&quot;) plt.show() . Remember, splits are determined by information gain, that is, the difference in entropy of the parent and the weighted summation of the entropies of the children. If you have a child node with an entropy of 1.0 that means that node contributes nothing to understanding of how to split the data into the two classes. If you have a child node with an entropy of 0.0 you have been able to perfectly segment out those samples. But again, we want the weighted combination of the entropies of the children to be low. You might have a situation where one is very low and one is very high, but it&#39;s better to split elsewhere to make the weighted sum lower. . Finding the best split through iteration . Here&#39;s our original list of targets from the Iris example: . y_train . array([1, 1, 2, 0, 2, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 2, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 0, 1, 2, 1, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 2, 0, 0, 2, 0, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 1, 0, 2, 1, 2, 1, 0, 2, 0, 2, 0, 0, 2, 0, 2, 1, 1, 1, 2, 2, 1, 1, 0, 1, 2, 2, 0, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2, 0]) . Here&#39;s the associated entropy which matches above. This is the entropy of the root node. . entropy(y_train) . 1.5807197138422104 . Now let&#39;s get the entropies of the child nodes for the split that was found. In this case it was feature 3 with the split at 0.8. . feat = 3 x = 0.8 . Here are the samples that went into the left node. As you can see all of them are of class 0. There is no entropy associated with this node - the node is pure. . left = y_train[X_train[:,feat] &lt;= x] left . array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]) . entropy(left) . -0.0 . Here are the samples that went into the right node: . right = y_train[X_train[:,feat] &gt; x] right . array([1, 1, 2, 2, 1, 2, 2, 2, 2, 1, 2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2]) . entropy(right) . 0.993707106604508 . Now we need the cycle through each feature, and then cycle through each possible split. . import math def calc_entropies(X, y): n = y.shape[0] m = X.shape[1] min_ents = np.zeros(m) min_splits = np.zeros(m) for feat in range(m): # Use set to remove dups; sort it to get halfway points points = sorted(list(set(X[:,feat]))) splits = [(points[i-1]+points[i])/2. for i in range(1, len(points))] entropies = [] for x in splits: l = y_train[X_train[:,feat] &lt;= x] r = y_train[X_train[:,feat] &gt; x] e = (l.shape[0]*entropy(l) + r.shape[0]*entropy(r)) / n entropies.append(e) feat_min_ent = np.argmin(entropies) min_ents[feat] = entropies[feat_min_ent] min_splits[feat] = splits[feat_min_ent] min_feat = np.argmin(min_ents) return min_feat, min_splits[min_feat], min_ents[min_feat] . calc_entropies(X_train, y_train) . (2, 2.35, 0.6654288660298044) . Note that in this example splitting feature 3 at 0.8 gives the same entropy as the above split. np.argmin returns the argument of the first minimum in the case of a tie. .",
            "url": "https://barnett.science/machinelearning/2019/05/14/decision-trees.html",
            "relUrl": "/machinelearning/2019/05/14/decision-trees.html",
            "date": " • May 14, 2019"
        }
        
    
  
    
        ,"post15": {
            "title": "Speed comparisons with Python, Numba, Fortran",
            "content": "The classic Monte Carlo &quot;Hello World&quot; is to calculate $ pi$ by generating random numbers and accepting or rejecting them. The algorithm below is taken from one of my graduate textbooks on molecular simulations - I apologize that I can&#39;t remember where, and that book is long gone. Essentially one is throwing darts into the unit square and then measuring if it is within the quarter of the unit circle. $ pi$ equals the number of darts in the circle divided by all of the attempts. Since it is a quarter of the unit circle, we multiple by 4. . I thought it would be fun to try to use some of the available tooling to get a speed performance in Python as well as using some Fortan inline since Fortran was the major language I used in graduate school. Note that fortranmagic is located in the fortran-magic package. . %load_ext fortranmagic import numpy as np from numba import jit, njit, prange . n = 10000000 . Plain Python and Numpy . def calc_pi_p(n): accept = 0.0 for i in range(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_p(n) . CPU times: user 47.9 s, sys: 25.6 ms, total: 47.9 s Wall time: 48.1 s . 3.141434 . Numba . @jit def calc_pi_numba(n): accept = 0.0 for i in range(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_numba(n) . CPU times: user 2.29 s, sys: 13.2 ms, total: 2.31 s Wall time: 2.31 s . 3.1418784 . Numba in parallel . @njit(parallel=True) def calc_pi_numba_parallel(n): accept = 0.0 for i in prange(n): r = np.random.random(2) if (np.dot(r,r) &lt;= 1.0): accept += 1.0 return 4.0 * accept / float(n) . %%time calc_pi_numba_parallel(n) . CPU times: user 1.24 s, sys: 9.69 ms, total: 1.25 s Wall time: 1.09 s . 3.1403444 . Fortran . %%fortran function calc_pi_f(n) result(pi) implicit none integer(8) :: accept real(8) :: r(2), pi integer(8), intent(in) :: n integer(8) :: i accept = 0 do i = 1, n call random_number(r) if (dot_product(r,r) &lt;= 1.0) then accept = accept + 1 end if end do pi = 4.0d0 * dble(accept)/dble(n) end function calc_pi_f . %%time calc_pi_f(n) . CPU times: user 406 ms, sys: 0 ns, total: 406 ms Wall time: 405 ms . 3.1423488 . Looks like Fortran is the winner in this case. We didn&#39;t even parallelize the operation, but Numba does give a signifcant speedup compared to plain Python/Numpy. There might be a way to vectorize some of the operations in Numpy though to perform better, but I failed to find out how to do that in this case. .",
            "url": "https://barnett.science/performance/2018/08/27/fortran-speed.html",
            "relUrl": "/performance/2018/08/27/fortran-speed.html",
            "date": " • Aug 27, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I solve business problems through the end-to-end implementation of machine learning products. . . I have a BS in Mechanical Engineering from Mississippi State University, an MDiv from New Orleans Baptist Theological Seminary, and a PhD in Chemical &amp; Molecular Engineering from Tulane University. My graduate research utilized programming, data science, and molecular simulations to study self-assembling molecules. After receiving my PhD, I worked as a postdoctoral research scientist at Columbia University where I studied polymer interactions and gas separation membranes using molecular simulations and machine learning. . More recently I was a fellow with Insight Data Science where I created a Chrome Extension to predict where a user would post their content. I currently am in a senior machine learning engineer role at a financial company. .",
          "url": "https://barnett.science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://barnett.science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}